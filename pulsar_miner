#!/usr/bin/env python3
# ALESSANDRO RIDOLFI ########################

import sys
import os
import os.path
import glob
import subprocess
import multiprocessing
import shlex
import shutil
import copy
import random
import time
import datetime
import numpy as np
import urllib

try:
        from presto import filterbank, infodata, parfile, psr_utils, psrfits, rfifind, sifting
except:
        print("\nERROR: Could not not load the PRESTO python modules!")
        print("Please make sure that your PRESTO python modules are actually installed and working.\n")
        exit()

        

import warnings
from multiprocessing.pool import ThreadPool

warnings.simplefilter('ignore', UserWarning)

string_version_full = "2.0-beta (27Jul2024)"
string_version = "27Jul2024"
number_version = 20240727.0

class colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    ERROR = '\033[91m'
    BOLD = '\033[1m'
    ENDCOLOR = '\033[0m'


def check_for_updates(number_this_version, string_this_version, flag_verbose=False):
        URL_version_history="https://raw.githubusercontent.com/alex88ridolfi/PULSAR_MINER/master/VERSION_HISTORY"
        flag_can_update = False

        try:
                top_line = urllib.request.urlopen(URL_version_history).read().decode('utf-8').split("\n")[0]
                number_latest_version = np.float64(top_line.split(":")[-1].split("(")[0].strip())
                string_latest_version = top_line.split("(")[1].split(")")[0]
                
                return (number_latest_version, string_latest_version)
        
        except: # If unable to retrieve the latest version
                
                if flag_verbose == True:
                        print("%sERROR%s: Could not retrieve the latest version of PULSAR_MINER from the web." % (colors.ERROR+colors.BOLD, colors.ENDCOLOR))
                        print("       Please check your internet connection and retry.")
                        print()


                return (number_this_version, string_this_version)

                        
def update_pulsar_miner(dir_pulsar_miner, string_this_version, string_latest_version):
        current_work_dir = os.getcwd()

        # Back-up the current version
        try:
                import tarfile
                parent_dir_pulsar_miner = "/".join(dir_pulsar_miner.split("/")[:-1])
                pulsar_miner_dirname = dir_pulsar_miner.split("/")[-1]

                bck_temp_dir_abspath = "%s_%s_bck" % (dir_pulsar_miner, string_this_version)
                bck_temp_dirname = "%s_%s_bck" % (pulsar_miner_dirname, string_this_version)

                tarred_file_abspath = os.path.join(parent_dir_pulsar_miner, "PULSAR_MINER_%s_bck.tar" % (string_this_version))
                tarred_file = tarfile.open(tarred_file_abspath,"w")
                shutil.copytree(dir_pulsar_miner, bck_temp_dir_abspath)
                os.chdir(parent_dir_pulsar_miner)
                tarred_file.add(bck_temp_dirname)
                tarred_file.close()
                shutil.rmtree(bck_temp_dir_abspath)
                os.chdir(current_work_dir)
                print("Old version of PULSAR_MINER backed up in %s." % tarred_file_abspath)
                print()
        except  Exception as error:
                print(error)
                print("%sWARNING%s: Could not backup your older version of PULSAR_MINER in a .tar file." % (colors.WARNING+colors.BOLD, colors.ENDCOLOR))
                print("     Anyway, you can always go always use an older version of the pipeline using 'git checkout' to an older commit.")
                print()

        # Update to the latest version
        try:
                os.chdir(dir_pulsar_miner)
                os.system("git pull")
                os.chdir(current_work_dir)
                print("PULSAR_MINER installation (%s) successfully updated to the latest version (%s-%s)!" % (dir_pulsar_miner, string_latest_version, number_latest_version) )
                print()

        except:
                print("%sERROR%s: Could not update PULSAR_MINER..." % (colors.ERROR+colors.BOLD, colors.ENDCOLOR))
                print()
    
def check_presto_path(presto_path, key):
        if os.path.exists(presto_path):
                if os.path.exists(presto_path+"/bin/accelsearch"):
                        return True
                else:
                        print("%sERROR%s: %s directory '%s' exists but I could not find 'accelsearch' in %s/bin!" % (colors.ERROR+colors.BOLD, colors.ENDCOLOR, key, presto_path, presto_path))
                        print("Please make sure that your %s installation is actually there and working." % (key))
                        exit()
                
        else:
                print("%sERROR:%s %s directory '%s' does not exist!" % (colors.ERROR+colors.BOLD, colors.ENDCOLOR, key, presto_path))
                print("Please make sure that the path of %s in your configuration file is correct." % (key))
                exit()
                

def check_if_enough_disk_space(root_workdir, num_DMs, T_obs_s, t_samp_s, list_segments_nofull, flag_remove_fftfiles, flag_remove_datfiles_of_segments):
        disk_space            = shutil.disk_usage(root_workdir)
        disk_space_free_bytes = disk_space.free

        N_samples_per_datfile_full   = int(T_obs_s / t_samp_s)
        datfile_full_size_bytes      = N_samples_per_datfile_full * 4

        if flag_remove_fftfiles == 0:
                datfile_full_size_bytes = datfile_full_size_bytes*2
        
        full_length_search_size_bytes = num_DMs * datfile_full_size_bytes


        print()
        print("******************************************")
        print("       Estimated disk space usage:")
        print("******************************************")
        if flag_remove_fftfiles == 0:
                print("Removing .fft files? NO  --> each DM trial will occupy double the space")
        else:
                print("Removing .fft files? YES")

        if flag_remove_datfiles_of_segments == 0:
                print("Removing .dat files of segments? NO  --> total used space will be very high")
        else:
                print("Removing .dat files of segments? YES  --> disk space usage of the segmented search will be negligible")
                
        print()
        print("Full-length      search: ~%4d GB        (%d DM trials * %5d MB per trial)" % (full_length_search_size_bytes/1.0e9, num_DMs, datfile_full_size_bytes/1.0e6) )


        
        total_search_size_bytes = full_length_search_size_bytes
        
        
        for seg in list_segments_nofull:
                seg_length_s = np.float64(seg)*60
                N_chunks = int(T_obs_s / seg_length_s)

                fraction_left = (T_obs_s % segment_length_s) / seg_length_s
                if fraction_left >= 0.80:
	                N_chunks = N_chunks + 1
                
                if flag_remove_datfiles_of_segments == 0:
                        N_samples_per_datfile_seg   = int(seg_length_s / t_samp_s)
                        datfile_seg_size_bytes      = N_samples_per_datfile_seg * 4
                else:
                        datfile_seg_size_bytes = 0
                if flag_remove_fftfiles == 0:
                        datfile_seg_size_bytes = datfile_seg_size_bytes*2
                
                seg_search_size_bytes  = datfile_seg_size_bytes * num_DMs * N_chunks
                total_search_size_bytes      = total_search_size_bytes + seg_search_size_bytes
                print("Segment   %5sm search: ~%4d GB        (%d DM trials * %5d MB per trial * %2d chunks)" % (seg, seg_search_size_bytes/1.0e9, num_DMs, datfile_seg_size_bytes/1.0e6, N_chunks) )

        print()
        
        print("EXPECTED DISK SPACE USAGE: ~%5d GB" % (1.1*total_search_size_bytes/1.0e9))
        print("     AVAILABLE DISK SPACE: ~%5d GB" % (disk_space_free_bytes/1.0e9), end="")
        if  disk_space_free_bytes > 1.1*total_search_size_bytes:
                print("   --> All good! That's enough.")
                return True
        else:
                print("   --> OUCH! That's NOT enough!")
                return False



class Pulsar(object):
        def __init__(self, parfilename):
                LIGHT_SPEED = 2.99792458e10   # Light speed in CGS units

                pulsar_parfile = parfile.psr_par(parfilename)
                
                self.parfilename = parfilename
                if hasattr(pulsar_parfile, 'PSR'):
                        self.psr_name = pulsar_parfile.PSR
                elif hasattr(pulsar_parfile, 'PSRJ'):
                        self.psr_name = pulsar_parfile.PSRJ

                self.PEPOCH = pulsar_parfile.PEPOCH
                self.F0 = pulsar_parfile.F0
                self.P0_s = 1./self.F0
                self.P0_ms = self.P0_s * 1000
                if hasattr(pulsar_parfile, 'F1'):
                        self.F1 = pulsar_parfile.F1
                else:
                        self.F1 = 0
                if hasattr(pulsar_parfile, 'F2'):
                        self.F2 = pulsar_parfile.F2
                else:
                        self.F2 = 0

                self.is_binary = hasattr(pulsar_parfile, 'BINARY')

                if self.is_binary:
                        self.pulsar_type = "binary"
                        self.binary_model = pulsar_parfile.BINARY

                        # 1) Orbital period
                        if hasattr(pulsar_parfile, 'PB'):
                                self.Pb_d = pulsar_parfile.PB
                                self.Pb_s = self.Pb_d*86400
                                self.Fb0 = 1./self.Pb_s
                        elif hasattr(pulsar_parfile, 'FB0'):
                                self.Fb0 = pulsar_parfile.FB0
                                self.Pb_s = 1./self.Fb0
                                self.Pb_d = self.Pb_s / 86400.

                        # 2) Projected semi-major axis of the pulsar orbit
                        self.x_p_lts = pulsar_parfile.A1
                        self.x_p_cm = pulsar_parfile.A1 * LIGHT_SPEED

                        # 3) Orbital eccentricity
                        if hasattr(pulsar_parfile, 'E'):
                                self.ecc = pulsar_parfile.E
                        elif hasattr(pulsar_parfile, 'ECC'):
                                self.ecc = pulsar_parfile.ECC
                        elif hasattr(pulsar_parfile, 'EPS1') and hasattr(pulsar_parfile, 'EPS2'):
                                self.eps1 = pulsar_parfile.EPS1
                                self.eps2 = pulsar_parfile.EPS2
                                self.ecc = np.sqrt(self.eps1**2 + self.eps2**2)
                        else:
                                self.ecc = 0

                        # 4) Longitude of periastron
                        if hasattr(pulsar_parfile, 'OM'):
                                self.omega_p_deg = pulsar_parfile.OM
                        else:
                                self.omega_p_deg = 0
                        self.omega_p_rad = self.omega_p_deg * np.pi/180

                        # 5) Epoch of passage at periastron/ascending node
                        if hasattr(pulsar_parfile, 'T0'):
                                self.T0 = pulsar_parfile.T0
                                self.Tasc = self.T0
                        elif hasattr(pulsar_parfile, 'TASC'):
                                self.Tasc = pulsar_parfile.TASC
                                self.T0 = self.Tasc

                        self.v_los_max = (2*np.pi * self.x_p_cm / self.Pb_s)
                        self.doppler_factor = self.v_los_max / LIGHT_SPEED

                else:
                        # If the pulsar is isolated
                        self.pulsar_type = "isolated"
                        self.v_los_max = 0
                        self.doppler_factor = 1e-4  # Account for Doppler shift due to the Earth motion around the Sun


def check_if_cand_is_known(candidate, list_known_pulsars, numharm):
        # Loop over all the known periods of the pulsars in the cluster
        P_cand_ms = candidate.p * 1000
        BOLD = '\033[1m'
        END = '\033[0m'

        for i in range(len(list_known_pulsars)):
                psrname = list_known_pulsars[i].psr_name

                P_ms = list_known_pulsars[i].P0_ms
                P_ms_min = P_ms * (1 - list_known_pulsars[i].doppler_factor)
                P_ms_max = P_ms * (1 + list_known_pulsars[i].doppler_factor)

                if (P_cand_ms > P_ms_min) and (P_cand_ms < P_ms_max):
                        # print "%sP_min = %.6f ms < %.6f < %.6f ms = P_max    --> ALREADY KNOWN: PSR %s !!%s" % (BOLD, P_ms_min, P_cand_ms, P_ms_max, psrname, END)
                        str_harm = "Fundamental (%.7f ms)" % (P_ms)
                        return True, psrname, str_harm

                else:
                        # print "----------------- HARMONICS ----"
                        for nh in range(1, numharm + 1):
                                for n in range(1, 16+1):
                                        P_known_ms_nh_min = P_ms_min * (np.float64(n) / nh)
                                        P_known_ms_nh_max = P_ms_max * (np.float64(n) / nh)

                                        # print "nh = %d/%d  --> Pulsar %s (P=%.10f) has a period = %.10f - %.10f  (cand = %.10f)" % (nh, n, psrname, P_ms, P_known_ms_nh_min, P_known_ms_nh_max, P_cand_ms)

                                        if (P_cand_ms >= P_known_ms_nh_min) and (P_cand_ms <= P_known_ms_nh_max):
                                                # print "%sARGH! Candidate with P = %.10f ms is the %d/%d-th harmonic of pulsar %s (P = %.10f ms)%s" % (BOLD, P_cand_ms, nh, n, psrname, P_ms, END)
                                                str_harm = "%d/%d of %.7f ms" % (n, nh, P_ms)
                                                return True, psrname, str_harm

                        # print "----------------- SUBHARMONICS ----"
                        for ns in range(2, numharm + 1):
                                for n in range(1, 16+1):
                                        P_known_ms_ns_min = P_ms_min * (np.float64(ns) / n)
                                        P_known_ms_ns_max = P_ms_max * (np.float64(ns) / n)
                                        # print "ns = %d/%d  --> Pulsar %s (P=%.10f) has a period = %.10f - %.10f (cand = %.10f)" % (n, ns, psrname, P_ms, P_known_ms_ns_min, P_known_ms_ns_max, P_cand_ms)

                                        if (P_cand_ms >= P_known_ms_ns_min) and (P_cand_ms <= P_known_ms_ns_max):
                                                # print "%sARGH! Candidate with P = %.10f ms is the %d/%d-th subharmonic of pulsar %s (P = %.10f ms%s)" % (BOLD, P_cand_ms, n, ns, psrname, P_ms, END)
                                                str_harm = "%d/%d of %.7f ms" % (ns, n, P_ms)
                                                return True, psrname, str_harm

        return False, "", ""


class Inffile(object):
        def __init__(self, inffilename):
                inffile = open(inffilename, "r")
                for line in inffile:
                        if "Data file name without suffix" in line:
                                self.datafilebasename = line.split("=")[-1].strip()
                        elif "Telescope used" in line:
                                self.telescope = line.split("=")[-1].strip()
                        elif "Instrument used" in line:
                                self.instrument = line.split("=")[-1].strip()
                        elif "Object being observed" in line:
                                self.source = line.split("=")[-1].strip()
                        elif "J2000 Right Ascension" in line:
                                self.RAJ = line.split("=")[-1].strip()
                        elif "J2000 Declination" in line:
                                self.DECJ = line.split("=")[-1].strip()
                        elif "Data observed by" in line:
                                self.observer = line.split("=")[-1].strip()
                        elif "Epoch of observation" in line:
                                self.start_MJD = np.float128(line.split("=")[-1].strip())
                        elif "Barycentered?" in line:
                                self.barycentered = int(line.split("=")[-1].strip())
                        elif "Number of bins in the time series" in line:
                                self.nsamples = int(line.split("=")[-1].strip())
                        elif "Width of each time series bin" in line:
                                self.tsamp_s = np.float128(line.split("=")[-1].strip())
                        elif "Any breaks in the data?" in line:
                                self.breaks_in_data = int(line.split("=")[-1].strip())
                        elif "Type of observation" in line:
                                self.obstype = line.split("=")[-1].strip()
                        elif "Beam diameter" in line:
                                self.beamdiameter = np.float128(line.split("=")[-1].strip())
                        elif "Dispersion measure" in line:
                                self.DM = np.float128(line.split("=")[-1].strip())
                        elif "Central freq of low channel" in line:
                                self.freq_ch1 = np.float128(line.split("=")[-1].strip())
                        elif "Total bandwidth" in line:
                                self.bw = np.float128(line.split("=")[-1].strip())
                        elif "Number of channels" in line:
                                self.nchan = int(line.split("=")[-1].strip())
                        elif "Channel bandwidth" in line:
                                self.bw_chan = np.float128(line.split("=")[-1].strip())
                        elif "Data analyzed by" in line:
                                self.analyzer = line.split("=")[-1].strip()
                inffile.close()


class Observation(object):
        def __init__(self, file_name, data_type="psrfits", verbosity_level=1):
                self.file_abspath = os.path.abspath(file_name)
                self.file_nameonly = self.file_abspath.split("/")[-1]
                self.file_basename, self.file_extension = os.path.splitext(self.file_nameonly)
                self.file_buffer_copy = ""

                if data_type =="filterbank":

                        try:
                                object_file = filterbank.FilterbankFile(self.file_abspath)

                                self.N_samples = object_file.nspec
                                self.t_samp_s = object_file.dt
                                self.T_obs_s = self.N_samples * self.t_samp_s
                                self.nbits = object_file.header['nbits']
                                self.nchan = object_file.nchan
                                self.chanbw_MHz = object_file.header['foff']
                                self.bw_MHz = self.nchan * self.chanbw_MHz
                                self.freq_central_MHz = object_file.header['fch1'] + object_file.header['foff']*0.5*object_file.nchan
                                self.freq_high_MHz = np.amax(object_file.freqs)
                                self.freq_low_MHz = np.amin(object_file.freqs)
                                self.MJD_int = int(object_file.header['tstart'])
                                self.Tstart_MJD = object_file.header['tstart']

                                self.source_name = object_file.header['source_name'].strip()

                        except ValueError:
                                if verbosity_level >= 1:
                                        print("WARNING: I got a Value Error! Likely your filterbank data is not 8-,16- or 32-bit. Using PRESTO's 'readfile' to get the necessary information...")

                                try:
                                        self.N_samples        = np.float64(get_command_output_with_pipe("readfile %s" % (self.file_abspath), "grep Spectra").split("=")[-1])                                        
                                        self.t_samp_s         = 1.0e-6*int(get_command_output_with_pipe("readfile %s" % (file_name), "grep Sample").split("=")[-1])
                                        self.T_obs_s          = self.N_samples*self.t_samp_s
                                        self.nbits            = int(get_command_output_with_pipe("readfile %s" % (file_name), "grep bits").split("=")[-1])
                                        self.nchan            = int(get_command_output_with_pipe("readfile %s" % (file_name), "grep channels").split("=")[-1])
                                        self.chanbw_MHz       = np.float64(get_command_output_with_pipe("readfile %s" % (file_name), "grep Channel").split("=")[-1])
                                        self.bw_MHz = self.chanbw_MHz*self.nchan                                        
                                        self.Tstart_MJD       = np.float64(get_command_output_with_pipe("readfile %s" % (file_name), "grep MJD").split("=")[-1])
                                        self.freq_high_MHz    = np.float64(get_command_output_with_pipe("readfile %s" % (file_name), "grep High").split("=")[-1])
                                        self.freq_low_MHz     = np.float64(get_command_output_with_pipe("readfile %s" % (file_name), "grep Low").split("=")[-1])
                                        self.freq_central_MHz = (self.freq_high_MHz + self.freq_low_MHz)/2.0
                                        print(self.N_samples, self.t_samp_s, self.T_obs_s, self.nbits, self.nchan, self.chanbw_MHz, self.bw_MHz, self.Tstart_MJD, self.freq_high_MHz, self.freq_central_MHz, self.freq_low_MHz)
                                except:
                                        print("WARNING: 'readfile' failed. Trying to use 'header' to get the necessary information...")
                                        
                                        self.N_samples        = np.abs(int(get_command_output("header %s -nsamples" % (self.file_abspath)).split()[-1]  ))
                                        self.t_samp_s         = np.float64(      get_command_output("header %s -tsamp"    % (self.file_abspath)).split()[-1]) * 1.0e-6
                                        self.T_obs_s          = np.float64(get_command_output("header %s -tobs"     % (self.file_abspath)).split()[-1])
                                        self.nbits            = int(get_command_output("header %s -nbits"    % (self.file_abspath)).split()[-1])
                                        self.nchan            = int(get_command_output("header %s -nchans"   % (self.file_abspath)).split()[-1])
                                        self.chanbw_MHz       = np.fabs(np.float64(get_command_output("header %s -foff"     % (self.file_abspath)).split()[-1]))
                                        self.bw_MHz = self.chanbw_MHz*self.nchan
                                        self.backend = get_command_output("header %s -machine" % (self.file_abspath)).split()[-1]
                                        self.Tstart_MJD              = np.float64(get_command_output("header %s -tstart"   % (self.file_abspath)).split()[-1])
                                        self.freq_high_MHz    = np.float64(get_command_output("header %s -fch1"     % (self.file_abspath)).split()[-1]) + 0.5*self.chanbw_MHz
                                        self.freq_central_MHz = self.freq_high_MHz - 0.5*self.bw_MHz
                                        self.freq_low_MHz = self.freq_high_MHz - self.bw_MHz
                                        
                                        print(self.N_samples, self.t_samp_s, self.T_obs_s, self.nbits, self.nchan, self.chanbw_MHz, self.bw_MHz, self.backend, self.Tstart_MJD, self.freq_high_MHz, self.freq_central_MHz, self.freq_low_MHz)


                if data_type =="psrfits":
                        if verbosity_level >= 2:
                                print("Reading PSRFITS....")
                        if psrfits.is_PSRFITS(file_name) == True:
                                if verbosity_level >= 2:
                                        print("File '%s' correctly recognized as PSRFITS" % (file_name))
                                object_file = psrfits.PsrfitsFile(self.file_abspath)
                                self.bw_MHz = object_file.specinfo.BW
                                self.N_samples = object_file.specinfo.N
                                self.T_obs_s = object_file.specinfo.T
                                self.backend = object_file.specinfo.backend
                                self.nbits = object_file.specinfo.bits_per_sample
                                self.date_obs = object_file.specinfo.date_obs
                                self.dec_deg = object_file.specinfo.dec2000
                                self.dec_str = object_file.specinfo.dec_str
                                self.chanbw_MHz = object_file.specinfo.df
                                self.t_samp_s = object_file.specinfo.dt
                                self.freq_central_MHz = object_file.specinfo.fctr
                                self.receiver = object_file.specinfo.frontend
                                self.freq_high_MHz = object_file.specinfo.hi_freq
                                self.freq_low_MHz = object_file.specinfo.lo_freq
                                self.MJD_int = object_file.specinfo.mjd
                                self.MJD_sec = object_file.specinfo.secs
                                self.Tstart_MJD = self.MJD_int + np.float64(self.MJD_sec/86400.)
                                self.nchan = object_file.specinfo.num_channels
                                self.observer = object_file.specinfo.observer
                                self.project = object_file.specinfo.project_id
                                self.ra_deg = object_file.specinfo.ra2000
                                self.ra_str = object_file.specinfo.ra_str
                                self.seconds_of_day = object_file.specinfo.secs
                                self.source_name = object_file.specinfo.source
                                self.telescope = object_file.specinfo.telescope

                        else:
                                print("Reading PSRFITS (header only)....")
                                self.bw_MHz = np.float64(get_command_output("vap -n -c bw %s" % (file_name)).split()[-1])
                                self.N_samples = np.float64(get_command_output_with_pipe("readfile %s" % (file_name), "grep Spectra").split("=")[-1])
                                self.T_obs_s = np.float64(get_command_output("vap -n -c length %s" % (file_name)).split()[-1])
                                self.backend = get_command_output("vap -n -c backend %s" % (file_name)).split()[-1]
                                self.nbits = int(get_command_output_with_pipe("readfile %s" % (file_name), "grep bits").split("=")[-1])
                                self.chanbw_MHz = np.float64(get_command_output_with_pipe("readfile %s" % (file_name), "grep Channel").split("=")[-1])
                                self.t_samp_s = np.float64(get_command_output("vap -n -c tsamp %s" % (file_name)).split()[-1])
                                self.freq_central_MHz = np.float64(get_command_output("vap -n -c freq %s" % (file_name)).split()[-1])
                                self.receiver = get_command_output("vap -n -c rcvr %s" % (file_name)).split()[-1]
                                self.freq_high_MHz = np.float64(get_command_output_with_pipe("readfile %s" % (file_name), "grep High").split("=")[-1])
                                self.freq_low_MHz = np.float64(get_command_output_with_pipe("readfile %s" % (file_name), "grep Low").split("=")[-1])
                                self.nchan = int(get_command_output("vap -n -c nchan %s" % (file_name)).split()[-1])
                                self.MJD_int = int(get_command_output("psrstat -Q -c ext:stt_imjd %s" % (file_name)).split()[-1])
                                self.MJD_sec_int = int(get_command_output("psrstat -Q -c ext:stt_smjd %s" % (file_name)).split()[-1])
                                self.MJD_sec_frac = np.float64(get_command_output("psrstat -Q -c ext:stt_offs %s" % (file_name)).split()[-1])
                                self.MJD_sec = self.MJD_sec_int + self.MJD_sec_frac
                                self.Tstart_MJD       = self.MJD_int + np.float64(self.MJD_sec/86400.)


def execute_and_log_in_thread_pool(command, log_dir, work_dir, id_num, N_ids, flag_log=1):
       datetime_start = (datetime.datetime.now()).strftime("%Y/%m/%d  %H:%M")
       datetime_start_single_string = (datetime.datetime.now()).strftime("%Y%m%d_%H%M")
       time_start = time.time()
       if "/" in command.split()[0]:
              command_label = command.split("/")[-1].split()[0]
       else:
              command_label = command.split()[0]
       list_for_Popen = command.split()

       for i in range(len(list_for_Popen)):
              current_piece = list_for_Popen[i]

              if "?" in current_piece or "*" in current_piece:
                     new_list_for_Popen = list_for_Popen[:i] + sorted(glob.glob(current_piece)) + list_for_Popen[i+1:]
                     list_for_Popen = new_list_for_Popen

       if flag_log == 1:
               log_filename = "LOG_%s_%s_%03d.txt" % (command_label, datetime_start_single_string, int(id_num))
               log_abspath = os.path.join(log_dir, log_filename)
               log_file = open(log_abspath, "w+")
               log_file.write("****************************************************************\n")
               log_file.write("START DATE AND TIME: %s\n" % (datetime_start))
               log_file.write("\nCOMMAND:\n")
               log_file.write("%s\n\n" % (command))
               log_file.write("WORKING DIRECTORY: %s\n" % (work_dir))
               log_file.write("****************************************************************\n")
               log_file.flush()
               proc = subprocess.Popen(list_for_Popen, cwd=work_dir, stdout=log_file, stderr=log_file)
       elif flag_log == 0:
               print("Not logging..")
               proc = subprocess.Popen(list_for_Popen, cwd=work_dir, stdout=subprocess.PIPE)
       proc.communicate()

       datetime_end = (datetime.datetime.now()).strftime("%Y/%m/%d  %H:%M")
       time_end = time.time()

       log_file.write("\nEND DATE AND TIME: %s\n" % (datetime_end))
       log_file.write("\nTOTAL TIME TAKEN: %d s\n" % (time_end - time_start))
       log_file.close()
       print("Command %4d/%d ('%s') complete." % (id_num+1, N_ids, command_label)); sys.stdout.flush()

def execute_and_log(command, work_dir, log_abspath, dict_envs={}, flag_append=0, verbosity_level=0):
        datetime_start = (datetime.datetime.now()).strftime("%Y/%m/%d  %H:%M")
        time_start = time.time()
        if flag_append == 1:
                flag_open_mode = "a"
        else:
                flag_open_mode = "w+"
        log_file = open("%s" % (log_abspath), flag_open_mode)
        executable = command.split()[0]

        log_file.write("****************************************************************\n")
        log_file.write("START DATE AND TIME: %s\n" % (datetime_start))
        log_file.write("\nCOMMAND:\n")
        log_file.write("%s\n\n" % (command))
        log_file.write("WORKING DIRECTORY: %s\n" % (work_dir))
        log_file.write("****************************************************************\n")
        log_file.flush()

        list_for_Popen = command.split()
        env_subprocess = os.environ.copy()
        if dict_envs:  # If the dictionary is not empty                                                                                                                                                            
                for k in list(dict_envs.keys()):
                        env_subprocess[k] = dict_envs[k]

        proc = subprocess.Popen(list_for_Popen, stdout=log_file, stderr=log_file, cwd=work_dir, env=env_subprocess)
        proc.communicate()  # Wait for the process to complete                                                                                                                                                    

        datetime_end = (datetime.datetime.now()).strftime("%Y/%m/%d  %H:%M")
        time_end = time.time()

        if verbosity_level >= 1:
                print("execute_and_log:: COMMAND: %s" % (command))
                print("execute_and_log:: which %s: " % (executable), get_command_output("which %s" % (executable)))
                print("execute_and_log:: WORKING DIRECTORY = ", work_dir)
                print("execute_and_log:: CHECK LOG WITH: \"tail -f %s\"" % (log_abspath))
                sys.stdout.flush()
                print("execute_and_log: list_for_Popen = ", list_for_Popen)
                print("execute_and_log: log_file        = ", log_file)
                print("execute_and_log: env_subprocess = ", env_subprocess)

        log_file.write("\nEND DATE AND TIME: %s\n" % (datetime_end))
        log_file.write("\nTOTAL TIME TAKEN: %d s\n" % (time_end - time_start))
        log_file.close()


def sift_candidates(work_dir, log_dir, LOG_basename,  dedispersion_dir, observation_basename, segment_label, chunk_label, list_zmax, jerksearch_zmax, jerksearch_wmax, flag_remove_duplicates, flag_DM_problems, flag_remove_harmonics, minimum_numDMs_where_detected, minimum_acceptable_DM=2.0, period_to_search_min_s=0.001, period_to_search_max_s=15.0, verbosity_level=0):
        work_dir_basename = os.path.basename(work_dir)
        string_ACCEL_files_dir = os.path.join(dedispersion_dir, observation_basename, segment_label, chunk_label)

        best_cands_filename = "%s/best_candidates_%s.siftedcands" % (work_dir, work_dir_basename)
        if verbosity_level >= 3:
                print("sift_candidates:: best_cands_filename = %s" % (best_cands_filename))
                print("sift_candidates:: string_ACCEL_files_dir = %s" % (string_ACCEL_files_dir))

        list_ACCEL_files = []
        for z in list_zmax:
                string_glob = "%s/*ACCEL_%d" % (string_ACCEL_files_dir, z)
                if verbosity_level >= 1:
                        print("Reading files '%s'..." % (string_glob), end=' ')
                list_ACCEL_files = list_ACCEL_files + glob.glob(string_glob)
                if verbosity_level >= 1:
                        print("done!")

        string_glob_jerk_files = "%s/*ACCEL_%d_JERK_%d" % (string_ACCEL_files_dir, jerksearch_zmax, jerksearch_wmax)
        if verbosity_level >= 3:
                print("JERK: Also reading files '%s'.." % (string_glob_jerk_files))
                print("Found: ", glob.glob(string_glob_jerk_files))

        list_ACCEL_files = list_ACCEL_files + glob.glob(string_glob_jerk_files)

        if verbosity_level >= 3:
                print()
                print("ACCEL files found: ", list_ACCEL_files)
        log_abspath = "%s/LOG_%s.txt" % (log_dir, LOG_basename)
        if verbosity_level >= 1:
                print("\033[1m >> TIP:\033[0m Check sifting output with '\033[1mcat %s\033[0m'" % (log_abspath))

        list_DMs = [x.split("_ACCEL")[0].split("DM")[-1] for x in list_ACCEL_files]
        candidates = sifting.read_candidates(list_ACCEL_files, track=True)

        if verbosity_level >= 3:
                print("sift_candidates:: z = %d" % (z))
                print("sift_candidates:: %s/*ACCEL_%d" % (string_ACCEL_files_dir, z))
                print("sift_candidates:: list_ACCEL_files = %s" % (list_ACCEL_file))
                print("sift_candidates:: list_DMs = %s" % (list_DMs))
                print("sift_candidates:: candidates.cands = ", candidates.cands)
                print("sift_candidates:: Original N_cands = ", len(candidates.cands))
                print("sift_candidates:: sifting.sigma_threshold = ", sifting.sigma_threshold)

        sifting.short_period = period_to_search_min_s
        sifting.long_period = period_to_search_max_s
        print()
        print("Selecting candidates with periods %.4f < P < %.4f seconds..." % (period_to_search_min_s, period_to_search_max_s), end=' ')
        sys.stdout.flush()
        candidates.reject_shortperiod()
        candidates.reject_longperiod()
        print("done!")


        if len(candidates.cands) >= 1:

                if flag_remove_duplicates ==1: 
                        candidates = sifting.remove_duplicate_candidates(candidates)
                        if verbosity_level >= 1:
                                print("sift_candidates:: removed duplicates. N_cands = ", len(candidates.cands))
                if flag_DM_problems==1:
                        candidates = sifting.remove_DM_problems(candidates, minimum_numDMs_where_detected, list_DMs, minimum_acceptable_DM)
                        if verbosity_level >= 1:
                                print("sift_candidates:: removed DM probems. N_cands = ", len(candidates.cands))
                if flag_remove_harmonics==1:
                        try:
                                candidates = sifting.remove_harmonics(candidates)
                        except:
                                pass
                        if verbosity_level >= 1:
                                print("sift_candidates:: removed harmonics. N_cands = ", len(candidates.cands))
        # else:
        #        print "sift_candidates:: ERROR: len(candidates.cands) < 1!!! candidates = %s" % (candidates)
        #        exit()


        if verbosity_level >= 1:
                print("sift_candidates:: Sorting the candidates by sigma...", end=' ') ; sys.stdout.flush()
        try:
                candidates.sort(sifting.cmp_sigma)              # If using PRESTO 2.1's sifting.py
        except AttributeError:
                candidates.sort(key=sifting.attrgetter('sigma'), reverse=True)  # If using PRESTO 3's sifting.py

        if verbosity_level >= 1:
                print("done!")

        if verbosity_level >= 1:
                print("sift_candidates:: Writing down the best candidates on file '%s'..." % (best_cands_filename), end=' ') ; sys.stdout.flush()
        sifting.write_candlist(candidates, best_cands_filename)
        if verbosity_level >= 1:
                print("done!")

        if verbosity_level >= 1:
                print("sift_candidates:: writing down report on file '%s'..." % (log_abspath), end=' ') ; sys.stdout.flush()
        candidates.write_cand_report(log_abspath)
        if verbosity_level >= 1:
                print("done!")

        return candidates


def fold_candidate(work_dir, log_dir, LOG_basename, observation, dir_dedispersion, obs, seg, ck, candidate, ignorechan_list, other_flags_prepfold="", presto_env=os.environ['PRESTO'], verbosity_level=0, flag_LOG_append=1, what_fold="rawdata", num_simultaneous_prepfolds=1):
        log_abspath = "%s/LOG_%s.txt" % (log_dir, LOG_basename)
        dict_env = {'PRESTO': presto_env, 'PATH': "%s/bin:%s" % (presto_env, os.environ['PATH']), 'LD_LIBRARY_PATH': "%s/lib:%s" % (presto_env, os.environ['LD_LIBRARY_PATH'])}
        cand = candidate
        dir_accelfile = "%s/%s/%s/%s" % (dir_dedispersion, obs, seg, ck)
        cand_zmax = cand.filename.split("ACCEL_")[-1].split("_JERK")[0]

        if "JERK_" in os.path.basename(cand.filename):
                cand_wmax = cand.filename.split("JERK_")[-1]
                str_zmax_wmax = "z%s_w%s" % (cand_zmax, cand_wmax)
        else:
                str_zmax_wmax = "z%s" % (cand_zmax)

        file_script_fold_name = "script_fold.txt"
        file_script_fold_abspath = "%s/%s" % (work_dir, file_script_fold_name)
        file_script_fold = open(file_script_fold_abspath, "a")

        if ignorechan_list !="":
                flag_ignorechan = "-ignorechan %s " % (ignorechan_list)
        else:
                flag_ignorechan = ""

        if '-nsub' not in other_flags_prepfold:
                other_flags_prepfold = other_flags_prepfold + " -nsub %d" % (observation.nchan)

        if what_fold =="timeseries":
                file_to_fold = os.path.join(dir_dedispersion, obs, seg, ck, cand.filename.split("_ACCEL")[0] + ".dat")
                cmd_prepfold = "prepfold %s -noxwin -accelcand %d -accelfile %s/%s.cand -o ts_fold_%s_%s_%s_DM%.2f_%s   %s" % (other_flags_prepfold, cand.candnum, dir_accelfile, cand.filename, obs, seg, ck, cand.DM, str_zmax_wmax, file_to_fold)
                execute_and_log(cmd_prepfold, work_dir, log_abspath, dict_env, flag_LOG_append)
        elif what_fold =="rawdata":
                file_to_fold = observation.file_abspath
                if seg == "full":
                        cmd_prepfold = "prepfold %s -noxwin -accelcand %d -accelfile %s/%s.cand -dm %.2f %s -mask %s -o raw_fold_%s_%s_%s_DM%.2f_%s    %s" % (other_flags_prepfold, cand.candnum, dir_accelfile, cand.filename, cand.DM, flag_ignorechan, observation.mask, obs, seg, ck, cand.DM, str_zmax_wmax, file_to_fold)
                else:
                        segment_min = np.float64(seg.replace("m", ""))
                        i_chunk = int(ck.replace("ck", ""))
                        T_obs_min = observation.T_obs_s / 60.
                        start_frac = (i_chunk * segment_min) / T_obs_min
                        end_frac = ((i_chunk + 1) * segment_min) / T_obs_min
                        if end_frac > 1:
                                end_frac = 1.0

                        cmd_prepfold = "prepfold %s -start %.5f -end %.5f -noxwin -accelcand %d -accelfile %s/%s.cand -dm %.2f %s -mask %s -o raw_fold_%s_%s_%s_DM%.2f_%s    %s" % (other_flags_prepfold, start_frac, end_frac, cand.candnum, dir_accelfile, cand.filename, cand.DM, flag_ignorechan, observation.mask, obs, seg, ck, cand.DM, str_zmax_wmax, file_to_fold)

                file_script_fold.write("%s\n" % cmd_prepfold)
                if verbosity_level >= 2:
                        print(cmd_prepfold)

        if verbosity_level >= 2:
                print("fold_candidates:: cand.filename: ",  cand.filename)
                print("file_to_fold = ", file_to_fold)
                print("fold_candidates:: cmd_prepfold = %s" % (cmd_prepfold))

        file_script_fold.close()


def make_even_number(number_int):
        if int(number_int) % 2 == 1:
                return int(number_int)-1
        elif int(number_int) % 2 == 0:
                return int(number_int)
        else:
                print("ERROR: make_even_number:: number does not appear neither EVEN nor ODD!")
                exit()


def get_command_output(command, shell_state=False, work_dir=os.getcwd()):
        list_for_Popen = command.split()
        if shell_state ==False:
                proc = subprocess.Popen(list_for_Popen, stdout=subprocess.PIPE, shell=shell_state, cwd=work_dir)
        else:
                proc = subprocess.Popen([command], stdout=subprocess.PIPE, shell=shell_state, cwd=work_dir)
        out, err = proc.communicate()

        return out.decode('ascii')


def get_command_output_with_pipe(command1, command2):
        list_for_Popen_cmd1 = command1.split()
        list_for_Popen_cmd2 = command2.split()

        p1 = subprocess.Popen(list_for_Popen_cmd1, stdout=subprocess.PIPE)
        p2 = subprocess.Popen(list_for_Popen_cmd2, stdin=p1.stdout, stdout=subprocess.PIPE)
        p1.stdout.close()

        out, err = p2.communicate()
        return out.decode('ascii')


def get_rfifind_result(file_mask, LOG_file, verbosity_level=0):
        rfifind_mask = rfifind.rfifind(file_mask)

        N_int = rfifind_mask.nint
        N_chan = rfifind_mask.nchan
        N_int_masked = len(rfifind_mask.mask_zap_ints)
        N_chan_masked = len(rfifind_mask.mask_zap_chans)
        fraction_int_masked = np.float64(N_int_masked/N_int)
        fraction_chan_masked = np.float64(N_chan_masked/N_chan)

        if verbosity_level >= 2:
                print("get_rfifind_result:: file_mask: %s" % file_mask)
                print("get_rfifind_result:: LOG_file: %s" % LOG_file)

        if (fraction_int_masked > 0.5) or (fraction_chan_masked > 0.5):
                return "!Mask>50%"

        # Check if there was a problem with the clipping in first block and get the filename with that problem. Otherwise return True.
        cmd_grep_problem_clipping = "grep -l 'problem with clipping' %s" % (LOG_file)  # -l option returns the name of the file that contains the expression
        cmd_grep_inf_results = "grep -l ' inf ' %s" % (LOG_file)
        output = get_command_output(cmd_grep_problem_clipping, True).strip()
        if output != "":
                if verbosity_level >= 1:
                        print()
                        print("WARNING: File '%s' contains a problem with clipping in first block!" % (LOG_file))
                return "!ProbFirstBlock"

        output = get_command_output(cmd_grep_inf_results, True).strip()
        if output != "":
                if verbosity_level >= 1:
                        print()
                        print("WARNING: File '%s' contains an infinite result!" % (LOG_file))
                return "!ProbInfResult"

        return "done"


def check_prepdata_result(LOG_file, verbosity_level=0):
        # Check if there was a problem with the clipping in first block and get the filename with that problem. Otherwise return True.
        cmd_grep_problem_clipping = "grep -l 'problem with clipping' %s" % (LOG_file)  # -l option returns the name of the file that contains the expression
        cmd_grep_inf_results = "grep -l ' inf ' %s" % (LOG_file)
        output = get_command_output(cmd_grep_problem_clipping, True).strip()
        print("check_prepdata_result::output: -%s-" % (output))
        if output != "":
                if verbosity_level >= 1:
                        print("WARNING: File '%s' contains a problem with clipping in first block!" % (LOG_file))
                return False

        return True


def check_rfifind_outfiles(out_dir, basename, verbosity_level=0):
        for suffix in ["bytemask", "inf", "mask", "ps", "rfi", "stats"]:
                file_to_check = "%s/%s_rfifind.%s" % (out_dir, basename, suffix)
                if not os.path.exists(file_to_check):
                        if verbosity_level >= 1:
                                print("ERROR: file %s not found!" % (file_to_check))
                        return False
                elif os.stat(file_to_check).st_size == 0:  # If the file has size 0 bytes
                        print("ERROR: file %s has size 0!" % (file_to_check))
                        return False
        return True


def check_rednoise_outfiles(fftfile_rednoise_abspath, verbosity_level=0):
        inffile_rednoise_abspath = fftfile_rednoise_abspath.replace("_red.fft", "_red.inf")

        if os.path.exists(fftfile_rednoise_abspath ) and (os.path.getsize(fftfile_rednoise_abspath) > 0) and os.path.exists(inffile_rednoise_abspath) and (os.path.getsize(inffile_rednoise_abspath) > 0):
                return True
        else:
                return False


def check_accelsearch_result(fft_infile, zmax, verbosity_level=0):
        fft_infile_nameonly = os.path.basename(fft_infile)
        fft_infile_basename = os.path.splitext(fft_infile_nameonly)[0]

        if verbosity_level >= 2:
                print("check_accelsearch_result:: infile_basename: ", fft_infile_basename)
                print("check_accelsearch_result:: ACCEL_filename = ", ACCEL_filename)
                print("check_accelsearch_result:: ACCEL_cand_filename", ACCEL_cand_filename)
                print("check_accelsearch_result:: ACCEL_txtcand_filename = ", ACCEL_txtcand_filename)

        ACCEL_filename =  fft_infile.replace(".fft", "_ACCEL_%d" % (zmax))
        ACCEL_cand_filename =  fft_infile.replace(".fft", "_ACCEL_%d.cand" % (zmax))
        ACCEL_txtcand_filename =  fft_infile.replace(".fft", "_ACCEL_%d.txtcand" % (zmax))

        try:
                if (os.path.getsize(ACCEL_filename) > 0) and (os.path.getsize(ACCEL_cand_filename) > 0) and (os.path.getsize(ACCEL_txtcand_filename) > 0):
                        result_message = "check_accelsearch_result:: Files exist and their size is > 0! Skipping..."
                        check_result = True
                else:
                        result_message = "check_accelsearch_result:: Files exists but at least one of them has size = 0!"
                        check_result = False
        except OSError:
                result_message = "check_accelsearch_result:: OSError: It seems accelsearch has not been executed!"
                check_result = False

        if verbosity_level >= 1:
                print(result_message)

        return check_result


def check_jerksearch_result(fft_infile, zmax, wmax, verbosity_level=0):
        fft_infile_nameonly = os.path.basename(fft_infile)
        fft_infile_basename = os.path.splitext(fft_infile_nameonly)[0]

        if verbosity_level >= 1:
                print("check_jerksearch_result:: infile_basename: ", fft_infile_basename)
                print("check_jerksearch_result:: ACCEL_filename = ", ACCEL_filename)
                print("check_jerksearch_result:: ACCEL_cand_filename", ACCEL_cand_filename)
                print("check_jerksearch_result:: ACCEL_txtcand_filename = ", ACCEL_txtcand_filename)

        ACCEL_filename =  fft_infile.replace(".fft", "_ACCEL_%d_JERK_%d"          % (zmax, wmax))
        ACCEL_cand_filename =  fft_infile.replace(".fft", "_ACCEL_%d_JERK_%d.cand"     % (zmax, wmax))
        ACCEL_txtcand_filename =  fft_infile.replace(".fft", "_ACCEL_%d_JERK_%d.txtcand"  % (zmax, wmax))

        try:
                if (os.path.getsize(ACCEL_filename) > 0) and (os.path.getsize(ACCEL_cand_filename) > 0) and (os.path.getsize(ACCEL_txtcand_filename) > 0):
                        result_message = "check_jerksearch_result:: Files exist and their size is > 0! Skipping..."
                        check_result = True
                else:
                        result_message = "check_jerksearch_result:: Files exists but at least one of them has size = 0!"
                        check_result = False
        except OSError:
                result_message = "check_jerksearch_result:: OSError: It seems jerksearch has not been executed!"
                check_result = False

        if verbosity_level >= 1:
                print(result_message)

        return check_result


def accelsearch(infile, work_dir, log_abspath, numharm=8, zmax=0, other_flags="", dict_env= {}, verbosity_level=0, flag_LOG_append=1):
        infile_nameonly = os.path.basename(infile)
        infile_basename = os.path.splitext(infile_nameonly)[0]
        inffile_empty = infile.replace(".fft", "_ACCEL_%d_empty" % (zmax))

        cmd_accelsearch = "accelsearch %s -zmax %s -numharm %s %s" % (other_flags, zmax, numharm, infile)

        if verbosity_level >= 2:
                print()
                print("BEGIN ACCELSEARCH ----------------------------------------------------------------------")

                print("accelsearch:: cmd_accelsearch: ", cmd_accelsearch)
                print("accelsearch:: ENV: ", dict_env)
                print("accelsearch:: check_accelsearch_result(infile, int(zmax)) :: %s" % (check_accelsearch_result(infile, int(zmax))))
                print("accelsearch:: work_dir = %s" % (work_dir))
                print("accelsearch:: infile = %s" % (infile))

        if check_accelsearch_result(infile, int(zmax)) == False and check_accelsearch_result(inffile_empty, int(zmax)) == False:
                if verbosity_level >= 2:
                        print("accelsearch:: running: %s" % (cmd_accelsearch))
                execute_and_log(cmd_accelsearch, work_dir, log_abspath, dict_env, flag_LOG_append)
        else:
                if verbosity_level >= 2:
                        print("accelsearch:: WARNING: accelsearch with zmax=%d seems to have been already executed on file %s. Skipping..." % (int(zmax), infile_nameonly))

        if verbosity_level >= 2:
                print("accelsearch:: NOW I CHECK THE RESULT OF THE EXECUTION!")

        if check_accelsearch_result(infile, int(zmax)) == False:
                if verbosity_level >= 2:
                        print("False! Then I create a _empty file!")
                file_empty = open(inffile_empty, "w")
                if verbosity_level >=1:
                        print("%sWARNING%s: accelsearch did not produce any candidates! Writing file %s to signal this..." % (colors.WARNING+colors.BOLD, colors.ENDCOLOR, inffile_empty), end='')
                file_empty.write("ACCELSEARCH DID NOT PRODUCE ANY CANDIDATES!")
        else:
                if verbosity_level >= 2:
                        print("accelsearch: GOOD! CANDIDATES HAVE BEEN PRODUCED for %s!" % (infile))

        if verbosity_level >= 2:
                print("END ACCELSEARCH ---------------------------------------------------------------------- ")




def jerksearch(infile, work_dir, log_abspath, numharm=4, zmax=50, wmax=150, other_flags="", dict_env={}, verbosity_level=0, flag_LOG_append=1):
        infile_nameonly = os.path.basename(infile)
        infile_basename = os.path.splitext(infile_nameonly)[0]
        inffile_empty = infile.replace(".fft", "_ACCEL_%d_JERK_%d_empty" % (zmax, wmax))
        sys.stdout.flush()
        cmd_jerksearch = "accelsearch %s -zmax %d -wmax %d -numharm %d %s" % (other_flags, zmax, wmax, numharm, infile)

        if verbosity_level >= 2:
                print()
                print("BEGIN JERKSEARCH ----------------------------------------------------------------------")

                print("jerksearch:: cmd_jerksearch: ", cmd_jerksearch)
                print("jerksearch:: AND THIS IS THE ENV: ", dict_env)
                print("jerksearch:: check_accelsearch_result(infile, int(zmax)) :: %s" % (check_accelsearch_result(infile, int(zmax))))
                print("jerksearch:: work_dir = %s" % (work_dir))
                print("jerksearch:: infile = %s" % (infile))

        if check_jerksearch_result(infile, zmax, wmax) == False and check_jerksearch_result(inffile_empty, zmax, wmax) == False:
                if verbosity_level >= 2:
                        print("jerksearch:: executing: %s" % (cmd_jerksearch))
                execute_and_log(cmd_jerksearch, work_dir, log_abspath, dict_env, flag_LOG_append)
        else:
                if verbosity_level >= 2:
                        print("jerksearch:: WARNING: jerk search with zmax=%d and wmax=%s seems to have been already executed on file %s. Skipping..." % (int(zmax), int(wmax), infile_nameonly))

        if verbosity_level >= 2:
                print("jerksearch:: NOW I CHECK THE RESULT OF THE EXECUTION!")

        if check_jerksearch_result(infile, zmax, wmax) == False:
                if verbosity_level >= 2:
                        print("False! Then I create a _empty file!")
                file_empty = open(inffile_empty, "w")
                if verbosity_level >=1:
                        print("%sWARNING%s: jerk search did not produce any candidates! Writing file %s to signal this..." % (colors.WARNING+colors.BOLD, colors.ENDCOLOR, inffile_empty))
                file_empty.write("JERK SEARCH DID NOT PRODUCE ANY CANDIDATES!")
        else:
                if verbosity_level >= 2:
                        print("jerksearch:: GOOD! CANDIDATES HAVE BEEN PRODUCED for %s!" % (infile))

        if verbosity_level >= 2:
                print("END JERKSEARCH ---------------------------------------------------------------------- ")


def split_into_chunks(infile, list_datfiles_to_split, log_dir, LOG_basename,  work_dir, segment_min, i_chunk, list_zmax, flag_jerk_search, jerksearch_zmax, jerksearch_wmax, presto_env=os.environ['PRESTO'], flag_LOG_append=1, flag_remove_datfiles_of_segments=0 ):
        segment_length_s = segment_min * 60
        dict_env = {'PRESTO': presto_env, 'PATH': "%s/bin:%s" % (presto_env, os.environ['PATH']), 'LD_LIBRARY_PATH': "%s/lib:%s" % (presto_env, os.environ['LD_LIBRARY_PATH'])}

        log_abspath = "%s/LOG_%s.txt" % (log_dir, LOG_basename)

        observation_filename = os.path.basename(infile)
        observation_basename = os.path.splitext(observation_filename)[0]
        
        str_seg = int(segment_min)
        for datfile_name in sorted(list_datfiles_to_split):
                datfile_nameonly = os.path.basename(datfile_name)               # myfile_DM24.40.dat
                inffile_name = datfile_name.replace(".dat", ".inf")             # myfile_DM24.40.inf
                str_DM = datfile_nameonly.split(".dat")[0].split("DM")[-1]      # 24.40
                
                info_datfile = infodata.infodata(inffile_name)

                
                datfile_chunk_name = "%s/%s_%sm_ck%02d_DM%s.dat" % (work_dir, observation_basename, str_seg, i_chunk, str_DM)
                #print("split_into_chunks:: datfile_chunk_name = ", datfile_chunk_name)
                DM_trial_was_searched = check_if_DM_trial_was_searched(datfile_chunk_name, list_zmax, flag_jerk_search, jerksearch_zmax, jerksearch_wmax)
                #print("split_into_chunks:: DM_trial_was_searched = ", DM_trial_was_searched)
                
                string_min = "%dm" % int(segment_min)
                string_chunk = "ck%02d" % i_chunk
                path_old = os.path.splitext(datfile_name)[0]
                path_new = path_old.replace("full", string_min).replace("ck00", string_chunk)
                new_outfile_name = "%s" % (os.path.basename(path_new))


                if DM_trial_was_searched == False:
                        t_samp_s = info_datfile.dt
                        N_samp = info_datfile.N
                        T_obs_s = t_samp_s * N_samp

                        start_fraction = (i_chunk * segment_length_s)/T_obs_s
                        numout = make_even_number(int(segment_length_s / t_samp_s))

                        
                        cmd_prepdata_split = "prepdata -nobary -o %s/%s -start %.3f -numout %s %s" % (work_dir, new_outfile_name, start_fraction, numout,  datfile_name)

                        output_datfile = "%s/%s.dat" % (work_dir, new_outfile_name)
                        output_inffile = "%s/%s.inf" % (work_dir, new_outfile_name)
                        output_scriptfile = "%s/%s.dat.makecmd" % (work_dir, new_outfile_name)

                        if flag_remove_datfiles_of_segments == 1 and (not os.path.exists(output_scriptfile)):
                                with open(output_scriptfile, 'w') as f:
                                        f.write("%s\n" % (cmd_prepdata_split))
                                os.chmod(output_scriptfile, 0o775)

                        if check_prepdata_outfiles(output_datfile.replace(".dat", "")) == False:
                                if verbosity_level >= 1:        print("Making chunk '%s' of segment '%sm' from '%s'..." % (string_chunk, segment_min, datfile_name), end=''); sys.stdout.flush()
                                execute_and_log(cmd_prepdata_split, work_dir, log_abspath, dict_env, flag_LOG_append)
                                if verbosity_level >= 1:        print("done!")
                        else:
                                if verbosity_level >= 1:
                                        print("NOTE: '%s.dat' already exists. No need to create it again." % (new_outfile_name))
                else:
                        if verbosity_level >= 1:
                                print("NOTE: '%s.dat' was already successfully searched. Skipping..." % (new_outfile_name))


                                
def check_if_DM_trial_was_searched(dat_file, list_zmax, flag_jerk_search, jerksearch_zmax, jerksearch_wmax):
        dat_file_nameonly = os.path.basename(dat_file)
        fft_file = dat_file.replace(".dat", ".fft")
        fft_file_nameonly = os.path.basename(fft_file)

        for z in list_zmax:
                ACCEL_filename          = dat_file.replace(".dat", "_ACCEL_%s" % (int(z)))
                ACCEL_filename_empty = dat_file.replace(".dat", "_ACCEL_%s_empty" % (int(z)))
                ACCEL_cand_filename = ACCEL_filename + ".cand"
                ACCEL_txtcand_filename = ACCEL_filename + ".txtcand"

                #print("check_if_DM_trial_was_searched:: checking: %s, %s, %s" % (ACCEL_filename, ACCEL_cand_filename, ACCEL_txtcand_filename))
                #print("check_if_DM_trial_was_searched:: checking: %s, %s, %s" % (ACCEL_filename_empty, ACCEL_cand_filename, ACCEL_txtcand_filename))

                
                if (not os.path.exists(ACCEL_filename)       or os.path.getsize(ACCEL_filename) ==0        ) and \
                   (not os.path.exists(ACCEL_filename_empty) or os.path.getsize(ACCEL_filename_empty) ==0  ):
                        return False
                if (not os.path.exists(ACCEL_cand_filename) or os.path.getsize(ACCEL_cand_filename) ==0) and \
                   (not os.path.exists(ACCEL_filename_empty) or os.path.getsize(ACCEL_filename_empty) ==0  ):
                        return False
                if not os.path.exists(ACCEL_txtcand_filename):
                        return False

        if flag_jerk_search == 1 and jerksearch_wmax > 0:
                ACCEL_filename          = dat_file.replace(".dat", "_ACCEL_%s_JERK_%s" % (jerksearch_zmax, jerksearch_wmax))
                ACCEL_filename_empty = dat_file.replace(".dat", "_ACCEL_%s_JERK_%s_empty" % (jerksearch_zmax, jerksearch_wmax))
                ACCEL_cand_filename = ACCEL_filename + ".cand"
                ACCEL_txtcand_filename = ACCEL_filename + ".txtcand"
                # print "check_if_DM_trial_was_searched:: checking: %s, %s, %s" % (ACCEL_filename, ACCEL_cand_filename, ACCEL_txtcand_filename)
                if (not os.path.exists(ACCEL_filename)       or os.path.getsize(ACCEL_filename) ==0        ) and \
                   (not os.path.exists(ACCEL_filename_empty) or os.path.getsize(ACCEL_filename_empty) ==0  ):
                        return False
                if (not os.path.exists(ACCEL_cand_filename) or os.path.getsize(ACCEL_cand_filename) ==0) and \
                   (not os.path.exists(ACCEL_filename_empty) or os.path.getsize(ACCEL_filename_empty) ==0  ):
                        return False
                if not os.path.exists(ACCEL_txtcand_filename):
                        return False

        return True


def singlepulse_search(work_dir, log_dir, LOG_basename, list_files_to_search, singlepulse_search_flags, num_simultaneous_singlepulse_searches, presto_env=os.environ['PRESTO'], verbosity_level=1, flag_singlepulse_search=1):
        if verbosity_level >= 2:
                print()
                print("BEGIN SINGLE PULSE SEARCH ----------------------------------------------------------------------")

        N_files_to_search = len(list_files_to_search)

        log_abspath = os.path.join(log_dir, LOG_basename+".txt")
        dict_env = {'PRESTO': presto_env, 'PATH': "%s/bin:%s" % (presto_env, os.environ['PATH']), 'LD_LIBRARY_PATH': "%s/lib:%s" % (presto_env, os.environ['LD_LIBRARY_PATH'])}
        if num_simultaneous_singlepulse_searches == 1:
                for i in range(N_files_to_search):
                        dat_file = list_files_to_search[i]
                        cmd_singlepulse_search = "single_pulse_search.py --noplot %s %s" % (singlepulse_search_flags, dat_file)
                        print("Running '%s'... " % cmd_singlepulse_search, end=""); sys.stdout.flush()
                        execute_and_log(cmd_singlepulse_search, work_dir, log_abspath, dict_env, 0)
                        print("done!"); sys.stdout.flush()
        elif num_simultaneous_singlepulse_searches >= 2 and flag_singlepulse_search == 1:
                list_singlepulse_search_commands = []
                list_singlepulse_search_workdirs = []
                
                print("\nSingle-pulse search with multiple CPUs active")

                for i in range(N_files_to_search):
                        print()
                        if verbosity_level >= 2:
                                print("singlepulse_search: inside loop with i = %d / %d" % (i, N_files_to_search-1))
                        dat_file = list_files_to_search[i]
                        dat_file_nameonly = os.path.basename(dat_file)
                        
                        #DM_trial_was_searched = check_if_DM_trial_was_searched(dat_file, list_zmax, flag_jerk_search, jerksearch_zmax, jerksearch_wmax)


                        
                        cmd_singlepulse_search = "single_pulse_search.py --noplot %s %s" % (singlepulse_search_flags, dat_file)
                        list_singlepulse_search_commands.append(cmd_singlepulse_search)
                        list_singlepulse_search_workdirs.append(work_dir)
                print()


                TP = ThreadPool(num_simultaneous_singlepulse_searches)
                N_commands = len(list_singlepulse_search_commands)
                print()
                print("Now doing parallelized single-pulse search using %d CPUs..." % num_simultaneous_singlepulse_searches);  sys.stdout.flush()
                print()
                for k in range(len(list_singlepulse_search_commands)):
                        print("Queing single-pulse search command #%d: '%s'" % (k+1, list_singlepulse_search_commands[k]))
                        time.sleep(0.1)
                        TP.apply_async(execute_and_log_in_thread_pool, (list_singlepulse_search_commands[k], log_dir, list_singlepulse_search_workdirs[k], k, N_commands, 1) )
                print("\n")
                print("Running %d single-pulse search commands at once..." % (num_simultaneous_singlepulse_searches)); sys.stdout.flush()
                TP.close()
                TP.join()
                print()
                print("%d commands completed!" % (len(list_singlepulse_search_commands)))


        print("Making final single-pulse plot with 'single_pulse_search.py *.singlepulse'...", end=''); sys.stdout.flush()
        os.system('single_pulse_search.py *.singlepulse')
        print("done!"); sys.stdout.flush()


        

def periodicity_search_FFT(work_dir, log_dir, LOG_basename, zapfile, segment_label, chunk_label, list_seg_ck_indices, list_DD_scheme, flag_use_cuda=0, list_cuda_ids=[0], flag_acceleration_search=1, numharm=8, list_zmax=[20], flag_jerk_search=1, jerksearch_zmax=0, jerksearch_wmax=0, jerksearch_numharm=4, num_simultaneous_jerksearches=1, period_to_search_min_s=0.001, period_to_search_max_s=20.0, other_flags_accelsearch="", flag_remove_fftfiles=0, flag_remove_datfiles_of_segments=0, presto_env_zmax_0=os.environ['PRESTO'], presto_env_zmax_any=os.environ['PRESTO'], verbosity_level=0, flag_LOG_append=1, dict_flag_steps= {'flag_step_dedisperse': 1 , 'flag_step_realfft': 1, 'flag_step_periodicity_search': 1}):

        i_seg, N_seg, i_ck, N_ck = list_seg_ck_indices

        if verbosity_level >= 2:
                print("periodicity_search_FFT:: Files to search: ", "%s/*DM*.*.dat" % (work_dir))
                print("periodicity_search_FFT:: presto_env_zmax_0 = ", presto_env_zmax_0)
                print("periodicity_search_FFT:: presto_env_zmax_any = ", presto_env_zmax_any)

        list_files_to_search = sorted([x for x in glob.glob("%s/*DM*.*.dat" % (work_dir)) ])

        N_DMs_to_search = 0
        for k in range(len(list_DD_scheme)):
                N_DMs_to_search = N_DMs_to_search + list_DD_scheme[k]['num_DMs']
        
        N_files_to_search = len(list_files_to_search)
        N_files_searched  = N_DMs_to_search - N_files_to_search
        
        frequency_to_search_max = 1./period_to_search_min_s
        frequency_to_search_min = 1./period_to_search_max_s
        if verbosity_level >= 2:
                print("frequency_to_search_min, ", frequency_to_search_min)
                print("frequency_to_search_max, ", frequency_to_search_max)

                print("periodicity_search_FFT:: WARNING: -flo and -fhi CURRENTLY DISABLED")
        dict_env_zmax_0 = {'PRESTO': presto_env_zmax_0,   'PATH': "%s/bin:%s" % (presto_env_zmax_0, os.environ['PATH']),   'LD_LIBRARY_PATH': "%s/lib:%s" % (presto_env_zmax_0,   os.environ['LD_LIBRARY_PATH'])}
        dict_env_zmax_any = {'PRESTO': presto_env_zmax_any, 'PATH': "%s/bin:%s" % (presto_env_zmax_any, os.environ['PATH']), 'LD_LIBRARY_PATH': "%s/lib:%s" % (presto_env_zmax_any, os.environ['LD_LIBRARY_PATH'])}

        if verbosity_level >= 2:
                print("periodicity_search_FFT:: dict_env_zmax_0 = ", dict_env_zmax_0)
                print("periodicity_search_FFT:: dict_env_zmax_any = ", dict_env_zmax_any)
                print("periodicity_search_FFT:: LOG_basename = ", LOG_basename)
                print("periodicity_search_FFT:: list_files_to_search = ", list_files_to_search)

        log_abspath = "%s/LOG_%s.txt" % (log_dir, LOG_basename)
        if verbosity_level >= 1:
                print()
                print("\033[1m >> TIP:\033[0m Follow periodicity search with: \033[1mtail -f %s\033[0m" % (log_abspath))

        zapfile_nameonly = os.path.basename(zapfile)

        #########################################################################################################
        #                                     NON-PARALLELIZED JERK SEARCH
        #########################################################################################################
        if num_simultaneous_jerksearches == 1 or jerksearch_wmax == 0 or flag_jerk_search == 0:
                for i in range(N_files_to_search):
                        print()
                        if verbosity_level >= 2:
                                print("periodicity_search_FFT: inside loop with i = %d / %d" % (i, N_files_to_search-1))
                        dat_file = list_files_to_search[i]
                        dat_file_nameonly = os.path.basename(dat_file)
                        fft_file = dat_file.replace(".dat", ".fft")
                        fft_file_nameonly = os.path.basename(fft_file)

                        DM_trial_was_searched = check_if_DM_trial_was_searched(dat_file, list_zmax, flag_jerk_search, jerksearch_zmax, jerksearch_wmax)
                        #print("periodicity_search_FFT:: DM_trial_was_searched = ", DM_trial_was_searched)
                        
                        if dict_flag_steps['flag_step_realfft'] == 1:

                                if DM_trial_was_searched == False:
                                        print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - Doing realfft  of %s..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search, dat_file_nameonly), end=' ')
                                        sys.stdout.flush()
                                        realfft(dat_file, work_dir, log_dir, LOG_basename, "", presto_env_zmax_0, 0, flag_LOG_append)
                                        print("done!")
                                        sys.stdout.flush()

                                        if flag_remove_datfiles_of_segments ==1 and (segment_label != "full") and os.path.exists(dat_file):
                                                if verbosity_level >= 1:
                                                        print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - Removing %s to save disk space (use \"%s\" to recreate it)..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search, dat_file_nameonly, dat_file_nameonly+".makecmd"), end=' ')
                                                        sys.stdout.flush()
                                                os.remove(dat_file)
                                                if verbosity_level >= 1:
                                                        print("done!")
                                                        sys.stdout.flush()

                                        print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - Doing rednoise of %s..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search, dat_file_nameonly), end=' ')
                                        sys.stdout.flush()
                                        rednoise(fft_file, work_dir, log_dir, LOG_basename, "", presto_env_zmax_0, verbosity_level)
                                        print("done!")
                                        sys.stdout.flush()

                                        print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - Applying zapfile '%s' to '%s'..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search, zapfile_nameonly, fft_file_nameonly), end=' ')
                                        sys.stdout.flush()
                                        zapped_fft_filename, zapped_inf_filename = zapbirds(fft_file, zapfile, work_dir, log_dir, LOG_basename, presto_env_zmax_0, verbosity_level)
                                        zapped_fft_nameonly = os.path.basename(zapped_fft_filename)
                                        print("done!")
                                        sys.stdout.flush()
                                else:
                                        print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - File '%s' was already successfully searched. Skipping..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search, dat_file_nameonly), end=' '); sys.stdout.flush()
                                        
                        else:
                                print("STEP_REALFFT = 0, skipping realfft, rednoise, zapbirds...")

                        # print "\033[1m >> TIP:\033[0m Follow accelsearch with '\033[1mtail -f %s\033[0m'" % (log_abspath)

                        if dict_flag_steps['flag_step_periodicity_search'] == 1:
                                if DM_trial_was_searched == False:
                                        if flag_acceleration_search == 1:
                                                for z in list_zmax:
                                                        tstart_accelsearch = time.time()
                                                        print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - Doing accelsearch of %s with zmax = %4d..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search, zapped_fft_nameonly, z), end=' ')
                                                        sys.stdout.flush()
                                                        if int(z) == 0:
                                                                dict_env = copy.deepcopy(dict_env_zmax_0)
                                                                if verbosity_level >= 2:
                                                                        print("accelsearch:: zmax == 0 ----> dict_env = %s" % (dict_env))
                                                                flag_cuda = ""
                                                        else:
                                                                if flag_use_cuda == 1:
                                                                        dict_env = copy.deepcopy(dict_env_zmax_any)
                                                                        gpu_id = random.choice(list_cuda_ids)
                                                                        flag_cuda = " -cuda %d " % (gpu_id)
                                                                else:
                                                                        dict_env = copy.deepcopy(dict_env_zmax_0)
                                                                        flag_cuda = ""

                                                                if verbosity_level >= 2:
                                                                        print("periodicity_search_FFT:: zmax == %d ----> dict_env = %s" % (int(z), dict_env))
                                                                        print("periodicity_search_FFT:: Now check CUDA: list_cuda_ids = ", list_cuda_ids)
                                                                        print("periodicity_search_FFT:: flag_use_cuda = ", flag_use_cuda)
                                                                        print("periodicity_search_FFT:: flag_cuda = ", flag_cuda)

                                                        accelsearch_flags = other_flags_accelsearch + flag_cuda  # + " -flo %s -fhi %s" % (frequency_to_search_min, frequency_to_search_max)

                                                        accelsearch(fft_file, work_dir, log_abspath, numharm=numharm, zmax=z, other_flags=accelsearch_flags, dict_env=dict_env, verbosity_level=verbosity_level, flag_LOG_append=flag_LOG_append)
                                                        tend_accelsearch = time.time()
                                                        time_taken_accelsearch_s = tend_accelsearch - tstart_accelsearch
                                                        print("done in %.2f s!" % (time_taken_accelsearch_s))
                                                        sys.stdout.flush()
                                                        ACCEL_filename = fft_file.replace(".fft", "_ACCEL_%s" % (int(z)))

                                        if jerksearch_wmax > 0 and flag_jerk_search == 1:
                                                tstart_jerksearch = time.time()
                                                print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - Doing jerk search of %s with zmax=%d, wmax=%d, numharm=%d..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search, zapped_fft_nameonly, jerksearch_zmax, jerksearch_wmax, jerksearch_numharm), end=' ')
                                                sys.stdout.flush()
                                                flag_cuda = ""
                                                jerksearch_flags = other_flags_accelsearch + flag_cuda
                                                jerksearch(fft_file, work_dir, log_abspath, numharm=jerksearch_numharm, zmax=jerksearch_zmax, wmax=jerksearch_wmax, other_flags=jerksearch_flags, dict_env=dict_env_zmax_0, verbosity_level=verbosity_level, flag_LOG_append=flag_LOG_append)
                                                tend_jerksearch = time.time()
                                                time_taken_jerksearch_s = tend_jerksearch - tstart_jerksearch
                                                print("done in %.2f s!" % (time_taken_jerksearch_s))
                                                sys.stdout.flush()
                                                ACCEL_filename = fft_file.replace(".fft", "_ACCEL_%s_JERK_%s" % (jerksearch_zmax, jerksearch_wmax))

                                else:   
                                        print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - File '%s' was already successfully searched. Skipping..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search, dat_file_nameonly), end=' '); sys.stdout.flush()


                        if flag_remove_fftfiles ==1  and os.path.exists(fft_file):
                                if verbosity_level >= 1:
                                        print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - Removing %s to save disk space..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search, fft_file_nameonly), end=' '); sys.stdout.flush()
                                os.remove(fft_file)
                                if verbosity_level >= 1:
                                        print("done!");  sys.stdout.flush()


        #########################################################################################################
        #                                     PARALLELIZED JERK SEARCH
        # If we are doing a jerk search with multiple CPUs, the scheme will be different
        # We will first realfft, deredden and zap all the dat files, then search all the .fft files in parallel
        #########################################################################################################
        
        elif num_simultaneous_jerksearches >= 2 and jerksearch_wmax > 0 and flag_jerk_search == 1:
                list_jerksearch_commands = []
                list_jerksearch_workdirs = []
                jerksearch_flags = other_flags_accelsearch
                print("\nJerk search with multiple CPUs active")

                for i in range(N_files_to_search):
                        print()
                        if verbosity_level >= 2:
                                print("periodicity_search_FFT: inside loop with i = %d / %d" % (i, N_files_to_search-1))
                        dat_file = list_files_to_search[i]
                        dat_file_nameonly = os.path.basename(dat_file)
                        fft_file = dat_file.replace(".dat", ".fft")
                        fft_file_nameonly = os.path.basename(fft_file)

                        DM_trial_was_searched = check_if_DM_trial_was_searched(dat_file, list_zmax, flag_jerk_search, jerksearch_zmax, jerksearch_wmax)

                        if dict_flag_steps['flag_step_realfft'] == 1:

                                if DM_trial_was_searched == False:
                                        print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - Doing realfft  of %s..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search, dat_file_nameonly), end=' ')
                                        sys.stdout.flush()
                                        realfft(dat_file, work_dir, log_dir, LOG_basename, "", presto_env_zmax_0, 0, flag_LOG_append)
                                        print("done!")
                                        sys.stdout.flush()

                                        if flag_remove_datfiles_of_segments ==1 and (segment_label != "full") and os.path.exists(dat_file):
                                                if verbosity_level >= 1:
                                                        print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - Removing %s to save disk space (use \"%s\" to recreate it)..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search, dat_file_nameonly, dat_file_nameonly+".makecmd"), end=' ')
                                                        sys.stdout.flush()
                                                os.remove(dat_file)
                                                if verbosity_level >= 1:
                                                        print("done!")
                                                        sys.stdout.flush()

                                        print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - Doing rednoise of %s..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search, dat_file_nameonly), end=' ')
                                        sys.stdout.flush()
                                        rednoise(fft_file, work_dir, log_dir, LOG_basename, "", presto_env_zmax_0, verbosity_level)
                                        print("done!")
                                        sys.stdout.flush()

                                        print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - Applying zapfile '%s' to '%s'..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search, zapfile_nameonly, fft_file_nameonly), end=' ')
                                        sys.stdout.flush()
                                        zapped_fft_filename, zapped_inf_filename = zapbirds(fft_file, zapfile, work_dir, log_dir, LOG_basename, presto_env_zmax_0, verbosity_level)
                                        zapped_fft_nameonly = os.path.basename(zapped_fft_filename)
                                        print("done!")
                                        sys.stdout.flush()
                                else:
                                        print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - Already fully searched. Skipping..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search), end=' ')
                        else:
                                print("STEP_REALFFT = 0, skipping realfft, rednoise, zapbirds...")

                        if dict_flag_steps['flag_step_periodicity_search'] == 1:
                                if DM_trial_was_searched == False:
                                        for z in list_zmax:
                                                tstart_accelsearch = time.time()
                                                print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - Doing accelsearch of %s with zmax = %4d..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search, zapped_fft_nameonly, z), end=' ')
                                                sys.stdout.flush()
                                                if int(z) == 0:
                                                        dict_env = copy.deepcopy(dict_env_zmax_0)
                                                        if verbosity_level >= 2:
                                                                print("accelsearch:: zmax == 0 ----> dict_env = %s" % (dict_env))
                                                        flag_cuda = ""
                                                else:
                                                        if flag_use_cuda == 1:
                                                                dict_env = copy.deepcopy(dict_env_zmax_any)
                                                                gpu_id = random.choice(list_cuda_ids)
                                                                flag_cuda = " -cuda %d " % (gpu_id)
                                                        else:
                                                                dict_env = copy.deepcopy(dict_env_zmax_0)
                                                                flag_cuda = ""

                                                        if verbosity_level >= 2:
                                                                print("periodicity_search_FFT:: zmax == %d ----> dict_env = %s" % (int(z), dict_env))
                                                                print("periodicity_search_FFT:: Now check CUDA: list_cuda_ids = ", list_cuda_ids)
                                                                print("periodicity_search_FFT:: flag_use_cuda = ", flag_use_cuda)
                                                                print("periodicity_search_FFT:: flag_cuda = ", flag_cuda)

                                                accelsearch_flags = other_flags_accelsearch + flag_cuda  # + " -flo %s -fhi %s" % (frequency_to_search_min, frequency_to_search_max)

                                                accelsearch(fft_file, work_dir, log_abspath, numharm=numharm, zmax=z, other_flags=accelsearch_flags, dict_env=dict_env, verbosity_level=verbosity_level, flag_LOG_append=flag_LOG_append)
                                                tend_accelsearch = time.time()
                                                time_taken_accelsearch_s = tend_accelsearch - tstart_accelsearch
                                                print("done in %.2f s!" % (time_taken_accelsearch_s)); sys.stdout.flush()
                                                ACCEL_filename = fft_file.replace(".fft", "_ACCEL_%s" % (int(z)))

                                        print("Seg '%s' %d/%d | ck %d/%d | DM %d/%d - Will do jerk search of %s with zmax=%d, wmax=%d, numharm=%d at the end of the acceleration search of this chunk..." % (segment_label, i_seg+1, N_seg, i_ck+1, N_ck, N_files_searched+i+1, N_DMs_to_search, zapped_fft_nameonly, jerksearch_zmax, jerksearch_wmax, jerksearch_numharm), end=' ')
                                        cmd_jerksearch = "accelsearch %s -zmax %d -wmax %d -numharm %d %s" % (jerksearch_flags, jerksearch_zmax, jerksearch_wmax, jerksearch_numharm, fft_file)
                                        list_jerksearch_commands.append(cmd_jerksearch)
                                        list_jerksearch_workdirs.append(work_dir)
                                        print()

                                
                TP = ThreadPool(num_simultaneous_jerksearches)
                N_commands = len(list_jerksearch_commands)
                print()
                print("Now doing parallelized jerk search using %d CPUs..." % num_simultaneous_jerksearches);  sys.stdout.flush()
                print()
                for k in range(len(list_jerksearch_commands)):
                        print("Queing jerk search command #%d: '%s'" % (k+1, list_jerksearch_commands[k]))
                        time.sleep(0.1)
                        TP.apply_async(execute_and_log_in_thread_pool, (list_jerksearch_commands[k], log_dir, list_jerksearch_workdirs[k], k, N_commands, 1) )
                print("\n")
                print("Running %d jerk search commands at once..." % (num_simultaneous_jerksearches)); sys.stdout.flush()
                TP.close()
                TP.join()
                print()
                print("%d commands completed!" % (len(list_jerksearch_commands)))




                                        
def make_birds_file(ACCEL_0_filename, out_dir, log_dir, log_filename, width_Hz, flag_grow=1, flag_barycentre=0, sigma_birdies_threshold=4, verbosity_level=0):
        infile_nameonly = os.path.basename(ACCEL_0_filename)
        infile_basename = infile_nameonly.replace("_ACCEL_0", "")
        birds_filename = ACCEL_0_filename.replace("_ACCEL_0", ".birds")
        log_file = open(log_filename, "a")

        # Skip first three lines
        if verbosity_level >= 1:
                print("make_birds_file:: Opening the candidates: %s" % (ACCEL_0_filename))

        candidate_birdies = sifting.candlist_from_candfile(ACCEL_0_filename)
        candidate_birdies.reject_threshold(sigma_birdies_threshold)

        # Write down candidates above a certain sigma threshold
        list_birdies = candidate_birdies.cands
        if verbosity_level >= 1:
                print("make_birds_file:: Number of birdies = %d" % (len(list_birdies)))
        file_birdies = open(birds_filename, "w")
        if verbosity_level >= 1:
                print("make_birds_file:: File_birdies: %s" % (birds_filename))
        for cand in list_birdies:
                file_birdies.write("%.3f     %.20f     %d     %d     %d\n" % (cand.f, width_Hz, cand.numharm, flag_grow, flag_barycentre))
        file_birdies.close()

        return birds_filename


def get_Fourier_bin_width(fft_infile):
        inffile_name = fft_infile.replace(".fft", ".inf")
        inffile = infodata.infodata(inffile_name)
        Tobs_s = inffile.dt * inffile.N
        fourier_bin_width_Hz = 1./Tobs_s

        return fourier_bin_width_Hz


def check_zaplist_outfiles(fft_infile, verbosity_level=0):
        birds_filename   = fft_infile.replace(".fft", ".birds")
        zaplist_filename = fft_infile.replace(".fft", ".zaplist")
        try:
                if (os.path.getsize(birds_filename) > 0) and (os.path.getsize(zaplist_filename) >0): #checks if it exists and its
                        return True
                else:
                        return False
        except OSError:
                return False


def check_prepdata_outfiles(basename, verbosity_level=0):
        dat_filename  = basename + ".dat"
        inf_filename = basename + ".inf"
        try:
                if (os.path.getsize(dat_filename) > 0) and (os.path.getsize(inf_filename) >0): #checks if it exists and its
                        return True
                else:
                        return False
        except OSError:
                return False


def make_zaplist(fft_infile, out_dir, log_dir, LOG_basename, common_birdies_filename, birds_numharm=4, other_flags_accelsearch="", presto_env=os.environ['PRESTO'], verbosity_level=0):
        fft_infile_nameonly = os.path.basename(fft_infile)
        fft_infile_basename = os.path.splitext(fft_infile_nameonly)[0]
        log_abspath = "%s/LOG_%s.txt" % (log_dir, LOG_basename)
        # file_log = open(log_abspath, "w"); file_log.close()
        dict_env = {'PRESTO': presto_env, 'PATH': "%s/bin:%s" % (presto_env, os.environ['PATH']), 'LD_LIBRARY_PATH': "%s/lib:%s" % (presto_env, os.environ['LD_LIBRARY_PATH'])}

        # accelsearch

        if check_zaplist_outfiles(fft_infile) == False:
                if verbosity_level >= 2:
                        print("Doing accelsearch...", end=' ')
                        sys.stdout.flush()
                        print(fft_infile, birds_numharm, 0, other_flags_accelsearch, presto_env, verbosity_level)
                accelsearch(fft_infile, out_dir, log_abspath, birds_numharm, 0, other_flags_accelsearch, dict_env, verbosity_level)
                if verbosity_level >= 2:
                        print("Done accelsearch!")
                ACCEL_0_filename = fft_infile.replace(".fft", "_ACCEL_0")
                fourier_bin_width_Hz = get_Fourier_bin_width(fft_infile)
                if verbosity_level >= 2:
                        print("fourier_bin_width_Hz: ", fourier_bin_width_Hz)
                        print("Doing make_birds_file")
                        sys.stdout.flush()
                try:
                        birds_filename = make_birds_file(ACCEL_0_filename=ACCEL_0_filename, out_dir=out_dir, log_dir=log_dir, log_filename=log_abspath, width_Hz=fourier_bin_width_Hz, flag_grow=1, flag_barycentre=0, sigma_birdies_threshold=4, verbosity_level=0)
                except:
                        print()
                        print("WARNING: no further birdies found in the 0-DM time series: very clean band/very good mask?")
                        
                        infile_nameonly = os.path.basename(ACCEL_0_filename)
                        infile_basename = infile_nameonly.replace("_ACCEL_0", "")
                        birds_filename = ACCEL_0_filename.replace("_ACCEL_0", ".birds")

                file_common_birdies = open(common_birdies_filename, 'r')
                file_birds          = open(birds_filename, 'a')

                for line in file_common_birdies:
                        file_birds.write(line)
                file_birds.close()

                if verbosity_level >= 2:
                        print("Done make_birds_file!")
                        sys.stdout.flush()

                
                cmd_makezaplist = "makezaplist.py %s" % (birds_filename)
                if verbosity_level >= 2:
                        print("***********************************************")
                        sys.stdout.flush()
                        print("Doing execute_and_log")
                        sys.stdout.flush()
                        print("cmd_makezaplist = ", cmd_makezaplist)
                        sys.stdout.flush()
                execute_and_log(cmd_makezaplist, out_dir, log_abspath, dict_env, 0)
                if verbosity_level >= 2:
                        print("Done execute_and_log!")
                        sys.stdout.flush()
                        print("***********************************************")

        else:
                if verbosity_level >= 1:
                        print("Zaplist for %s already exists! " % (fft_infile_basename), end=' ')

        zaplist_filename = fft_infile.replace(".fft", ".zaplist")
        return zaplist_filename


def rednoise(fftfile, out_dir, log_dir, LOG_basename, other_flags="", presto_env=os.environ['PRESTO'], verbosity_level=0):
        # print "rednoise:: Inside rednoise"
        fftfile_nameonly = os.path.basename(fftfile)
        fftfile_basename = os.path.splitext(fftfile_nameonly)[0]
        log_abspath = "%s/LOG_%s.txt" % (log_dir, LOG_basename)

        dereddened_ffts_filename = "%s/dereddened_ffts.txt" % (out_dir)
        fftfile_rednoise_abspath = os.path.join(out_dir, "%s_red.fft" % (fftfile_basename))
        inffile_rednoise_abspath = os.path.join(out_dir, "%s_red.inf" % (fftfile_basename))
        inffile_original_abspath = os.path.join(out_dir, "%s.inf" % (fftfile_basename))

        cmd_rednoise = "rednoise %s %s" % (other_flags, fftfile)

        if verbosity_level >= 2:
                print("rednoise:: dereddened_ffts_filename = ", dereddened_ffts_filename)
                print("rednoise:: fftfile_rednoise_abspath = ", fftfile_rednoise_abspath)
                print("rednoise:: cmd_rednoise = ", cmd_rednoise)
                # print "%s | Running:" % (datetime.datetime.now()).strftime("%Y/%m/%d  %H:%M"); sys.stdout.flush()
                # print "%s" % (cmd_rednoise) ; sys.stdout.flush()
                print("rednoise:: opening '%s'" % (dereddened_ffts_filename))

        try:
                file_dereddened_ffts = open(dereddened_ffts_filename, 'r')
        except:
                if verbosity_level >= 2:
                        print("rednoise:: File '%s' does not exist. Creating it..." % (dereddened_ffts_filename), end=' ') ; sys.stdout.flush()
                os.mknod(dereddened_ffts_filename)
                if verbosity_level >= 2:
                        print("done!") ; sys.stdout.flush()
                file_dereddened_ffts = open(dereddened_ffts_filename, 'r')

        # If the fftfile is already in the list of dereddened files...
        if "%s\n" % (fftfile) in file_dereddened_ffts.readlines():
                if verbosity_level >= 2:
                        print("rednoise:: NB: File '%s' is already in the list of dereddened files (%s)." % (fftfile, dereddened_ffts_filename))
                        # Then check is the file has size > 0...
                        print("rednoise:: Checking the size of '%s'" % (fftfile))

                if (os.path.getsize(fftfile) > 0):
                        operation = "skip"
                        if verbosity_level >= 2:
                                print("rednoise:: size is > 0. Then skipping...")
                else:
                        operation = "make_from_scratch"
                        if verbosity_level >= 2:
                                print("rednoise:: size is = 0. Making from scratch...")

        else:
                operation = "make_from_scratch"
                if verbosity_level >= 2:
                        print("rednoise:: File '%s' IS NOT in the list of dereddened files (%s). I will make the file from scratch..." % (fftfile_basename, dereddened_ffts_filename))

        file_dereddened_ffts.close()

        if operation =="make_from_scratch":
                if verbosity_level >= 2:
                        print("rednoise:: making the file from scratch...")
                dict_env = {'PRESTO': presto_env, 'PATH': "%s/bin:%s" % (presto_env, os.environ['PATH']), 'LD_LIBRARY_PATH': "%s/lib:%s" % (presto_env, os.environ['LD_LIBRARY_PATH'])}
                execute_and_log(cmd_rednoise, out_dir, log_abspath, dict_env, 0)
                if verbosity_level >= 2:
                        print("done!", end=' ')
                        sys.stdout.flush()
                file_dereddened_ffts = open(dereddened_ffts_filename, 'a')
                file_dereddened_ffts.write("%s\n" % (fftfile))
                file_dereddened_ffts.close()
                os.rename(fftfile_rednoise_abspath, fftfile_rednoise_abspath.replace("_red.", "."))
                os.rename(inffile_rednoise_abspath, inffile_rednoise_abspath.replace("_red.", "."))


def realfft(infile, out_dir, log_dir, LOG_basename, other_flags="", presto_env=os.environ['PRESTO'], verbosity_level=0, flag_LOG_append=0):
        infile_nameonly = os.path.basename(infile)
        infile_basename = os.path.splitext(infile_nameonly)[0]
        log_abspath = "%s/LOG_%s.txt" % (log_dir, LOG_basename)
        fftfile_abspath = os.path.join(out_dir, "%s.fft" % (infile_basename))
        cmd_realfft = "realfft %s %s" % (other_flags, infile)
        if verbosity_level >= 2:
                print("%s | realfft:: Running:" % (datetime.datetime.now()).strftime("%Y/%m/%d  %H:%M"))
                sys.stdout.flush()
                print("%s" % (cmd_realfft))
                sys.stdout.flush()

        if os.path.exists(fftfile_abspath ) and (os.path.getsize(fftfile_abspath) > 0):
                if verbosity_level >= 1:
                        print()
                        print("WARNING: File %s already present. Skipping realfft..." % (fftfile_abspath), end=' ')
        else:
                dict_env = {'PRESTO': presto_env, 'PATH': "%s/bin:%s" % (presto_env, os.environ['PATH']), 'LD_LIBRARY_PATH': "%s/lib:%s" % (presto_env, os.environ['LD_LIBRARY_PATH'])}
                execute_and_log(cmd_realfft, out_dir, log_abspath, dict_env, 0)
                if os.path.exists(fftfile_abspath ) and (os.stat(fftfile_abspath).st_size > 0):
                        if verbosity_level >= 2:
                                print("%s | realfft on \"%s\" completed successfully!" % (datetime.datetime.now().strftime("%Y/%m/%d  %H:%M"), infile_nameonly))
                                sys.stdout.flush()
                else:
                        print("WARNING (%s) | could not find all the output files from realfft on \"%s\"!" % (datetime.datetime.now().strftime("%Y/%m/%d  %H:%M"), infile_nameonly))
                        sys.stdout.flush()


# PREPDATA
def prepdata(infile, out_dir, log_dir, LOG_basename, DM, Nsamples=0, ignorechan_list="", mask="", downsample_factor=1, reference="barycentric", other_flags="", presto_env=os.environ['PRESTO'], verbosity_level=0):
        infile_nameonly = os.path.basename(infile)
        infile_basename = os.path.splitext(infile_nameonly)[0]
        log_abspath = "%s/LOG_%s.txt" % (log_dir, LOG_basename)
        # file_log = open(log_abspath, "w"); file_log.close()
        outfile_basename = "%s_DM%05.2f" % (infile_basename, np.float64(DM))
        datfile_abspath = os.path.join(out_dir, "%s.dat" % (outfile_basename))
        inffile_abspath = os.path.join(out_dir, "%s.inf" % (outfile_basename))


        if reference =="topocentric":
                flag_nobary = "-nobary "
        elif reference =="barycentric":
                flag_nobary = ""
        else:
                print("ERROR: Invalid value for barycentering option: \"%s\"" % (reference))
                exit()

        if Nsamples >= 0:
                flag_numout = "-numout %d " % (make_even_number(int(Nsamples/np.float64(downsample_factor))) )
        else:
                flag_numout = ""

        if mask !="":
                flag_mask = "-mask %s " % (mask)
        else:
                flag_mask = ""

        if ignorechan_list !="":
                flag_ignorechan = "-ignorechan %s " % (ignorechan_list)
        else:
                flag_ignorechan = ""

        cmd_prepdata = "prepdata -o %s %s%s %s%s%s -dm %s -downsamp %s %s" % (outfile_basename, flag_numout, flag_ignorechan, flag_mask, flag_nobary, other_flags, str(DM), downsample_factor, infile)

        if verbosity_level >= 2:
                print("%s | Running:" % (datetime.datetime.now()).strftime("%Y/%m/%d  %H:%M"))
                sys.stdout.flush()
                print("%s" % (cmd_prepdata))
                sys.stdout.flush()



        if os.path.exists(datfile_abspath ) and os.path.exists( inffile_abspath):
                if verbosity_level >= 1:
                        print()
                        print("WARNING: File '%s.dat' and '%s.inf' already present. Skipping and checking results..." % (outfile_basename, outfile_basename), end=' ')
        else:
                dict_env = {'PRESTO': presto_env, 'PATH': "%s/bin:%s" % (presto_env, os.environ['PATH']), 'LD_LIBRARY_PATH': "%s/lib:%s" % (presto_env, os.environ['LD_LIBRARY_PATH'])}

                execute_and_log(cmd_prepdata, out_dir, log_abspath, dict_env, 0)
                if os.path.exists(datfile_abspath ) and os.path.exists( inffile_abspath):
                        if verbosity_level >= 2:
                                print("%s | prepdata on \"%s\" completed successfully!" % (datetime.datetime.now().strftime("%Y/%m/%d  %H:%M"), infile_nameonly))
                                sys.stdout.flush()
                else:
                        print("WARNING (%s) | could not find all the output files from prepdata on \"%s\"!" % (datetime.datetime.now().strftime("%Y/%m/%d  %H:%M"), infile_nameonly))
                        sys.stdout.flush()


def make_rfifind_mask(infile, out_dir, log_dir, LOG_basename, time=2.0, time_intervals_to_zap="", chans_to_zap="", other_flags="", presto_env=os.environ['PRESTO'], verbosity_level=0):
        infile_nameonly = os.path.basename(infile)
        infile_basename = os.path.splitext(infile_nameonly)[0]

        log_abspath = "%s/LOG_%s.txt" % (log_dir, LOG_basename)

        flag_zapints = ""
        flag_zapchan = ""
        if time_intervals_to_zap != "":
                flag_zapints = "-zapints %s" %  (time_intervals_to_zap)
        if chans_to_zap != "":
                flag_zapchan = "-zapchan %s" %  (chans_to_zap)

        cmd_rfifind = "rfifind %s -o %s -time %s %s %s %s" % (other_flags, infile_basename, time, flag_zapints, flag_zapchan, infile)
        if verbosity_level >= 2:
                print("%s | Running:" % (datetime.datetime.now()).strftime("%Y/%m/%d  %H:%M"))
                sys.stdout.flush()
                print("%s" % (cmd_rfifind))
                sys.stdout.flush()

        dict_env = {'PRESTO': presto_env, 'PATH': "%s/bin:%s" % (presto_env, os.environ['PATH']), 'LD_LIBRARY_PATH': "%s/lib:%s" % (presto_env, os.environ['LD_LIBRARY_PATH'])}

        execute_and_log(cmd_rfifind, out_dir, log_abspath, dict_env, 0)
        if verbosity_level >= 1:
                print("done!")

        if check_rfifind_outfiles(out_dir, infile_basename) == True:
                if verbosity_level >= 2:
                        print("make_rfifind_mask:: %s | rfifind on \"%s\" completed successfully!" % (datetime.datetime.now().strftime("%Y/%m/%d  %H:%M"), infile_nameonly))
                        sys.stdout.flush()
        else:
                print("WARNING (%s) | could not find all the output files from rfifind on \"%s\"!" % (datetime.datetime.now().strftime("%Y/%m/%d  %H:%M"), infile_nameonly))
                sys.stdout.flush()
                raise Exception("Your STEP_RFIFIND flag is set to 0, but the rfifind files could not be found!")

        mask_file = "%s/%s_rfifind.mask" % (out_dir, infile_basename)
        result = get_rfifind_result(mask_file, log_abspath, verbosity_level)


def get_DD_scheme_from_DDplan_output(output_DDplan, N_DMs_per_prepsubband, nsubbands):
        list_dict_schemes = []
        output_DDplan_list_lines = output_DDplan.split("\n")
        if nsubbands == 0:
                index = output_DDplan_list_lines.index("  Low DM    High DM     dDM  DownSamp   #DMs  WorkFract")   + 1
        else:
                index = output_DDplan_list_lines.index("  Low DM    High DM     dDM  DownSamp  dsubDM   #DMs  DMs/call  calls  WorkFract")   + 1
                
        print
        print("+++++++++++++++++++++++++++++++++")
        print(output_DDplan)
        print("+++++++++++++++++++++++++++++++++")

        flag_add_plans = 1
        while flag_add_plans == 1:
                if output_DDplan_list_lines[index] == "":
                        return list_dict_schemes
                else:
                        if nsubbands == 0:
                                param = output_DDplan_list_lines[index].split()
                                low_DM_by_DDplan   = np.float64(param[0])
                                high_DM_by_DDplan = np.float64(param[1])
                                dDM = np.float64(param[2])
                                downsamp = int(param[3])
                                num_DMs = int(param[4])

                                if num_DMs > N_DMs_per_prepsubband:
                                        N_schemes = int(num_DMs / N_DMs_per_prepsubband) + 1

                                        for n in range(N_schemes-1):
                                                lowDM   = low_DM_by_DDplan + (n    * N_DMs_per_prepsubband * dDM)
                                                highDM = lowDM + N_DMs_per_prepsubband * dDM                                
                                                dict_scheme = {'loDM': lowDM, 'highDM': highDM, 'dDM': dDM, 'downsamp': downsamp, 'num_DMs': N_DMs_per_prepsubband}
                                                if verbosity_level >= 2:
                                                        print("dict_scheme for loop = ",  dict_scheme)
                                                list_dict_schemes.append(dict_scheme)

                                        lowDM =  round(low_DM_by_DDplan + (N_schemes-1)   * N_DMs_per_prepsubband * dDM,             3 ) #round to third decimal digit
                                        highDM = round(high_DM_by_DDplan, 3)
                                        numDMs = int(round((highDM - lowDM) / dDM, 3))
				
                                        dict_scheme = {'loDM': lowDM, 'highDM': highDM, 'dDM': dDM, 'downsamp': downsamp, 'num_DMs': numDMs}
                                        list_dict_schemes.append(dict_scheme)

                                else:
                                        dict_scheme = {'loDM': low_DM_by_DDplan, 'highDM': high_DM_by_DDplan, 'dDM': dDM, 'downsamp': downsamp, 'num_DMs': num_DMs}
                                        if verbosity_level >= 2:
                                                print("num_DMs else =", num_DMs)
                                                print("dict_scheme else =", dict_scheme)
                                        list_dict_schemes.append(dict_scheme)

                                
                                
                        elif nsubbands > 0:
                                param = output_DDplan_list_lines[index].split()
                                low_DM_by_DDplan   = np.float64(param[0])
                                high_DM_by_DDplan = np.float64(param[1])
                                dDM = np.float64(param[2])
                                downsamp = int(param[3])
                                dsubDM = np.float64(param[4])
                                num_DMs = int(param[5])
                                num_DMs_percall = int(param[6])
                                N_calls = int(param[7])

                                

                                for k in range(N_calls):
                                        dict_scheme = {'loDM': round(low_DM_by_DDplan + k*dsubDM, 3), 'highDM': round(low_DM_by_DDplan+(k+1)*dsubDM, 3), 'dDM': dDM, 'downsamp': downsamp, 'num_DMs': num_DMs_percall}
                                        list_dict_schemes.append(dict_scheme)


                                        
                index = index + 1


def check_prepsubband_result_single_scheme(work_dir, DD_scheme, verbosity_level=1):
        for dm in np.arange(DD_scheme['loDM'], DD_scheme['highDM'] - 0.5*DD_scheme['dDM'], DD_scheme['dDM']):
                if verbosity_level >= 2:
                        print("check_prepsubband_result_single_scheme:: Looking for: ", [os.path.join(work_dir, "*DM%.2f.dat"%(dm) )],  [os.path.join(work_dir, "*DM%.2f.inf"%(dm) )] )
                        print("check_prepsubband_result_single_scheme:: This is what I found: %s, %s" % ([ x for x in glob.glob(os.path.join(work_dir, "*DM%.2f.dat"%(dm))) if not "_red" in x]  , [ x for x in glob.glob(os.path.join(work_dir, "*DM%.2f.inf"%(dm))) if not "_red" in x]    ))
                if len([ x for x in glob.glob(os.path.join(work_dir, "*DM%.2f.dat"%(dm))) if not "_red" in x]   + [ x for x in glob.glob(os.path.join(work_dir, "*DM%.2f.inf"%(dm))) if not "_red" in x] ) != 2:
                        if verbosity_level >= 2:
                                print("check_prepsubband_result_single_scheme: False")
                        return False
        if verbosity_level >= 2:
                print("check_prepsubband_result_single_scheme: True")

        return True


def get_DDplan_scheme(infile, out_dir, log_dir, LOG_basename, loDM, highDM, DM_coherent_dedispersion, N_DMs_per_prepsubband, freq_central_MHz, bw_MHz, nchan, nsubbands, t_samp_s):
        infile_nameonly = os.path.basename(infile)
        infile_basename = os.path.splitext(infile_nameonly)[0]
        log_abspath = "%s/LOG_%s.txt" % (log_dir, LOG_basename)
        
        if np.float64(DM_coherent_dedispersion) == 0:
                if nsubbands == 0:                        
                        cmd_DDplan = "DDplan.py -o ddplan_%s -l %s -d %s -f %s -b %s -n %s -t %s" % (infile_basename, loDM, highDM, freq_central_MHz, np.fabs(bw_MHz), nchan, t_samp_s)
                else:
                        cmd_DDplan = "DDplan.py -o ddplan_%s -l %s -d %s -f %s -b %s -n %s -t %s -s %s" % (infile_basename, loDM, highDM, freq_central_MHz, np.fabs(bw_MHz), nchan, t_samp_s, nsubbands)
                        print("Use of subbands enabled with %s subbands (number of channels in the data: %d)" % (nsubbands, nchan))

        elif np.float64(DM_coherent_dedispersion) > 0:
                print("Coherent dedispersion enabled with DM = %.3f pc cm-3" % (np.float64(DM_coherent_dedispersion)))
                if nsubbands == 0: 
                        cmd_DDplan = "DDplan.py -o ddplan_%s -l %s -d %s -c %s -f %s -b %s -n %s -t %s" % (infile_basename, loDM, highDM, DM_coherent_dedispersion, freq_central_MHz, np.fabs(bw_MHz), nchan, t_samp_s)
                else:
                        cmd_DDplan = "DDplan.py -o ddplan_%s -l %s -d %s -c %s -f %s -b %s -n %s -t %s -s %s" % (infile_basename, loDM, highDM, DM_coherent_dedispersion, freq_central_MHz, np.fabs(bw_MHz), nchan, t_samp_s, nsubbands)
                        print("Use of subbands enabled with %s subbands (number of channels in the data: %d)" % (nsubbands, nchan))
                        
        elif np.float64(DM_coherent_dedispersion) < 0:
                print("ERROR: The DM of coherent dedispersion < 0! Exiting...")
                exit()

        print("Running:  \033[1m %s \033[0m" % (cmd_DDplan))
        output_DDplan = get_command_output(cmd_DDplan, shell_state=False, work_dir=out_dir)

        list_DD_schemes = get_DD_scheme_from_DDplan_output(output_DDplan, N_DMs_per_prepsubband, nsubbands)
        
        return list_DD_schemes


def dedisperse(infile, out_dir, log_dir, LOG_basename, segment_label, chunk_label, Nsamples, ignorechan_list, mask_file, list_DD_schemes, nchan, nsubbands=0, num_simultaneous_prepsubbands=1, other_flags="", presto_env=os.environ['PRESTO'], verbosity_level=0):
        infile_nameonly = os.path.basename(infile)
        infile_basename = os.path.splitext(infile_nameonly)[0]
        prepsubband_outfilename = "%s_%s_%s" % (infile_basename, segment_label, chunk_label)
        dict_env = {'PRESTO': presto_env, 'PATH': "%s/bin:%s" % (presto_env, os.environ['PATH']), 'LD_LIBRARY_PATH': "%s/lib:%s" % (presto_env, os.environ['LD_LIBRARY_PATH'])}
        file_script_prepsubband_name = "script_prepsubband.txt"
        file_script_prepsubband_abspath = "%s/%s" % (out_dir, file_script_prepsubband_name)
        
        log_abspath = "%s/LOG_%s.txt" % (log_dir, LOG_basename)

        
        N_schemes = len(list_DD_schemes)

        string_mask = ""
        if mask_file != "":
                string_mask = "-mask %s" % (mask_file)
        string_ignorechan = ""
        if ignorechan_list != "":
                string_ignorechan = "-ignorechan %s" % (ignorechan_list)

        print("----------------------------------------------------------------------")
        print("prepsubband will be run %d times with the following DM ranges:" % (N_schemes))
        print()
        print("%10s %10s %10s %10s %10s " % ("Low DM", "High DM", "dDM",  "DownSamp",   "#DMs"))
        for i in range(N_schemes):
                offset = 0
                if i == N_schemes-1 : offset = 1
                print("%10.3f %10.3f %10s %10s %10d " % (list_DD_schemes[i]['loDM'], np.float64(list_DD_schemes[i]['loDM']) + int(list_DD_schemes[i]['num_DMs'])*np.float64(list_DD_schemes[i]['dDM']), list_DD_schemes[i]['dDM'],  list_DD_schemes[i]['downsamp'],  list_DD_schemes[i]['num_DMs'] + offset))
        print()
        sys.stdout.flush()
        
        if nsubbands == 0:
                nsubbands = nchan
        elif (nchan % nsubbands != 0):
                print("ERROR: requested number of subbands is %d, which is not an integer divisor of the number of channels %d! " % (nsubbands, nchan))
                exit()

        file_script_prepsubband = open(file_script_prepsubband_abspath, "w")
        for i in range(N_schemes):
                flag_numout = ""
                if i < N_schemes-1:
                        cmd_prepsubband = "prepsubband %s %s -o %s %s %s -lodm %s -dmstep %s -numdms %s -downsamp %s -nsub %s %s" % (other_flags, flag_numout, prepsubband_outfilename, string_ignorechan, string_mask, list_DD_schemes[i]['loDM'], list_DD_schemes[i]['dDM'], list_DD_schemes[i]['num_DMs']    , list_DD_schemes[i]['downsamp'], nsubbands, infile)
                elif i == N_schemes-1:
                        cmd_prepsubband = "prepsubband %s %s -o %s %s %s -lodm %s -dmstep %s -numdms %s -downsamp %s -nsub %s %s" % (other_flags, flag_numout, prepsubband_outfilename, string_ignorechan, string_mask, list_DD_schemes[i]['loDM'], list_DD_schemes[i]['dDM'], list_DD_schemes[i]['num_DMs'] + 1, list_DD_schemes[i]['downsamp'], nsubbands, infile)
                file_script_prepsubband.write("%s\n" % cmd_prepsubband)

        file_script_prepsubband.close()


        if verbosity_level >= 1:
                print("Dedispersing with %d subbands (original number of channels: %d)" % (nsubbands, nchan))
                print()
                if N_schemes == 1:
                        print("\033[1m >> TIP:\033[0m Check prepsubband progress with '\033[1mtail -f %s\033[0m'" % (log_abspath))
                elif N_schemes > 1:
                        print("\033[1m >> TIP:\033[0m Check prepsubband progress with '\033[1mfor f in %s/LOG_prepsubband_*.txt; do tail -1 ${f}; echo; done\033[0m'" % (LOG_dir))
                print()

        N_prepsubband_schemes_done = 0
        list_prepsubband_commands = []
        list_prepsubband_workdirs = []


        while (N_prepsubband_schemes_done < N_schemes):
                if verbosity_level >= 2:
                        print("dedisperse:: N_prepsubband_schemes_done = ", N_prepsubband_schemes_done)
                for i in range(N_schemes):
                        flag_numout = ""
                        loDM = np.float64(list_DD_schemes[i]['loDM'])
                        dDM  = np.float64(list_DD_schemes[i]['dDM'])
                        hiDM = loDM + int(list_DD_schemes[i]['num_DMs'])*dDM

                        if i < N_schemes-1:
                                str_parentesis = ")"
                                cmd_prepsubband = "prepsubband %s %s -o %s %s %s -lodm %s -dmstep %s -numdms %s -downsamp %s -nsub %s %s" % (other_flags, flag_numout, prepsubband_outfilename, string_ignorechan, string_mask, list_DD_schemes[i]['loDM'], list_DD_schemes[i]['dDM'], list_DD_schemes[i]['num_DMs']    , list_DD_schemes[i]['downsamp'], nsubbands, infile)
                        elif i == N_schemes-1:
                                str_parentesis = "]"
                                cmd_prepsubband = "prepsubband %s %s -o %s %s %s -lodm %s -dmstep %s -numdms %s -downsamp %s -nsub %s %s" % (other_flags, flag_numout, prepsubband_outfilename, string_ignorechan, string_mask, list_DD_schemes[i]['loDM'], list_DD_schemes[i]['dDM'], list_DD_schemes[i]['num_DMs'] + 1, list_DD_schemes[i]['downsamp'], nsubbands, infile)

                        if check_prepsubband_result_single_scheme(out_dir, list_DD_schemes[i], verbosity_level) == False:
                                if num_simultaneous_prepsubbands == 1 or N_schemes == 1:
                                        if verbosity_level >= 1:
                                                print("Running  prepsubband for DM range [%.3f-%.3f%s pc cm-3 (scheme %d/%d) on observation '%s'..." % (loDM, hiDM, str_parentesis, i+1, N_schemes, infile), end=' '); sys.stdout.flush()
                                        if verbosity_level >= 2:
                                                print("dedisperse:: %d) RUNNING: %s" % (i, cmd_prepsubband))

                                        execute_and_log("which prepsubband", out_dir, log_abspath, dict_env, 1)
                                        execute_and_log(cmd_prepsubband, out_dir, log_abspath, dict_env, 1)
                                        if check_prepsubband_result_single_scheme(out_dir, list_DD_schemes[i], verbosity_level) == True:
                                                N_prepsubband_schemes_done = N_prepsubband_schemes_done + 1
                                        if verbosity_level >= 1:
                                                print("done!"); sys.stdout.flush()

                                elif num_simultaneous_prepsubbands > 1 and N_schemes > 1:
                                        list_prepsubband_commands.append(cmd_prepsubband)
                                        list_prepsubband_workdirs.append(out_dir)
                                        #print("list_prepsubband_commands = %s", list_prepsubband_commands)
                                        N_prepsubband_schemes_done = N_prepsubband_schemes_done + 1
        

                        else:
                                print("WARNING: prepsubband for DM range [%.3f-%.3f%s pc cm-3 (scheme %d/%d) on observation '%s' already successfully run. Skipping..." % (list_DD_schemes[i]['loDM'], hiDM, str_parentesis, i+1, N_schemes, infile))
                                N_prepsubband_schemes_done = N_prepsubband_schemes_done + 1

        if num_simultaneous_prepsubbands > 1 and N_schemes > 1:
                TP = ThreadPool(num_simultaneous_prepsubbands)
                N_commands = len(list_prepsubband_commands)
                print()
                print("Total of %d prepsubband commands, running %d of them at once..." % (N_commands, num_simultaneous_prepsubbands));  sys.stdout.flush()
                print()
                for k in range(len(list_prepsubband_commands)):
                        print("Queing prepsubband command #%d: '%s'" % (k+1, list_prepsubband_commands[k]))
                        time.sleep(0.1)
                        TP.apply_async(execute_and_log_in_thread_pool, (list_prepsubband_commands[k], log_dir, list_prepsubband_workdirs[k], k, N_commands, 1) )
                TP.close()
                TP.join()
                print()
                print("%d commands completed!" % (len(list_prepsubband_commands)))


def check_zapbirds_outfiles2(zapped_fft_filename, verbosity_level=0):
        zapped_inf_filename = zapped_fft_filename.replace(".fft", ".inf")

        if ("zapped" in zapped_fft_filename) and ("zapped" in zapped_inf_filename):
                try:
                        if (os.path.getsize(zapped_fft_filename) > 0) and (os.path.getsize(zapped_inf_filename) >0): #checks if it exists and its size is > 0
                                return True
                        else:
                                return False
                except OSError:
                        return False
        else:
                return False


def check_zapbirds_outfiles(fftfile, list_zapped_ffts_abspath, verbosity_level=0):
        fftfile_nameonly = os.path.basename(fftfile)
        try:
                file_list_zapped_ffts = open(list_zapped_ffts_abspath, 'r')
                if "%s\n" % (fftfile_nameonly) in file_list_zapped_ffts.readlines():
                        if verbosity_level >= 1:
                                print("check_zapbirds_outfiles:: NB: File '%s' is already in the list of zapped files (%s)." % (fftfile_nameonly, list_zapped_ffts_abspath))
                        if (os.path.getsize(fftfile) > 0):
                                if verbosity_level >= 1:
                                        print("check_zapbirds_outfiles:: size is > 0. Returning True...")
                                return True
                        else:
                                if verbosity_level >= 1:
                                        print("rednoise:: size is = 0. Returning False...")
                                return False
                else:
                        if verbosity_level >= 1:
                                print("check_zapbirds_outfiles:: File '%s' IS NOT in the list of zapped files (%s). I will zap the file from scratch..." % (fftfile_nameonly, list_zapped_ffts_abspath))
                        return False
        except:
                if verbosity_level >= 1:
                        print("check_zapbirds_outfiles:: File '%s' does not exist. Creating it and returning False..." % (list_zapped_ffts_abspath))
                os.mknod(list_zapped_ffts_abspath)
                return False


def zapbirds(fft_infile, zapfile_name, work_dir, log_dir, LOG_basename, presto_env, verbosity_level=0):
        fft_infile_nameonly = os.path.basename(fft_infile)
        fft_infile_basename = os.path.splitext(fft_infile_nameonly)[0]
        inffile_filename = fft_infile.replace(".fft", ".inf")
        log_abspath = "%s/LOG_%s.txt" % (log_dir, LOG_basename)
        # file_log = open(log_abspath, "w"); file_log.close()
        dict_env = {'PRESTO': presto_env, 'PATH': "%s/bin:%s" % (presto_env, os.environ['PATH']), 'LD_LIBRARY_PATH': "%s/lib:%s" % (presto_env, os.environ['LD_LIBRARY_PATH'])}

        cmd_zapbirds = "zapbirds -zap -zapfile %s %s" % (zapfile_name, fft_infile)
        zapped_fft_filename = fft_infile.replace(".fft", "_zapped.fft")
        zapped_inf_filename = inffile_filename.replace(".inf", "_zapped.inf")

        list_zapped_ffts_abspath = os.path.join(work_dir, "list_zapped_ffts.txt")
        if verbosity_level >= 2:
                print("zapbirds:: list_zapped_ffts_abspath = ", list_zapped_ffts_abspath)

        if check_zapbirds_outfiles(fft_infile, list_zapped_ffts_abspath, verbosity_level=0) == False:
                if verbosity_level >= 2:
                        print("Running ZAPBIRDS: %s" % (cmd_zapbirds))
                        sys.stdout.flush()
                execute_and_log(cmd_zapbirds, work_dir, log_abspath, dict_env, 0)
                file_list_zapped_ffts = open(list_zapped_ffts_abspath, 'a')
                file_list_zapped_ffts.write("%s\n" % (fft_infile))
                file_list_zapped_ffts.close()

        return zapped_fft_filename, zapped_inf_filename


def dedisperse_rednoise_and_periodicity_search_FFT(infile, out_dir, root_workdir, log_dir, LOG_basename, flag_search_full, segment_label, chunk_label, list_seg_ck_indices, zapfile, Nsamples, ignorechan_list, mask_file, list_DD_schemes, nchan, subbands=0, num_simultaneous_prepsubbands=1, other_flags_prepsubband="", presto_env_prepsubband=os.environ['PRESTO'], flag_use_cuda=0, list_cuda_ids=[0], flag_acceleration_search=1, numharm=8, list_zmax=[20], flag_jerk_search=0, jerksearch_zmax=10, jerksearch_wmax=30, jerksearch_numharm=4, num_simultaneous_jerksearches=1, period_to_search_min_s=0.001, period_to_search_max_s=20.0, other_flags_accelsearch="", flag_remove_fftfiles=0, flag_remove_datfiles_of_segments=0, presto_env_accelsearch_zmax_0=os.environ['PRESTO'], presto_env_accelsearch_zmax_any=os.environ['PRESTO'], verbosity_level=0, dict_flag_steps = {'flag_step_dedisperse': 1 , 'flag_step_realfft': 1, 'flag_step_periodicity_search': 1}):
        infile_nameonly = os.path.basename(infile)
        infile_basename = os.path.splitext(infile_nameonly)[0]

        if verbosity_level >= 2:
                print("dedisperse_rednoise_and_periodicity_search_FFT:: launching dedisperse")
                sys.stdout.flush()
                print("dedisperse_rednoise_and_periodicity_search_FFT:: list_zmax = ", list_zmax)


        if dict_flag_steps['flag_step_dedisperse'] == 1:
                if segment_label == "full":
                        dedisperse(infile, out_dir, log_dir, LOG_basename, segment_label, chunk_label, Nsamples, ignorechan_list, mask_file, list_DD_schemes, nchan, subbands, num_simultaneous_prepsubbands, other_flags_prepsubband, presto_env_prepsubband, verbosity_level)
                else:

                        search_string = "%s/03_DEDISPERSION/%s/full/ck00/*.dat" % (root_workdir, infile_basename) #List of datfiles to split
                        list_datfiles_to_split = glob.glob(search_string)

                        if verbosity_level >= 3:
                                print("dedisperse_rednoise_and_periodicity_search_FFT:: segment_label: '%s'" % (segment_label))
                                print("search_string = ", search_string)
                                print("list_datfiles_to_split = ", list_datfiles_to_split)

                        segment_min = np.float64(segment_label.replace("m", ""))
                        i_chunk = int(chunk_label.replace("ck", ""))
                        split_into_chunks(infile, list_datfiles_to_split, log_dir, LOG_basename, out_dir, segment_min, i_chunk, list_zmax, flag_jerk_search, jerksearch_zmax, jerksearch_wmax, presto_env=os.environ['PRESTO'], flag_LOG_append=1, flag_remove_datfiles_of_segments=flag_remove_datfiles_of_segments)

                if verbosity_level >= 2:
                        print("dedisperse_rednoise_and_periodicity_search_FFT:: launching periodicity_search_FFT")
                        sys.stdout.flush()
                        print("dedisperse_rednoise_and_periodicity_search_FFT:: looking for %s/*DM*.dat" % (out_dir))
                        print("dedisperse_rednoise_and_periodicity_search_FFT:: list_cuda_ids = %s" % (list_cuda_ids))
        else:
                if verbosity_level >= 2:
                        print("dedisperse_rednoise_and_periodicity_search_FFT:: STEP_DEDISPERSE = 0, skipping prepsubband...")

        if not (segment_label == "full" and flag_search_full == 0):
                periodicity_search_FFT(out_dir, log_dir, LOG_basename, zapfile, segment_label, chunk_label, list_seg_ck_indices, list_DD_schemes, flag_use_cuda, list_cuda_ids, flag_acceleration_search, numharm, list_zmax, flag_jerk_search, jerksearch_zmax, jerksearch_wmax, jerksearch_numharm, num_simultaneous_jerksearches, period_to_search_min_s, period_to_search_max_s, other_flags_accelsearch, flag_remove_fftfiles, flag_remove_datfiles_of_segments, presto_env_accelsearch_zmax_0, presto_env_accelsearch_zmax_any, verbosity_level, 1, dict_flag_steps)


class SurveyConfiguration(object):
        def __init__(self, config_filename, verbosity_level=1):
                self.config_filename = config_filename
                self.list_datafiles = []

                self.list_survey_configuration_ordered_params = ['SEARCH_LABEL', 'DATA_TYPE', 'ROOT_WORKDIR', 'PRESTO', 'PRESTO_GPU', 'DM_MIN', 'DM_MAX', 'DM_COHERENT_DEDISPERSION', 'N_SUBBANDS', 'PERIOD_TO_SEARCH_MIN', 'PERIOD_TO_SEARCH_MAX', 'LIST_SEGMENTS', 'RFIFIND_TIME', 'RFIFIND_CHANS_TO_ZAP', 'RFIFIND_TIME_INTERVALS_TO_ZAP', 'IGNORECHAN_LIST', 'ZAP_ISOLATED_PULSARS_FROM_FFTS', 'ZAP_ISOLATED_PULSARS_MAX_HARM', 'FLAG_ACCELERATION_SEARCH', 'ACCELSEARCH_LIST_ZMAX', 'ACCELSEARCH_NUMHARM', 'FLAG_JERK_SEARCH', 'JERKSEARCH_ZMAX', 'JERKSEARCH_WMAX', 'JERKSEARCH_NUMHARM', 'SIFTING_FLAG_REMOVE_DUPLICATES', 'SIFTING_FLAG_REMOVE_DM_PROBLEMS', 'SIFTING_FLAG_REMOVE_HARMONICS', 'SIFTING_MINIMUM_NUM_DMS', 'SIFTING_MINIMUM_DM', 'SIFTING_SIGMA_THRESHOLD', 'FLAG_FOLD_KNOWN_PULSARS', 'FLAG_FOLD_TIMESERIES', 'FLAG_FOLD_RAWDATA', 'RFIFIND_FLAGS', 'PREPDATA_FLAGS', 'PREPSUBBAND_FLAGS', 'REALFFT_FLAGS', 'REDNOISE_FLAGS', 'ACCELSEARCH_FLAGS', 'ACCELSEARCH_GPU_FLAGS', 'ACCELSEARCH_JERK_FLAGS', 'PREPFOLD_FLAGS', 'FLAG_SINGLEPULSE_SEARCH', 'SINGLEPULSE_SEARCH_FLAGS', 'USE_CUDA', 'CUDA_IDS', 'NUM_SIMULTANEOUS_JERKSEARCHES', 'NUM_SIMULTANEOUS_PREPFOLDS', 'NUM_SIMULTANEOUS_PREPSUBBANDS', 'MAX_SIMULTANEOUS_DMS_PER_PREPSUBBAND', 'FAST_BUFFER_DIR', 'FLAG_KEEP_DATA_IN_BUFFER_DIR', 'FLAG_REMOVE_FFTFILES', 'FLAG_REMOVE_DATFILES_OF_SEGMENTS', 'STEP_RFIFIND', 'STEP_ZAPLIST', 'STEP_DEDISPERSE', 'STEP_REALFFT', 'STEP_PERIODICITY_SEARCH', 'STEP_SIFTING', 'STEP_FOLDING', 'STEP_SINGLEPULSE_SEARCH']

                self.dict_survey_configuration = {}

                config_file = open(config_filename, "r" )

                for line in config_file:
                        if line != "\n" and (not line.startswith("#")):
                                list_line = shlex.split(line)
                                self.dict_survey_configuration[list_line[0]] = list_line[1]  # Save parameter key and value in the dictionary 
                for key in list(self.dict_survey_configuration.keys()):
                        if   key == "SEARCH_LABEL":                      self.search_label                     = self.dict_survey_configuration[key]
                        elif key == "DATA_TYPE":                         self.data_type                        = self.dict_survey_configuration[key]
                        elif key == "ROOT_WORKDIR":
                                if self.dict_survey_configuration[key] == "(cwd)":
                                        self.root_workdir                     = os.getcwd()
                                        print("ROOT_WORKDIR == '(cwd)' --->  ROOT_WORKDIR set as current working directory '%s' \n" % self.root_workdir)
                                else:
                                        self.root_workdir                     = self.dict_survey_configuration[key]
                                        if os.path.exists(self.root_workdir) == False:
                                                print("%sERROR:%s %s directory '%s' does not exist!" % (colors.ERROR+colors.BOLD, colors.ENDCOLOR, key, self.root_workdir ))
                                                print("Please make sure that the path of %s in your configuration file is correct." % (key))
                                                print("Alternatively, you can use '(cwd)' to tell PULSAR_MINER to use the Current Working Directory as the ROOT_WORKDIR")
                                                exit()
                        elif key == "PRESTO":
                                if check_presto_path(presto_path=self.dict_survey_configuration[key], key=key) == True:
                                        self.presto_env                       = self.dict_survey_configuration[key]

                        elif key == "PRESTO_GPU":
                                if check_presto_path(presto_path=self.dict_survey_configuration[key], key=key) == True:
                                        self.presto_gpu_env                   = self.dict_survey_configuration[key]
                        elif key == "DM_MIN":                               self.dm_min                                = self.dict_survey_configuration[key]
                        elif key == "DM_MAX":                               self.dm_max                                = self.dict_survey_configuration[key]
                        elif key == "DM_COHERENT_DEDISPERSION":             self.dm_coherent_dedispersion              = self.dict_survey_configuration[key]
                        elif key == "N_SUBBANDS":                           self.nsubbands                             = int(self.dict_survey_configuration[key])

                        elif key == "PERIOD_TO_SEARCH_MIN":                 self.period_to_search_min                  = np.float64(self.dict_survey_configuration[key])
                        elif key == "PERIOD_TO_SEARCH_MAX":                 self.period_to_search_max                  = np.float64(self.dict_survey_configuration[key])
                        elif key == "LIST_SEGMENTS":                        self.list_segments                         = self.dict_survey_configuration[key].split(",")

                        elif key == "RFIFIND_TIME":                         self.rfifind_time                          = self.dict_survey_configuration[key]
                        elif key == "RFIFIND_CHANS_TO_ZAP":                 self.rfifind_chans_to_zap                  = self.dict_survey_configuration[key]
                        elif key == "RFIFIND_TIME_INTERVALS_TO_ZAP":        self.rfifind_time_intervals_to_zap         = self.dict_survey_configuration[key]
                        elif key == "IGNORECHAN_LIST":                      self.ignorechan_list                       = self.dict_survey_configuration[key]
                        elif key == "ZAP_ISOLATED_PULSARS_FROM_FFTS":       self.zap_isolated_pulsars_from_ffts        = int(self.dict_survey_configuration[key])
                        elif key == "ZAP_ISOLATED_PULSARS_MAX_HARM":        self.zap_isolated_pulsars_max_harm         = int(self.dict_survey_configuration[key])
			
                        elif key == "FLAG_ACCELERATION_SEARCH":             self.flag_acceleration_search              = int(self.dict_survey_configuration[key])
                        elif key == "ACCELSEARCH_LIST_ZMAX":                self.accelsearch_list_zmax                 = [int(x) for x in self.dict_survey_configuration[key].split(",")]
                        elif key == "ACCELSEARCH_NUMHARM":                  self.accelsearch_numharm                   = int(self.dict_survey_configuration[key])

                        elif key == "FLAG_JERK_SEARCH":                     self.flag_jerk_search                      = int(self.dict_survey_configuration[key])
                        elif key == "JERKSEARCH_ZMAX":                      self.jerksearch_zmax                       = int(self.dict_survey_configuration[key])
                        elif key == "JERKSEARCH_WMAX":                      self.jerksearch_wmax                       = int(self.dict_survey_configuration[key])
                        elif key == "JERKSEARCH_NUMHARM":                   self.jerksearch_numharm                    = int(self.dict_survey_configuration[key])

                        elif key == "SIFTING_FLAG_REMOVE_DUPLICATES":       self.sifting_flag_remove_duplicates        = int(self.dict_survey_configuration[key])
                        elif key == "SIFTING_FLAG_REMOVE_DM_PROBLEMS":      self.sifting_flag_remove_dm_problems       = int(self.dict_survey_configuration[key])
                        elif key == "SIFTING_FLAG_REMOVE_HARMONICS":        self.sifting_flag_remove_harmonics         = int(self.dict_survey_configuration[key])
                        elif key == "SIFTING_MINIMUM_NUM_DMS":              self.sifting_minimum_num_DMs               = int(self.dict_survey_configuration[key])
                        elif key == "SIFTING_MINIMUM_DM":                   self.sifting_minimum_DM                    = np.float64(self.dict_survey_configuration[key])
                        elif key == "SIFTING_SIGMA_THRESHOLD":              self.sifting_sigma_threshold               = np.float64(self.dict_survey_configuration[key])

                        elif key == "FLAG_FOLD_KNOWN_PULSARS":              self.flag_fold_known_pulsars               = int(self.dict_survey_configuration[key])
                        elif key == "FLAG_FOLD_TIMESERIES":                 self.flag_fold_timeseries                  = int(self.dict_survey_configuration[key])
                        elif key == "FLAG_FOLD_RAWDATA":                    self.flag_fold_rawdata                     = int(self.dict_survey_configuration[key])

                        elif key == "RFIFIND_FLAGS":                        self.rfifind_flags                         = self.dict_survey_configuration[key]
                        elif key == "PREPDATA_FLAGS":                       self.prepdata_flags                        = self.dict_survey_configuration[key]
                        elif key == "PREPSUBBAND_FLAGS":                    self.prepsubband_flags                     = self.dict_survey_configuration[key]
                        elif key == "REALFFT_FLAGS":                        self.realfft_flags                         = self.dict_survey_configuration[key]
                        elif key == "REDNOISE_FLAGS":                       self.rednoise_flags                        = self.dict_survey_configuration[key]
                        elif key == "ACCELSEARCH_FLAGS":                    self.accelsearch_flags                     = self.dict_survey_configuration[key]
                        elif key == "ACCELSEARCH_GPU_FLAGS":                self.accelsearch_gpu_flags                 = self.dict_survey_configuration[key]
                        elif key == "ACCELSEARCH_JERK_FLAGS":               self.accelsearch_jerk_flags                = self.dict_survey_configuration[key]
                        elif key == "PREPFOLD_FLAGS":                       self.prepfold_flags                        = self.dict_survey_configuration[key]

                        elif key == "FLAG_SINGLEPULSE_SEARCH":              self.flag_singlepulse_search               = int(self.dict_survey_configuration[key])
                        elif key == "SINGLEPULSE_SEARCH_FLAGS":             self.singlepulse_search_flags              = self.dict_survey_configuration[key]

                        elif key == "USE_CUDA":                             self.flag_use_cuda                         = int(self.dict_survey_configuration[key])
                        elif key == "CUDA_IDS":                             self.list_cuda_ids                         = [int(x) for x in self.dict_survey_configuration[key].split(",")]

                        elif key == "NUM_SIMULTANEOUS_JERKSEARCHES":           self.num_simultaneous_jerksearches           = int(self.dict_survey_configuration[key])
                        elif key == "NUM_SIMULTANEOUS_PREPFOLDS":              self.num_simultaneous_prepfolds              = int(self.dict_survey_configuration[key])
                        elif key == "NUM_SIMULTANEOUS_PREPSUBBANDS":           self.num_simultaneous_prepsubbands           = int(self.dict_survey_configuration[key])
                        elif key == "NUM_SIMULTANEOUS_SINGLEPULSE_SEARCHES":   self.num_simultaneous_singlepulse_searches   = int(self.dict_survey_configuration[key])
                        elif key == "MAX_SIMULTANEOUS_DMS_PER_PREPSUBBAND":    self.max_simultaneous_dms_per_prepsubband    = int(self.dict_survey_configuration[key])

                        elif key == "FAST_BUFFER_DIR":                      self.fast_buffer_dir                       = self.dict_survey_configuration[key]
                        elif key == "FLAG_KEEP_DATA_IN_BUFFER_DIR":         self.flag_keep_data_in_buffer_dir          = int(self.dict_survey_configuration[key])
                        elif key == "FLAG_REMOVE_FFTFILES":                 self.flag_remove_fftfiles                  = int(self.dict_survey_configuration[key])
                        elif key == "FLAG_REMOVE_DATFILES_OF_SEGMENTS":     self.flag_remove_datfiles_of_segments      = int(self.dict_survey_configuration[key])

                        elif key == "STEP_RFIFIND":                         self.flag_step_rfifind                     = int(self.dict_survey_configuration[key])
                        elif key == "STEP_ZAPLIST":                         self.flag_step_zaplist                     = int(self.dict_survey_configuration[key])
                        elif key == "STEP_DEDISPERSE":                      self.flag_step_dedisperse                  = int(self.dict_survey_configuration[key])
                        elif key == "STEP_REALFFT":                         self.flag_step_realfft                     = int(self.dict_survey_configuration[key])
                        elif key == "STEP_PERIODICITY_SEARCH":              self.flag_step_periodicity_search          = int(self.dict_survey_configuration[key])
                        elif key == "STEP_SIFTING":                         self.flag_step_sifting                     = int(self.dict_survey_configuration[key])
                        elif key == "STEP_FOLDING":                         self.flag_step_folding                     = int(self.dict_survey_configuration[key])
                        elif key == "STEP_SINGLEPULSE_SEARCH":              self.flag_step_singlepulse_search          = int(self.dict_survey_configuration[key])


                config_file.close()

                self.log_filename = "%s.log" % (self.search_label)

                self.list_0DM_datfiles = []
                self.list_0DM_fftfiles = []
                self.list_0DM_fftfiles_rednoise = []

                if "full" in self.list_segments:
                        self.list_segments_nofull        = copy.deepcopy(self.list_segments)
                        self.list_segments_nofull.remove("full")
                        self.flag_search_full = 1
                else:
                        self.list_segments_nofull        = copy.deepcopy(self.list_segments)
                        self.flag_search_full = 0


                self.dict_chunks = {}      # {'filename': {'20m':   [0,1,2]}}
                self.dict_search_structure = {}
                if self.presto_gpu_env == "":
                        self.presto_gpu_env = self.presto_env

        def get_list_datafiles(self, list_datafiles_filename):
                list_datafiles_file = open(list_datafiles_filename, "r" )
                list_datafiles = [line.split()[0] for line in list_datafiles_file if not line.startswith("#") ] #Skip commented line
                list_datafiles_file.close()
                print("get_list_datafiles:: list_datafiles = ", list_datafiles)

                return list_datafiles

        def print_configuration(self):
                print("*********************************************")
                print("             SURVEY CONFIGURATION:")
                print("*********************************************")
                print()
                for param in self.list_survey_configuration_ordered_params:
                        print("%-32s %s" % (param, self.dict_survey_configuration[param]))
                print()


def init_default(observation_filename):

        # Create known_pulsars folder
        if not os.path.exists("known_pulsars"):
                os.mkdir("known_pulsars")
        if not os.path.exists("01_RFIFIND"):
                os.mkdir("01_RFIFIND")
                
        default_file_format = "filterbank"
        default_obs = "<observation>"
        if observation_filename != "":
                default_obs = observation_filename
                if observation_filename.endswith(".fil"):
                        print("Input file '%s' seems to be FILTERBANK. Setting default file format to 'filterbank'." % (observation_filename))
                        default_file_format = "filterbank"
                elif observation_filename.endswith(".fits") or observation_filename.endswith(".sf"):
                        if psrfits.is_PSRFITS(observation_filename) == True:
                                default_file_format = "psrfits"
                                print("Input file '%s' seems to be PSRFITS. Setting default file format to 'psrfits'." % (observation_filename))
                else:
                        print()
                        print("WARNING: Cannot determine format of input file '%s'.... Setting default file format to 'filterbank'." % (observation_filename))
        else:
                print("WARNING: Input observation file not provided. Setting default file format to 'filterbank'.")
        try:
                presto_path = os.environ['PRESTO']
        except:             presto_path = "*** PRESTO environment variable undefined ***" 

        try:
                presto_gpu_path = os.environ['PRESTO2_ON_GPU']
                use_cuda = '1'
        except:
                try:
                        presto_gpu_path = os.environ['PRESTO_ON_GPU']
                        use_cuda = '1'
                except:
                        try:
                                presto_gpu_path = os.environ['PRESTO']
                                use_cuda = '0'
                                print("WARNING: PRESTO2_ON_GPU / PRESTO_ON_GPU environment variables undefined - GPU acceleration will not be used!")

                        except:
                                print("ERROR: no PRESTO/PRESTO_ON_GPU environment variable seems to be defined!")
                                exit()

        dict_survey_configuration_default_values = {'SEARCH_LABEL':                          "%s                # Label of this search project" % os.path.basename(os.getcwd()),
                                                    'DATA_TYPE':                             "%-18s            # Options: filterbank, psrfits" % (default_file_format),
                                                    'ROOT_WORKDIR':                          "(cwd)            # Path of the root working directory. Options: /absolute/path or '(cwd)' for current working directory",
                                                    'PRESTO':                                "%s               # Path of the main PRESTO installation" % presto_path,
                                                    'PRESTO_GPU':                            "%s               # Path of the PRESTO_ON_GPU installation (if present) " % presto_gpu_path,
                                                    'DM_MIN':                                "2.0                    # Minimum DM to search",
                                                    'DM_MAX':                                "100.0                  # Maximum DM to search",
                                                    'DM_COHERENT_DEDISPERSION':              "0                      # DM value of possible coherent dedispersion (CDD) (0 = NO CDD)",
                                                    'N_SUBBANDS':                            "0                      # Number of subbands to use (0 = use all channels)",
                                                    'PERIOD_TO_SEARCH_MIN':                  "0.001                  # Mimimum acceptable candidate period (s) ",
                                                    'PERIOD_TO_SEARCH_MAX':                  "20.0                   # Maximum acceptable candidate period (s) ",
                                                    'LIST_SEGMENTS':                         "full                   # Comma-separated lengths (in minutes) of chunks to search (e.g. \"full,20,10\")",
                                                    'RFIFIND_TIME':                          "2.1                    # Value for RFIFIND -time option",
                                                    'RFIFIND_CHANS_TO_ZAP':                  "\"\"                     # List of channels to zap in the RFIFIND mask",
                                                    'RFIFIND_TIME_INTERVALS_TO_ZAP':         "\"\"                     # List of time intervals to zap in the RFIFIND mask",
                                                    'IGNORECHAN_LIST':                       "\"\"                     # List of channels to completey ignore from the analysis (PRESTO -ignorechan option)",
                                                    'ZAP_ISOLATED_PULSARS_FROM_FFTS':        "0                      # Zap the known pulsars in the power spectra? (1=yes, 0=no)",
                                                    'ZAP_ISOLATED_PULSARS_MAX_HARM':         "8                      # If zap the known pulsars in the power spectra, do it up to this harmonic ",
                                                    'FLAG_ACCELERATION_SEARCH':              "1                      # Perform acceleration search? (1=yes, 0=no)",
                                                    'ACCELSEARCH_LIST_ZMAX':                 "0,200                  # List (comma-separated) of zmax values to use with PRESTO accelsearch ",
                                                    'ACCELSEARCH_NUMHARM':                   "8                      # Number of harmonics to use for acceleration search",
                                                    'FLAG_JERK_SEARCH':                      "0                      # Perform acceleration search? (1=yes, 0=no)",
                                                    'JERKSEARCH_ZMAX':                       "100                    # Zmax value to use for jerk search",
                                                    'JERKSEARCH_WMAX':                       "300                    # Wmax value to use for jerk search (0 = do not do jerk search)",
                                                    'JERKSEARCH_NUMHARM':                    "4                      # Number of harmonics to use for jerk search",
                                                    'SIFTING_FLAG_REMOVE_DUPLICATES' :       "1                      # Remove candidate duplicates when sifting? (1=yes, 0=no)",
                                                    'SIFTING_FLAG_REMOVE_DM_PROBLEMS' :      "1                      # Remove candidates that appear in few DM values? (1=yes, 0=no)",
                                                    'SIFTING_FLAG_REMOVE_HARMONICS' :        "1                      # Remove harmoniacally related candidates? (1=yes, 0=no)",
                                                    'SIFTING_MINIMUM_NUM_DMS' :              "3                      # Minimum number of DM values at which a candidate has to appear in order to be considered 'good'",
                                                    'SIFTING_MINIMUM_DM' :                   "2.0                    # Minimum DM value at  at which a candidate has to appear in order to be considered 'good'",
                                                    'SIFTING_SIGMA_THRESHOLD' :              "4.0                    # Minimum acceptable significance of a candidate",
                                                    'FLAG_FOLD_KNOWN_PULSARS' :              "1                      # Fold candidates that are likely redetections of known pulsars? (1=yes, 0=no)",
                                                    'FLAG_FOLD_TIMESERIES' :                 "0                      # Fold the candidates using the time series (super-fast, but no frequency information)? (1=yes, 0=no)",
                                                    'FLAG_FOLD_RAWDATA':                     "1                      # Fold the candidates using raw data file (slow, but has all the information)? (1=yes, 0=no)",
                                                    'RFIFIND_FLAGS':                         "\"\"                     # Any additional options to give to RFIFIND",
                                                    'PREPDATA_FLAGS':                        "\"\"                     # Any additional options to give to PREPDATA",
                                                    'PREPSUBBAND_FLAGS':                     "\"-ncpus 4\"             # Any additional options to give to PREPSUBBAND",
                                                    'REALFFT_FLAGS':                         "\"\"                     # Any additional options to give to REALFFT",
                                                    'REDNOISE_FLAGS':                        "\"\"                     # Any additional options to give to REDNOISE",
                                                    'ACCELSEARCH_FLAGS':                     "\"\"                     # Any additional options to give to ACCELSEARCH when doing acceleration search",
                                                    'ACCELSEARCH_GPU_FLAGS':                 "\"\"                     # Any additional options to give to ACCELSEARCH when doing acceleration search with PRESTO_ON_GPU",
                                                    'ACCELSEARCH_JERK_FLAGS':                "\"\"                     # Any additional options to give to ACCELSEARCH when doing jerk search",
                                                    'PREPFOLD_FLAGS':                        "\"-ncpus %-3d -n 64\"     # Any additional options to give to PREPFOLD" % (multiprocessing.cpu_count()/4),
                                                    'FLAG_SINGLEPULSE_SEARCH':               "1                      # Perform single-pulse search? (1=yes, 0=no)",
                                                    'SINGLEPULSE_SEARCH_FLAGS':              "\"\"                   # Any additional options to give to SINGLE_PULSE_SEARCH.py when doing single-pulse search",
                                                    'USE_CUDA':                              "%s                      # Use GPU-acceleration? (1=yes, 0=no)" % use_cuda,
                                                    'CUDA_IDS':                              "0                      # Comma-separated ids of NVIDIA GPUs to use (e.g. \"0,1,2,3\" - check with 'nvidia-smi')",
                                                    'NUM_SIMULTANEOUS_JERKSEARCHES':         "%-4d                   # Number of jerk search instances to run at once" % (multiprocessing.cpu_count()),
                                                    'NUM_SIMULTANEOUS_PREPFOLDS':            "4                      # Max number of prepfold instances to run at once",
                                                    'NUM_SIMULTANEOUS_PREPSUBBANDS':         "%-4d                   # Maximum number of prepsubband instances to run at once" % (multiprocessing.cpu_count()/4),
                                                    'MAX_SIMULTANEOUS_DMS_PER_PREPSUBBAND':  "1000                   # Number of simultaneous DM values processed at once by prepsubband (max 1000) ",
                                                    'NUM_SIMULTANEOUS_SINGLEPULSE_SEARCHES': "%-4d                   # Number of single-pulse search instances to run at once" % (multiprocessing.cpu_count()),
                                                    'FAST_BUFFER_DIR':                       "\"\"                     # Path of a SSD/Fast memory buffer (minimizes I/O bottlenecks, optional)",
                                                    'FLAG_KEEP_DATA_IN_BUFFER_DIR':          "0                      # Keep copy of observation in buffer after searching? (1=yes, 0=no)",
                                                    'FLAG_REMOVE_FFTFILES':                  "1                      # Remove FFT files after searching to save disk space? (1=yes, 0=no)",
                                                    'FLAG_REMOVE_DATFILES_OF_SEGMENTS':      "1                      # Remove .dat files of the shorter segments after searching to save disk space? (1=yes, 0=no)",                                                     
                                                    'STEP_RFIFIND':                          "1                      # Run the RFIFIND step? (1=yes, 0=no)",
                                                    'STEP_ZAPLIST':                          "1                      # Run the ZAPLIST step? (1=yes, 0=no)",
                                                    'STEP_DEDISPERSE':                       "1                      # Run the DEDISPERSION step? (1=yes, 0=no)",
                                                    'STEP_REALFFT':                          "1                      # Run the REALFFT step? (1=yes, 0=no)",
                                                    'STEP_PERIODICITY_SEARCH':               "1                      # Run the PERIODICITY SEARCH step? (1=yes, 0=no)",
                                                    'STEP_SIFTING':                          "1                      # Run the SIFTING step? (1=yes, 0=no)",
                                                    'STEP_FOLDING':                          "1                      # Run the FOLDING step? (1=yes, 0=no)",
                                                    'STEP_SINGLEPULSE_SEARCH':               "1                      # Run the SINGLEPULSE SEARCH step? (1=yes, 0=no)"
                                                    }

        default_cfg_filename = "%s.cfg" % (os.path.basename(os.getcwd()))
        if os.path.exists(default_cfg_filename):
                default_cfg_filename_existing = default_cfg_filename
                default_cfg_filename = "%s_2.cfg" % (os.path.basename(os.getcwd()))
                print("******************")
                print("WARNING: '%s' already exists! Saving the default configuration onto file '%s'" % (default_cfg_filename_existing, default_cfg_filename))
                print("******************")
                print()
        with open(default_cfg_filename, "w") as f:
                f.write("#===============================================================\n")
                f.write("# General parameters\n")
                f.write("#===============================================================\n")
                f.write("%-40s %s\n" % ('SEARCH_LABEL', dict_survey_configuration_default_values['SEARCH_LABEL']))
                f.write("%-40s %s\n" % ('DATA_TYPE', dict_survey_configuration_default_values['DATA_TYPE']))
                f.write("%-40s %s\n" % ('ROOT_WORKDIR', dict_survey_configuration_default_values['ROOT_WORKDIR']))
                f.write("%-40s %s\n" % ('PRESTO', dict_survey_configuration_default_values['PRESTO']))
                f.write("%-40s %s\n" % ('PRESTO_GPU', dict_survey_configuration_default_values['PRESTO_GPU']))
                f.write("\n")
                f.write("#===============================================================\n")
                f.write("# Core search parameters\n")
                f.write("#===============================================================\n")
                f.write("%-40s %s\n" % ('DM_MIN', dict_survey_configuration_default_values['DM_MIN']))
                f.write("%-40s %s\n" % ('DM_MAX', dict_survey_configuration_default_values['DM_MAX']))
                f.write("%-40s %s\n" % ('DM_COHERENT_DEDISPERSION', dict_survey_configuration_default_values['DM_COHERENT_DEDISPERSION']))
                f.write("%-40s %s\n" % ('N_SUBBANDS', dict_survey_configuration_default_values['N_SUBBANDS']))
                f.write("\n")                
                f.write("%-40s %s\n" % ('PERIOD_TO_SEARCH_MIN', dict_survey_configuration_default_values['PERIOD_TO_SEARCH_MIN']))
                f.write("%-40s %s\n" % ('PERIOD_TO_SEARCH_MAX', dict_survey_configuration_default_values['PERIOD_TO_SEARCH_MAX']))
                f.write("\n")
                f.write("%-40s %s\n" % ('LIST_SEGMENTS', dict_survey_configuration_default_values['LIST_SEGMENTS']))
                f.write("\n")
                f.write("#===============================================================\n")
                f.write("# Fourier domain search with PRESTO\n")
                f.write("#===============================================================\n")
                f.write("%-40s %s\n" % ('RFIFIND_TIME', dict_survey_configuration_default_values['RFIFIND_TIME']))
                f.write("%-40s %s\n" % ('RFIFIND_CHANS_TO_ZAP', dict_survey_configuration_default_values['RFIFIND_CHANS_TO_ZAP']))
                f.write("%-40s %s\n" % ('RFIFIND_TIME_INTERVALS_TO_ZAP', dict_survey_configuration_default_values['RFIFIND_TIME_INTERVALS_TO_ZAP']))
                f.write("%-40s %s\n" % ('IGNORECHAN_LIST', dict_survey_configuration_default_values['IGNORECHAN_LIST']))
                f.write("\n")
                f.write("%-40s %s\n" % ('ZAP_ISOLATED_PULSARS_FROM_FFTS', dict_survey_configuration_default_values['ZAP_ISOLATED_PULSARS_FROM_FFTS']))
                f.write("%-40s %s\n" % ('ZAP_ISOLATED_PULSARS_MAX_HARM', dict_survey_configuration_default_values['ZAP_ISOLATED_PULSARS_MAX_HARM']))
                f.write("\n")
                f.write("%-40s %s\n" % ('FLAG_ACCELERATION_SEARCH', dict_survey_configuration_default_values['FLAG_ACCELERATION_SEARCH']))
                f.write("%-40s %s\n" % ('ACCELSEARCH_LIST_ZMAX', dict_survey_configuration_default_values['ACCELSEARCH_LIST_ZMAX']))
                f.write("%-40s %s\n" % ('ACCELSEARCH_NUMHARM', dict_survey_configuration_default_values['ACCELSEARCH_NUMHARM']))
                f.write("\n")
                f.write("%-40s %s\n" % ('FLAG_JERK_SEARCH', dict_survey_configuration_default_values['FLAG_JERK_SEARCH']))
                f.write("%-40s %s\n" % ('JERKSEARCH_ZMAX', dict_survey_configuration_default_values['JERKSEARCH_ZMAX']))
                f.write("%-40s %s\n" % ('JERKSEARCH_WMAX', dict_survey_configuration_default_values['JERKSEARCH_WMAX']))
                f.write("%-40s %s\n" % ('JERKSEARCH_NUMHARM', dict_survey_configuration_default_values['JERKSEARCH_NUMHARM']))
                f.write("\n")
                f.write("%-40s %s\n" % ('SIFTING_FLAG_REMOVE_DUPLICATES', dict_survey_configuration_default_values['SIFTING_FLAG_REMOVE_DUPLICATES']))
                f.write("%-40s %s\n" % ('SIFTING_FLAG_REMOVE_DM_PROBLEMS', dict_survey_configuration_default_values['SIFTING_FLAG_REMOVE_DM_PROBLEMS']))
                f.write("%-40s %s\n" % ('SIFTING_FLAG_REMOVE_HARMONICS', dict_survey_configuration_default_values['SIFTING_FLAG_REMOVE_HARMONICS']))
                f.write("%-40s %s\n" % ('SIFTING_MINIMUM_NUM_DMS', dict_survey_configuration_default_values['SIFTING_MINIMUM_NUM_DMS']))
                f.write("%-40s %s\n" % ('SIFTING_MINIMUM_DM', dict_survey_configuration_default_values['SIFTING_MINIMUM_DM']))
                f.write("%-40s %s\n" % ('SIFTING_SIGMA_THRESHOLD', dict_survey_configuration_default_values['SIFTING_SIGMA_THRESHOLD']))
                f.write("\n")
                f.write("%-40s %s\n" % ('FLAG_FOLD_KNOWN_PULSARS', dict_survey_configuration_default_values['FLAG_FOLD_KNOWN_PULSARS']))
                f.write("%-40s %s\n" % ('FLAG_FOLD_TIMESERIES', dict_survey_configuration_default_values['FLAG_FOLD_TIMESERIES']))
                f.write("%-40s %s\n" % ('FLAG_FOLD_RAWDATA', dict_survey_configuration_default_values['FLAG_FOLD_RAWDATA']))
                f.write("\n")
                f.write("%-40s %s\n" % ('RFIFIND_FLAGS', dict_survey_configuration_default_values['RFIFIND_FLAGS']))
                f.write("%-40s %s\n" % ('PREPDATA_FLAGS', dict_survey_configuration_default_values['PREPDATA_FLAGS']))
                f.write("%-40s %s\n" % ('PREPSUBBAND_FLAGS', dict_survey_configuration_default_values['PREPSUBBAND_FLAGS']))
                f.write("%-40s %s\n" % ('REALFFT_FLAGS', dict_survey_configuration_default_values['REALFFT_FLAGS']))
                f.write("%-40s %s\n" % ('REDNOISE_FLAGS', dict_survey_configuration_default_values['REDNOISE_FLAGS']))
                f.write("%-40s %s\n" % ('ACCELSEARCH_FLAGS', dict_survey_configuration_default_values['ACCELSEARCH_FLAGS']))
                f.write("%-40s %s\n" % ('ACCELSEARCH_GPU_FLAGS', dict_survey_configuration_default_values['ACCELSEARCH_GPU_FLAGS']))
                f.write("%-40s %s\n" % ('ACCELSEARCH_JERK_FLAGS', dict_survey_configuration_default_values['ACCELSEARCH_JERK_FLAGS']))
                f.write("%-40s %s\n" % ('PREPFOLD_FLAGS', dict_survey_configuration_default_values['PREPFOLD_FLAGS']))
                f.write("\n")
                f.write("#===============================================================\n")
                f.write("# Single pulse search with PRESTO\n")
                f.write("#===============================================================\n")
                f.write("%-40s %s\n" % ('FLAG_SINGLEPULSE_SEARCH', dict_survey_configuration_default_values['FLAG_SINGLEPULSE_SEARCH']))
                f.write("%-40s %s\n" % ('SINGLEPULSE_SEARCH_FLAGS', dict_survey_configuration_default_values['SINGLEPULSE_SEARCH_FLAGS']))
                f.write("\n")
                f.write("#===============================================================\n")
                f.write("# Computational/Performance parameters\n")
                f.write("#===============================================================\n")
                f.write("%-40s %s\n" % ('USE_CUDA', dict_survey_configuration_default_values['USE_CUDA']))
                f.write("%-40s %s\n" % ('CUDA_IDS', dict_survey_configuration_default_values['CUDA_IDS']))
                f.write("\n")
                f.write("%-40s %s\n" % ('NUM_SIMULTANEOUS_JERKSEARCHES', dict_survey_configuration_default_values['NUM_SIMULTANEOUS_JERKSEARCHES']))
                f.write("%-40s %s\n" % ('NUM_SIMULTANEOUS_PREPFOLDS', dict_survey_configuration_default_values['NUM_SIMULTANEOUS_PREPFOLDS']))
                f.write("%-40s %s\n" % ('NUM_SIMULTANEOUS_PREPSUBBANDS', dict_survey_configuration_default_values['NUM_SIMULTANEOUS_PREPSUBBANDS']))
                f.write("%-40s %s\n" % ('MAX_SIMULTANEOUS_DMS_PER_PREPSUBBAND', dict_survey_configuration_default_values['MAX_SIMULTANEOUS_DMS_PER_PREPSUBBAND']))
                f.write("%-40s %s\n" % ('NUM_SIMULTANEOUS_SINGLEPULSE_SEARCHES', dict_survey_configuration_default_values['NUM_SIMULTANEOUS_SINGLEPULSE_SEARCHES']))
                f.write("\n")
                f.write("%-40s %s\n" % ('FAST_BUFFER_DIR', dict_survey_configuration_default_values['FAST_BUFFER_DIR']))
                f.write("%-40s %s\n" % ('FLAG_KEEP_DATA_IN_BUFFER_DIR', dict_survey_configuration_default_values['FLAG_KEEP_DATA_IN_BUFFER_DIR'])) 
                f.write("%-40s %s\n" % ('FLAG_REMOVE_FFTFILES', dict_survey_configuration_default_values['FLAG_REMOVE_FFTFILES']))
                f.write("%-40s %s\n" % ('FLAG_REMOVE_DATFILES_OF_SEGMENTS', dict_survey_configuration_default_values['FLAG_REMOVE_DATFILES_OF_SEGMENTS']))
                f.write("\n")                        
                f.write("#===============================================================\n")
                f.write("# Pipeline steps to execute (1=do, 0=skip)\n")
                f.write("#===============================================================\n")
                f.write("%-40s %s\n" % ('STEP_RFIFIND', dict_survey_configuration_default_values['STEP_RFIFIND']))
                f.write("%-40s %s\n" % ('STEP_ZAPLIST', dict_survey_configuration_default_values['STEP_ZAPLIST']))
                f.write("%-40s %s\n" % ('STEP_DEDISPERSE', dict_survey_configuration_default_values['STEP_DEDISPERSE']))
                f.write("%-40s %s\n" % ('STEP_REALFFT', dict_survey_configuration_default_values['STEP_REALFFT']))
                f.write("%-40s %s\n" % ('STEP_PERIODICITY_SEARCH', dict_survey_configuration_default_values['STEP_PERIODICITY_SEARCH']))
                f.write("%-40s %s\n" % ('STEP_SIFTING', dict_survey_configuration_default_values['STEP_SIFTING']))
                f.write("%-40s %s\n" % ('STEP_FOLDING', dict_survey_configuration_default_values['STEP_FOLDING']))
                f.write("%-40s %s\n" % ('STEP_SINGLEPULSE_SEARCH', dict_survey_configuration_default_values['STEP_SINGLEPULSE_SEARCH']))
                f.write("\n")
                f.write("#===============================================================\n")
                f.write("# Configuration file made with PULSAR_MINER version %s\n" % string_version_full)


        print()
        print("Default configuration written onto '%s'." % (default_cfg_filename))

        with open("common_birdies.txt", "w") as f:
                f.write("10.00   0.003     2     1     0\n")
                f.write("30.00    0.008     2     1     0\n")
                f.write("50.00    0.08      3     1     0\n")
        print("Some common birdies written on 'common_birdies.txt'.")
        print()
        print("Place the parfiles of the already-known pulsars, if any, in 'known_pulsars'")
        print()
        print("It is also recommended to create the rfifind mask of the observation separately, so as to make sure that a reasonable fraction of the band is masked.")
        print("Once you are happy with the mask, place all the relative files in the '01_RFIFIND' directory, making sure that their basenames correspond to the basename of the observation (e.g. 'myobs.fil' must have corresponding 'myobs_rfifind.mask', 'myobs_rfifind.inf' etc. files in '01_RFIFIND'), so that PULSAR_MINER can associate the mask to the observation and use it.")
        print()
        print("Now edit the config file, adjust the parameters and run the pipeline with:")
        print("%s -config %s -obs %s" % (os.path.basename(sys.argv[0]), default_cfg_filename, default_obs))
        print()
        exit()


def common_start_string(sa, sb):
        """ returns the longest common substring from the beginning of sa and sb """
        def _iter():
                for a, b in zip(sa, sb):
                        if a == b:
                                yield a
                        else:
                                return

        return ''.join(_iter())


###########################################################################################################################################################
verbosity_level = 1
obsname = ""
flag_update_pulsar_miner = False
dir_pulsar_miner = os.path.abspath( os.path.dirname( __file__ ) )

# SHELL ARGUMENTS
if (len(sys.argv) == 1 or ("-h" in sys.argv) or ("-help" in sys.argv) or ("--help" in sys.argv)):
        print("USAGE: %s -config <config_file> -obs <observation_file> [{-Q | -v | -V}]" % (os.path.basename(sys.argv[0])))
        print()
        print("%10s  %-32s:  %-50s" % ("-h", "", "Print help"))
        print("%10s  %-32s:  %-50s %s" % ("-config", "<config_file>", "Input search configuration file", ""))
        print("%10s  %-32s:  %-50s" % ("-obs", "<observation_file>", "Data file to search"))
        print("%10s  %-32s:  %-50s" % ("-Q", "", "Quiet mode - do not print anything"))
        print("%10s  %-32s:  %-50s" % ("-v", "", "Verbose mode - print more useful info about the processing"))
        print("%10s  %-32s:  %-50s" % ("-V", "", "Very verbose mode - print everything to debug issues"))
        print("%10s  %-32s:  %-50s" % ("-version", "", "Print code version"))
        print("%10s  %-32s:  %-50s" % ("-update", "", "Check for new versions and ask if want to update"))
        print()
        print("To create default configuration files: \033[1m%s -init_default [<observationfile>]\033[0m" % (os.path.basename(sys.argv[0])))
        print()
        exit()
elif (("-version" in sys.argv) or ("--version" in sys.argv)):
        print("PULSAR_MINER version: %s" % (string_version_full))
        exit()
else:
        for j in range(1, len(sys.argv)):
                if (sys.argv[j] == "-config"):
                        config_filename = sys.argv[j+1]
                elif (sys.argv[j] == "-obs"):
                        obsname = sys.argv[j+1]
                elif (sys.argv[j] == "-init_default"):
                        try:
                                observation_filename = sys.argv[j+1]
                        except: observation_filename = ""
                        init_default(observation_filename)
                elif (sys.argv[j] == "-Q"):
                        verbosity_level = 0
                elif (sys.argv[j] == "-v"):
                        verbosity_level = 2
                elif (sys.argv[j] == "-V"):
                        verbosity_level = 3
                elif (sys.argv[j] == "-update"):
                        number_latest_version, string_latest_version = check_for_updates(number_version, string_version, flag_verbose=True)
                        if number_latest_version > number_version:
                                print("%sA new version (%s-%s) of PULSAR_MINER is available (your version: %s-%s).%s" % (colors.OKBLUE+colors.BOLD, string_latest_version, number_latest_version, string_version, number_version, colors.ENDCOLOR))
                                string_answer = input("Proceed and install the new version? (y/n): ")
                                if string_answer == "y" or string_answer == "Y":
                                        update_pulsar_miner(dir_pulsar_miner, string_version, string_latest_version)
                        elif  number_latest_version == number_version:
                                print("%sGOOD NEWS!%s Your PULSAR_MINER is up-to-date! (current version: %s-%s)." % (colors.OKGREEN+colors.BOLD, colors.ENDCOLOR, string_version, number_version))
                               
                        elif number_latest_version < number_version:
                                print("Your PULSAR_MINER version (%s-%s) is actually ahead of the online repo (%s-%s)! :)" % (string_version, number_version, string_latest_version, number_latest_version))
                        print()
                        exit()
                        
pipeline_start_datetime_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
if verbosity_level >= 1:
        size_logo = 64
        size_string_version = len(string_version_full)
        dsize = int((size_logo - size_string_version)*0.5 - 1)
        dsize_offset = size_string_version % 2

        print()
        print("#"*size_logo)
        print("#" + " "*25 + "%s" % ("PULSAR MINER") + " "*25 + "#")
        print("#" + " "*(dsize + dsize_offset) + "%s" % (string_version_full) + " "*dsize + "#")
        print("#"*size_logo)
        print()
        print("Command: %s" % (" ".join(sys.argv)))
        print("Launch date_time: %s" % (pipeline_start_datetime_str) )
        print()


number_latest_version, string_latest_version = check_for_updates(number_version, string_version, flag_verbose=False)
if number_latest_version > number_version:
        print("%sTIP: A new version (%s-%s) of PULSAR_MINER is available (current version: %s-%s).%s" % (colors.OKBLUE+colors.BOLD, string_latest_version, number_latest_version, string_version, number_version, colors.ENDCOLOR))
        print("           Run 'pulsar_miner -update' to install the latest version.")
        print()
        time.sleep(5)

config = SurveyConfiguration(config_filename, verbosity_level)


# If option '-obs' was forgotten....
if obsname =="":
        print()
        print("%sERROR:%s Please give the observation file through the '-obs' option." % (colors.ERROR+colors.BOLD, colors.ENDCOLOR))
        exit()

# Else...
elif obsname !="":
        config.list_datafiles            = [os.path.basename(x) for x in glob.glob(obsname)]
        if len(config.list_datafiles) == 0:
                print()
                print("%sERROR:%s No observation file was found! Are you sure that the file name is correct?" % (colors.ERROR+colors.BOLD, colors.ENDCOLOR))
                exit()
        elif len(config.list_datafiles) >= 1:
                for f in config.list_datafiles:
                        if not os.path.exists(f):
                                print()
                                print("%sERROR:%s File '%s' does not exists! Perhaps you are using a broken symbolic link?" % (colors.ERROR+colors.BOLD, colors.ENDCOLOR, f))
                                exit()
                        elif os.path.getsize(f) == 0:
                                print()
                                print("%sERROR:%s File '%s' has size 0!" % (colors.ERROR+colors.BOLD, colors.ENDCOLOR, f))
                                exit()

        
        if verbosity_level >= 3:
                print("config.list_datafiles = ", config.list_datafiles)
        config.folder_datafiles           = os.path.dirname(os.path.abspath(obsname))   # ...the location of the input file is the location of all the data


# Now create the list of observations to search with their absolute paths
config.list_datafiles_abspath = [os.path.join(config.folder_datafiles, x) for x in config.list_datafiles]

# And create the list of Observation objects
config.list_Observations = [Observation(x, config.data_type) for x in config.list_datafiles_abspath]

# Add the common birdies file
config.file_common_birdies = os.path.join(config.root_workdir, "common_birdies.txt")


################################################################################
#   IMPORT PARFILES OF KNOWN PULSARS
################################################################################

dir_known_pulsars = os.path.join(config.root_workdir, "known_pulsars")

list_known_pulsars = []
if os.path.exists(dir_known_pulsars):
        list_parfilenames = sorted(glob.glob("%s/*.par" % dir_known_pulsars))
        dict_freqs_to_zap = {}

        for k in range(len(list_parfilenames)):
                current_pulsar = Pulsar(list_parfilenames[k])
                list_known_pulsars.append(current_pulsar)

                if not current_pulsar.is_binary:
                        current_freq = psr_utils.calc_freq(config.list_Observations[0].Tstart_MJD, current_pulsar.PEPOCH, current_pulsar.F0, current_pulsar.F1, current_pulsar.F2   )
                        dict_freqs_to_zap[current_pulsar.psr_name] = current_freq

                if verbosity_level >= 1:
                        print("Reading '%s' --> Added %s to the list of known pulsars (%s)" % (os.path.basename(list_parfilenames[k]), current_pulsar.psr_name, current_pulsar.pulsar_type))

        if verbosity_level >= 1:
                if config.zap_isolated_pulsars_from_ffts == 1:
                        print()
                        print()
                        print("WARNING: I will zap the Fourier frequencies of the isolated pulsars (up to the %d-th harmonic), namely" % (config.zap_isolated_pulsars_max_harm))
                        print()
                        for key in sorted(dict_freqs_to_zap.keys()):
                                print("%s  -->  Barycentric frequency at the epoch of the observation: %.14f Hz" % (key, dict_freqs_to_zap[key]))
                        print()

list_segments_to_remove = []
for seg in config.list_segments_nofull:
        if (np.float64(seg)*60) >= config.list_Observations[0].T_obs_s:
                print("%sWARNING%s: Segment %sm is longer than the length of the full observation (%d minutes). Will be ignored." % (colors.WARNING+colors.BOLD, colors.ENDCOLOR, seg, config.list_Observations[0].T_obs_s / 60.) )
                list_segments_to_remove.append(seg)
        elif (np.float64(seg)*60) >= 0.80 * config.list_Observations[0].T_obs_s:
                print("%sWARNING%s: Segment %sm is more than 80%% of the length of the full observation (%d minutes). Will be ignored." % (colors.WARNING+colors.BOLD, colors.ENDCOLOR, seg, config.list_Observations[0].T_obs_s / 60.) )
                list_segments_to_remove.append(seg)

#Remove large segments
for seg in list_segments_to_remove:
        config.list_segments_nofull.remove(seg)
        config.list_segments.remove(seg)

if verbosity_level >= 2:
        config.print_configuration()

sifting.sigma_threshold = config.sifting_sigma_threshold

if verbosity_level >= 2:
        print("main:: SIFTING.sigma_threshold = ", sifting.sigma_threshold)

LOG_dir = os.path.join(config.root_workdir, "LOG")


for i in range(len(config.list_Observations)):
        if verbosity_level >= 1:
                print()
                print("%s: \033[1m %s \033[0m  (%.2f s)" % ("Observation", config.list_Observations[i].file_nameonly, config.list_Observations[i].T_obs_s))
                print()

        # Check that observation base name is max 32-character long, to prevent file name truncation with prepfold
        if len(config.list_Observations[i].file_basename) > 32:
                print("%sERROR:%s Base name of observation ('%s') too long!" % (colors.ERROR+colors.BOLD, colors.ENDCOLOR, config.list_Observations[i].file_basename))
                print("       Please rename the file so that its base name (= file name without the extension) be max 32 characters long.")
                exit()
        
        
        if config.fast_buffer_dir != "":
                if os.path.exists(config.fast_buffer_dir):
                
                        file_buffer_abspath = os.path.join(config.fast_buffer_dir,config.list_Observations[i].file_nameonly)
                        if (not os.path.exists(file_buffer_abspath) or (os.path.getsize(file_buffer_abspath) != os.path.getsize(config.list_Observations[i].file_abspath))) :

                                if (os.path.getsize(config.list_Observations[i].file_abspath) <  shutil.disk_usage(config.fast_buffer_dir).free):
                                        print()
                                        print("Copying '%s' to the fast buffer directory '%s' (this may take a while)..." % (config.list_Observations[i].file_nameonly, config.fast_buffer_dir), end=""); sys.stdout.flush()
                                        file_buffer_abspath = shutil.copy(config.list_Observations[i].file_abspath, config.fast_buffer_dir)
                                        print("done!")
                                
                                        config.list_Observations[i].file_abspath = file_buffer_abspath
                                        config.list_Observations[i].file_buffer_copy = file_buffer_abspath

                                        print("Now config.list_Observations[i].file_abspath = ", config.list_Observations[i].file_abspath)
                                else:
                                        print()
                                        print("%sWARNING:%s Not enough space on fast buffer directory '%s'!" % (colors.WARNING+colors.BOLD, colors.ENDCOLOR, config.fast_buffer_dir) )
                                        print("    -->  Fast buffer will not be used. This may result in slower processing...")
                                        print()
                                        #time.sleep(10)

                        else:
                                print("Copy of '%s' already present in the fast buffer directory '%s'. Skipping..." % (config.list_Observations[i].file_nameonly, config.fast_buffer_dir))
                                file_buffer_abspath = os.path.join(config.fast_buffer_dir, config.list_Observations[i].file_nameonly)
                                config.list_Observations[i].file_abspath = file_buffer_abspath
                                config.list_Observations[i].file_buffer_copy = file_buffer_abspath
                                print()
                                print("The observation file used is now '%s'. " % (config.list_Observations[i].file_abspath))

                else:
                        print("%sWARNING:%s Fast buffer directory '%s' does not seem to exist!" % (colors.WARNING+colors.BOLD, colors.ENDCOLOR, config.fast_buffer_dir) )
                        print("    -->  Fast buffer will not be used. This may result in slower processing...")
                        print()
                        config.fast_buffer_dir = ""
                        #time.sleep(10)

        if verbosity_level >= 1:
                print()
                print()
                print("*******************************************************************")
                print("SEARCH SCHEME:")
                print("*******************************************************************")

        config.dict_search_structure[config.list_Observations[i].file_basename] = {}
        for s in config.list_segments:
                if verbosity_level >= 2:
                        print("Segment = %s of %s" % (s, config.list_segments))
                if s == "full":
                        segment_length_s             = config.list_Observations[i].T_obs_s
                        segment_length_min    = config.list_Observations[i].T_obs_s / 60.
                        segment_label = s
                else:
                        segment_length_min  = np.float64(s)
                        segment_length_s = np.float64(s) * 60
                        segment_label = "%dm" % (segment_length_min)
                        
                config.dict_search_structure[config.list_Observations[i].file_basename][segment_label] = {}

                N_chunks = int(config.list_Observations[i].T_obs_s / segment_length_s)
                fraction_left = (config.list_Observations[i].T_obs_s % segment_length_s) / segment_length_s
                if fraction_left >= 0.80:
                        N_chunks = N_chunks + 1
                                
                for ck in range(N_chunks):
                        chunk_label = "ck%02d" % (ck)
                        config.dict_search_structure[config.list_Observations[i].file_basename][segment_label][chunk_label] = {'candidates': []}

                print("    Segment: %8s     ---> %2d chunks (%s)" % (segment_label, N_chunks, ", ".join(sorted(config.dict_search_structure[config.list_Observations[i].file_basename][segment_label].keys()))), end=' ')

                if fraction_left >= 0.80:
                        print(" --> %sWARNING:%s Last chunk of segment '%sm' is in fact a bit shorter (%.2f minutes)!" % (colors.WARNING+colors.BOLD, colors.ENDCOLOR, s, fraction_left*segment_length_min))
                elif fraction_left > 0.10 and fraction_left < 0.80:
                        print("--> %sWARNING:%s Last chunk (ck%02d) of segment '%sm' is only %d minutes and will be ignored!" % (colors.WARNING+colors.BOLD, colors.ENDCOLOR, ck+1, s, fraction_left*segment_length_min))
                else:
                        print()


                
if verbosity_level >= 1:
        print()
        print("*******************************************************************")
        print()
        print()
        print()

if verbosity_level >= 2:
        print("config.dict_search_structure:")
        print(config.dict_search_structure)

if not os.path.exists(LOG_dir):
        os.mkdir(LOG_dir)
        
list_DDplan_scheme = get_DDplan_scheme(config.list_Observations[i].file_abspath,
                                       LOG_dir,
                                       LOG_dir,
                                       "LOG_diskspace",
                                       config.dm_min,
                                       config.dm_max,
                                       config.dm_coherent_dedispersion,
                                       config.max_simultaneous_dms_per_prepsubband,
                                       config.list_Observations[i].freq_central_MHz,
                                       config.list_Observations[i].bw_MHz,
                                       config.list_Observations[i].nchan,
                                       config.nsubbands,
                                       config.list_Observations[i].t_samp_s)




############################################################################################
#    CHECK DISK SPACE
############################################################################################

num_DMs = 0 
for j in range(len(list_DDplan_scheme)):
        num_DMs = num_DMs + list_DDplan_scheme[j]['num_DMs']
        
flag_enough_disk_space = False
flag_enough_disk_space = check_if_enough_disk_space(config.root_workdir, num_DMs, config.list_Observations[i].T_obs_s, config.list_Observations[i].t_samp_s, config.list_segments_nofull, config.flag_remove_fftfiles, config.flag_remove_datfiles_of_segments)

if flag_enough_disk_space == False:
        print()
        print("%sERROR:%s Not enough space on disk! Please free up space or change the working directory." % (colors.ERROR+colors.BOLD, colors.ENDCOLOR))
        print("> TIP: To minimize disk usage, make sure to leave FLAG_REMOVE_FFTFILES and FLAG_REMOVE_DATFILES_OF_SEGMENTS to their default value of 1 in the config file.")
        exit()

###########################################################################################
if verbosity_level >= 1:
        print()
        print("##################################################################################################")
        print("                                           STEP 1 - RFIFIND                                       ")
        print("##################################################################################################")
        print()


rfifind_masks_dir = os.path.join(config.root_workdir, "01_RFIFIND")

if not os.path.exists(rfifind_masks_dir):
        os.mkdir(rfifind_masks_dir)


for i in range(len(config.list_Observations)):
        time.sleep(0.2)
        config.list_Observations[i].mask = "%s/%s_rfifind.mask" % (rfifind_masks_dir, config.list_Observations[i].file_basename)

        flag_mask_present = check_rfifind_outfiles(rfifind_masks_dir, config.list_Observations[i].file_basename)

        # CASE 1: mask not present, STEP_RFIFIND = 0 
        if flag_mask_present == False and config.flag_step_rfifind == 0:
                print()
                print()
                print("\033[1m  ERROR! mask '%s' not found but STEP_RFIFIND = 0! \033[0m" % (config.list_Observations[i].mask))
                print()
                print("You have to create a mask for your observation, in order to run the pipeline.")
                print()
                print("Please either set STEP_RFIFIND = 1 in your configuration file, or create your mask separately and copy the relative files into the '01_RFIFIND' directory and retry.")
                print()
                exit()

        
        # CASE 2: mask not present, STEP_RFIFIND = 1
        if flag_mask_present == False and config.flag_step_rfifind == 1:
                LOG_basename = "01_rfifind_%s" % (config.list_Observations[i].file_nameonly)
                log_abspath = "%s/LOG_%s.txt" % (LOG_dir, LOG_basename)
                

                if verbosity_level >= 1:
                        print()
                        print("Mask not found in the 01_RFIFIND folder. Will make the mask using the parameters specified in the configuration file '%s'. " % (config_filename))
                        print()
                        print("\033[1m >> TIP:\033[0m Check rfifind progress with '\033[1mtail -f %s\033[0m'" % (log_abspath))
                        print()
                        print("Creating rfifind mask of observation %3d/%d: '%s'..." % (i+1, len(config.list_Observations), config.list_Observations[i].file_nameonly), end=' ')
                        sys.stdout.flush()

                make_rfifind_mask(config.list_Observations[i].file_abspath,
                                   rfifind_masks_dir,
                                   LOG_dir,
                                   LOG_basename,
                                   config.rfifind_time,
                                   config.rfifind_time_intervals_to_zap,
                                   config.rfifind_chans_to_zap,
                                   config.rfifind_flags,
                                   config.presto_env,
                                   verbosity_level
                                   )
        
        # CASE 3: mask is already present, STEP_RFIFIND = 1
        elif flag_mask_present == True and config.flag_step_rfifind == 1:
                if verbosity_level >= 1:
                        print()
                        print("GOOD! Mask '%s' already present! Will not create a new one." % (config.list_Observations[i].mask)  )
                        print()
                        print("STEP_RFIFIND=1. Will check that the mask found is ok.")
                        print()
                else:
                        pass

                
        # CASE 4: mask is already present, STEP_RFIFIND = 0
        elif flag_mask_present == True and config.flag_step_rfifind == 0:
                if verbosity_level >= 1:
                        print()
                        print("GOOD! Mask '%s' already present! Will not create a new one." % (config.list_Observations[i].mask)  )
                        print()
                        print("%sWARNING:%s%s STEP_RFIFIND=0. Will skip the step, and trust that the mask found is ok.%s" % (colors.WARNING+colors.BOLD, colors.ENDCOLOR, colors.BOLD, colors.ENDCOLOR))
                        print()

                else:
                        pass


        # If STEP_RFIFIND = 1, check the mask before continuing 
        if  config.flag_step_rfifind == 1:
                print("Checking what percentage of the band is masked (this may take a while, depending on the size of the mask)...", end=' '); sys.stdout.flush()
                mask = rfifind.rfifind(config.list_Observations[i].mask)
                fraction_masked_channels = np.float64(len(mask.mask_zap_chans))/mask.nchan
                print("done!"); sys.stdout.flush()
                if verbosity_level >= 1:
                        print()
                        print("RFIFIND: Percentage of frequency channels masked: %.2f%%" % (fraction_masked_channels * 100.))
                        print()
                if fraction_masked_channels > 0.5 and fraction_masked_channels <=0.95:
                        print()
                        print("************************************************************************************************")
                        print("!!! %sWARNING%s%s: %.2f%% of the band was masked! That seems quite a lot! %s !!!" % (colors.WARNING+colors.BOLD, colors.ENDCOLOR, colors.BOLD, fraction_masked_channels * 100., colors.ENDCOLOR))
                        print("!!! If you think it is too much, try to adjust the RFIFIND parameters in the configuration file (e.g. increasing RFIFIND_FREQSIG)")
                        print("************************************************************************************************")
                        time.sleep(10)

                if fraction_masked_channels > 0.95:
                        print()
                        print("************************************************************************************************")
                        print("!!! %sERROR%s%s: %.2f%% of the band was masked! This is too much. %s!!!" % (colors.ERROR+colors.BOLD, colors.ENDCOLOR, colors.BOLD, fraction_masked_channels * 100., colors.ENDCOLOR))
                        print("!!!")
                        print("!!! %sPlease adjust the RFIFIND parameters in the configuration file so that  the percentage of%s" % (colors.BOLD, colors.ENDCOLOR))
                        print("!!! %schannels masked is (possibly much) less than 95%% and retry. %s" % (colors.BOLD, colors.ENDCOLOR))
                        print("************************************************************************************************")
                        exit()



                        
        




        weights_file = config.list_Observations[i].mask.replace(".mask", ".weights")
        if os.path.exists(weights_file):
                array_weights = np.loadtxt(weights_file, unpack=True, usecols=(0, 1,), skiprows=1)
                config.ignorechan_list = ",".join([str(x) for x in np.where(array_weights[1] == 0)[0] ])
                config.nchan_ignored = len(config.ignorechan_list.split(","))
                if verbosity_level >= 1:
                        print()
                        print()
                        print("WEIGHTS file '%s' found. Using it to ignore %d channels out of %d (%.2f%%)" % (os.path.basename(weights_file), config.nchan_ignored, config.list_Observations[i].nchan, 100*config.nchan_ignored/np.float64(config.list_Observations[i].nchan)))
                        print("IGNORED CHANNELS: %s" % (config.ignorechan_list))


##################################################################################################
# 2) BIRDIES AND ZAPLIST
##################################################################################################

if verbosity_level >= 1:
        print()
        print()
        print()
        print("##################################################################################################")
        print("                                   STEP 2 - BIRDIES AND ZAPLIST                                   ")
        print("##################################################################################################")
        print()
if verbosity_level >= 2:
        print("STEP_ZAPLIST = %s" % (config.flag_step_zaplist))


dir_birdies = os.path.join(config.root_workdir, "02_BIRDIES")

if config.flag_step_zaplist == 1:

        if verbosity_level >= 2:
                print("# =====================================================================================")
                print("# a) Create a 0-DM TOPOCENTRIC time series for each of the files, using the mask.")
                print("# =====================================================================================")
        if not os.path.exists(dir_birdies):
                os.mkdir(dir_birdies)

        for i in range(len(config.list_Observations)):
                time.sleep(0.1)
                print()
                print("Running prepdata to create 0-DM and topocentric time series of \"%s\"..." % (config.list_Observations[i].file_nameonly), end=' ')
                sys.stdout.flush()
                LOG_basename = "02a_prepdata_%s" % (config.list_Observations[i].file_nameonly)
                prepdata(config.list_Observations[i].file_abspath,
                          dir_birdies,
                          LOG_dir,
                          LOG_basename,
                          0,
                          config.list_Observations[i].N_samples,
                          config.ignorechan_list,
                          config.list_Observations[i].mask,
                          1,
                          "topocentric",
                          config.prepdata_flags,
                          config.presto_env,
                          verbosity_level
                          )
                if verbosity_level >= 1:
                        print("done!"); sys.stdout.flush()


        if verbosity_level >= 2:
                print("# ===============================================")
                print("# b) Fourier transform all the files")
                print("# ===============================================")
                print()

        config.list_0DM_datfiles = glob.glob("%s/*%s*.dat" % (dir_birdies, config.list_Observations[i].file_basename))   # Collect the *.dat files in the 02_BIRDIES_FOLDERS
        for i in range(len(config.list_0DM_datfiles)):
                time.sleep(0.1)
                if verbosity_level >= 1:
                        print("Running realfft on the 0-DM topocentric timeseries '%s'..." % (os.path.basename(config.list_0DM_datfiles[i])), end=' ')
                        sys.stdout.flush()

                LOG_basename = "02b_realfft_%s" % (os.path.basename(config.list_0DM_datfiles[i]))
                realfft(config.list_0DM_datfiles[i],
                        dir_birdies,
                        LOG_dir,
                        LOG_basename,
                        config.realfft_flags,
                        config.presto_env,
                        verbosity_level,
                        flag_LOG_append=0
                        )

                if verbosity_level >= 1:
                        print("done!")
                        sys.stdout.flush()


        if verbosity_level >= 2:
                print()
                print("# ===============================================")
                print("# 02c) Remove rednoise")
                print("# ===============================================")
                print()
        config.list_0DM_fftfiles = [x for x in glob.glob("%s/*%s*DM00.00.fft" % (dir_birdies, config.list_Observations[i].file_basename)) if not "_red" in x]  # Collect the *.fft files in the 02_BIRDIES_FOLDERS, exclude red files

        # print "len(config.list_0DM_datfiles), len(config.list_0DM_fftfiles) = ", len(config.list_0DM_datfiles), len(config.list_0DM_fftfiles)

        for i in range(len(config.list_0DM_fftfiles)):
                time.sleep(0.1)
                print("Running rednoise on the FFT \"%s\"..." % (os.path.basename(config.list_0DM_datfiles[i])), end=' ')
                sys.stdout.flush()
                LOG_basename = "02c_rednoise_%s" % (os.path.basename(config.list_0DM_fftfiles[i]))
                rednoise(config.list_0DM_fftfiles[i],
                         dir_birdies,
                         LOG_dir,
                         LOG_basename,
                         config.rednoise_flags,
                         config.presto_env,
                         verbosity_level
                         )
                if verbosity_level >= 1:
                        print("done!")
                        sys.stdout.flush()


        if verbosity_level >= 2:
                print()
                print("# ===============================================")
                print("# 02d) Accelsearch e zaplist")
                print("# ===============================================")
                print()

        config.list_0DM_fft_rednoise_files = glob.glob("%s/*%s*_DM00.00.fft" % (dir_birdies, config.list_Observations[i].file_basename))
        for i in range(len(config.list_0DM_fft_rednoise_files)):
                time.sleep(0.1)
                print("Making zaplist of 0-DM topocentric time series \"%s\"..." % (os.path.basename(config.list_0DM_datfiles[i])), end=' ')
                sys.stdout.flush() 
                LOG_basename = "02d_makezaplist_%s" % (os.path.basename(config.list_0DM_fft_rednoise_files[i]))
                zaplist_filename = make_zaplist(config.list_0DM_fft_rednoise_files[i],
                                                dir_birdies,
                                                LOG_dir,
                                                LOG_basename,
                                                config.file_common_birdies,
                                                2,
                                                config.accelsearch_flags,
                                                config.presto_env,
                                                verbosity_level
                                                )
                if verbosity_level >= 1:
                        print("done!")
                        sys.stdout.flush()

                if config.zap_isolated_pulsars_from_ffts == 1:
                        fourier_bin_size =  1./config.list_Observations[0].T_obs_s
                        zaplist_file = open(zaplist_filename, 'a')

                        zaplist_file.write("########################################\n")
                        zaplist_file.write("#              KNOWN PULSARS           #\n")
                        zaplist_file.write("########################################\n")
                        for psr in sorted(dict_freqs_to_zap.keys()):
                                zaplist_file.write("# Pulsar %s \n" % (psr))
                                for i_harm in range(1, config.zap_isolated_pulsars_max_harm+1):
                                        zaplist_file.write("B%21.14f   %19.17f\n" % (dict_freqs_to_zap[psr]*i_harm, fourier_bin_size*i_harm))
                        zaplist_file.close()


dir_dedispersion = os.path.join(config.root_workdir, "03_DEDISPERSION")
if config.flag_step_dedisperse == 1:
        if verbosity_level >= 1:
                print()
                print()
                print("##################################################################################################")
                print("#                 STEP 3 - DEDISPERSION, DE-REDDENING AND PERIODICITY SEARCH")
                print("##################################################################################################")
                print()

        LOG_basename = "03_prepsubband_and_search_FFT_%s" % (config.list_Observations[i].file_nameonly)
                
        if verbosity_level >= 2:
                print("3) DEDISPERSION DE-REDDENING AND PERIODICITY SEARCH: creating working directories...", end=' '); sys.stdout.flush()
        if not os.path.exists(dir_dedispersion):
                os.mkdir(dir_dedispersion)
        if verbosity_level >= 2:
                print("done!")
                print("get_DDplan_scheme(config.list_Observations[i].file_abspath, = ", config.list_Observations[i].file_abspath)
                print("LOG_basename = %s" % LOG_basename)


        list_DDplan_scheme = get_DDplan_scheme(config.list_Observations[i].file_abspath,
                                                dir_dedispersion,
                                                LOG_dir,
                                                LOG_basename,
                                                config.dm_min,
                                                config.dm_max,
                                                config.dm_coherent_dedispersion,
                                                config.max_simultaneous_dms_per_prepsubband,
                                                config.list_Observations[i].freq_central_MHz,
                                                config.list_Observations[i].bw_MHz,
                                                config.list_Observations[i].nchan,
                                                config.nsubbands,
                                                config.list_Observations[i].t_samp_s)

        ################################################################################
        # 1) LOOP OVER EACH OBSERVATION
        # 2)      LOOP OVER THE SEGMENT
        # 3)            LOOP OVER THE CHUNK


        # 1) LOOP OVER EACH OBSERVATION
        for i in range(len(config.list_Observations)):
                obs = config.list_Observations[i].file_basename
                time.sleep(1.0)
                work_dir_obs = os.path.join(dir_dedispersion, config.list_Observations[i].file_basename)
                if verbosity_level >= 2:
                        print("3) DEDISPERSION, DE-REDDENING AND PERIODICITY SEARCH: Creating working directory '%s'..." % (work_dir_obs), end=' '); sys.stdout.flush()
                if not os.path.exists(work_dir_obs):
                        os.mkdir(work_dir_obs)
                if verbosity_level >= 2:
                        print("done!"); sys.stdout.flush()


        # 2) LOOP OVER EACH SEGMENT
                if not "full" in list(config.dict_search_structure[obs].keys()):
                        print("NOTICE: full-length observation will not be searched!")
                        config.dict_search_structure[obs]['full'] = {'ck00': {'candidates': []}}
                list_segments = ['full'] + ["%sm" % (x) for x in sorted(config.list_segments_nofull)]
                # else:
                #        list_segments =  ["%sm" % (x) for x in sorted(config.list_segments)]


                N_seg = len(list_segments)
                for seg, i_seg in zip(list_segments, list(range(N_seg))):
                        work_dir_segment = os.path.join(work_dir_obs, "%s" % seg)
                        if verbosity_level >= 1:
                                print("\n3) DEDISPERSION, DE-REDDENING AND PERIODICITY SEARCH: creating working directory '%s'..." % (work_dir_segment), end=' '); sys.stdout.flush()
                        if not os.path.exists(work_dir_segment):
                                os.makedirs(work_dir_segment)
                        if verbosity_level >= 1:
                                print("done!"); sys.stdout.flush()

        # 3) LOOP OVER THE CHUNK
                        N_ck = len(list(config.dict_search_structure[obs][seg].keys()))
                        for ck, i_ck in zip(sorted(config.dict_search_structure[obs][seg].keys()), list(range(N_ck))):
                                if verbosity_level >= 1:
                                        print()
                                        print("**************************************************************")
                                        print("SEGMENT %s of %s  -- chunk %s of %s" % (seg, sorted(config.dict_search_structure[obs].keys()), ck, sorted(config.dict_search_structure[obs][seg].keys())))
                                        print("**************************************************************")
                                work_dir_chunk = os.path.join(work_dir_segment, ck)
                                if verbosity_level >= 1:
                                        print("3) DEDISPERSION, DE-REDDENING AND PERIODICITY SEARCH: Creating working directory '%s'..." % (work_dir_chunk), end=' '); sys.stdout.flush()
                                if not os.path.exists(work_dir_chunk):
                                        os.mkdir(work_dir_chunk)
                                if verbosity_level >= 1:
                                        print("done!"); sys.stdout.flush()


                                zapfile = "%s/%s_DM00.00.zaplist" % (dir_birdies, config.list_Observations[i].file_basename)


                                if verbosity_level >= 2:
                                        print("mask::: ", config.list_Observations[i].mask)
                                        print()
                                        print("**********************")
                                        print()
                                        print("config.list_cuda_ids = ", config.list_cuda_ids)
                                        print()
                                        print("config.presto_env = ", config.presto_env)
                                        print("config.presto_gpu_env = ", config.presto_gpu_env)
                                        print("**********************")

                                dict_flag_steps = {'flag_step_dedisperse': config.flag_step_dedisperse, 'flag_step_realfft': config.flag_step_realfft, 'flag_step_periodicity_search': config.flag_step_periodicity_search}


                                dedisperse_rednoise_and_periodicity_search_FFT(config.list_Observations[i].file_abspath,
                                                                                work_dir_chunk,
                                                                                config.root_workdir,
                                                                                LOG_dir,
                                                                                LOG_basename,
                                                                                config.flag_search_full,
                                                                                seg,
                                                                                ck,
                                                                                [i_seg, N_seg, i_ck, N_ck],
                                                                               zapfile,
                                                                               make_even_number(config.list_Observations[i].N_samples/1.0),
                                                                               config.ignorechan_list,
                                                                               config.list_Observations[i].mask,
                                                                               list_DDplan_scheme,
                                                                               config.list_Observations[i].nchan,
                                                                               config.nsubbands,
                                                                               config.num_simultaneous_prepsubbands,
                                                                               config.prepsubband_flags,
                                                                               config.presto_env,
                                                                               config.flag_use_cuda,
                                                                               config.list_cuda_ids,
                                                                               config.flag_acceleration_search,
                                                                               config.accelsearch_numharm,
                                                                               config.accelsearch_list_zmax,
                                                                               config.flag_jerk_search,
                                                                               config.jerksearch_zmax,
                                                                               config.jerksearch_wmax,
                                                                               config.jerksearch_numharm,
                                                                               config.num_simultaneous_jerksearches,
                                                                               config.period_to_search_min,
                                                                               config.period_to_search_max,
                                                                               config.accelsearch_flags,
                                                                               config.flag_remove_fftfiles,
                                                                               config.flag_remove_datfiles_of_segments,
                                                                               config.presto_env,
                                                                               config.presto_gpu_env,
                                                                               verbosity_level,
                                                                               dict_flag_steps)
                                

if config.flag_step_sifting == 1:
        print()
        print("##################################################################################################")
        print("#                                  STEP 4 - CANDIDATE SIFTING ")
        print("##################################################################################################")

        dir_sifting = os.path.join(config.root_workdir, "04_SIFTING")
        if verbosity_level >= 1:
                print("4) CANDIDATE SIFTING: Creating working directories...", end=' '); sys.stdout.flush()
        if not os.path.exists(dir_sifting):
                os.mkdir(dir_sifting)
        if verbosity_level >= 1:
                print("done!")

        dict_candidate_lists = {}

        for i in range(len(config.list_Observations)):
                obs = config.list_Observations[i].file_basename

                if verbosity_level >= 2:
                        print("Sifting candidates for observation %3d/%d '%s'." % (i+1, len(config.list_Observations), obs)) 
                for seg in sorted(config.dict_search_structure[obs].keys()):
                        work_dir_segment = os.path.join(dir_sifting, config.list_Observations[i].file_basename, "%s" % seg)
                        if not os.path.exists(work_dir_segment):
                                os.makedirs(work_dir_segment)

                        for ck in sorted(config.dict_search_structure[obs][seg].keys()):
                                work_dir_chunk = os.path.join(work_dir_segment, ck)
                                if not os.path.exists(work_dir_chunk):
                                        os.makedirs(work_dir_chunk)

                                LOG_basename = "04_sifting_%s_%s_%s" % (obs, seg, ck)
                                work_dir_candidate_sifting = os.path.join(dir_sifting, obs, seg, ck)

                                if verbosity_level >= 1:
                                        print("4) CANDIDATE SIFTING: Creating working directory '%s'..." % (work_dir_candidate_sifting), end=' '); sys.stdout.flush()
                                if not os.path.exists(work_dir_candidate_sifting):
                                        os.mkdir(work_dir_candidate_sifting)
                                if verbosity_level >= 1:
                                        print("done!")

                                if verbosity_level >= 1:
                                        print("4) CANDIDATE SIFTING: Sifting observation %d) \"%s\" / %s / %s..." % (i+1, obs, seg, ck), end=' ')
                                        sys.stdout.flush()



                                config.dict_search_structure[obs][seg][ck]['candidates'] = sift_candidates(work_dir_chunk,
                                                                                                            LOG_dir,
                                                                                                            LOG_basename,
                                                                                                            dir_dedispersion,
                                                                                                            obs,
                                                                                                            seg,
                                                                                                            ck,
                                                                                                            config.accelsearch_list_zmax,
                                                                                                            config.jerksearch_zmax,
                                                                                                            config.jerksearch_wmax,
                                                                                                            config.sifting_flag_remove_duplicates,
                                                                                                            config.sifting_flag_remove_dm_problems,
                                                                                                            config.sifting_flag_remove_harmonics,
                                                                                                            config.sifting_minimum_num_DMs,
                                                                                                            config.sifting_minimum_DM,
                                                                                                            config.period_to_search_min,
                                                                                                            config.period_to_search_max
                                )


        for i in range(len(config.list_Observations)):
                candidates_summary_filename = "%s/%s_cands.summary" % (dir_sifting, config.list_Observations[i].file_basename)
                candidates_summary_file = open(candidates_summary_filename, 'w')

                count_candidates_to_fold_all = 0
                candidates_summary_file.write("\n*****************************************************************")
                candidates_summary_file.write("\nCandidates found in %s:\n\n" % (config.list_Observations[i].file_nameonly))
                for seg in sorted(config.dict_search_structure[obs].keys()):
                        for ck in sorted(config.dict_search_structure[obs][seg].keys()):
                                Ncands_seg_ck = len(config.dict_search_structure[obs][seg][ck]['candidates'])
                                candidates_summary_file.write("%20s  |  %10s  ---> %4d candidates\n" % (seg, ck, Ncands_seg_ck))
                                count_candidates_to_fold_all = count_candidates_to_fold_all + Ncands_seg_ck
                candidates_summary_file.write("\nTOT = %d candidates\n" % (count_candidates_to_fold_all))
                candidates_summary_file.write("*****************************************************************\n\n")

                count_candidates_to_fold_redet = 0
                count_candidates_to_fold_new = 0
                list_all_cands = []
                for seg in sorted(config.dict_search_structure[obs].keys()):
                        for ck in sorted(config.dict_search_structure[obs][seg].keys()):
                                config.dict_search_structure[obs][seg][ck]['candidates_redetections'] = []
                                config.dict_search_structure[obs][seg][ck]['candidates_new'] = []

                                for j in range(len(config.dict_search_structure[obs][seg][ck]['candidates'])):
                                        candidate = config.dict_search_structure[obs][seg][ck]['candidates'][j]

                                        flag_is_know, known_psrname, str_harmonic = check_if_cand_is_known(candidate, list_known_pulsars, numharm=16)

                                        if flag_is_know == True:
                                                config.dict_search_structure[obs][seg][ck]['candidates_redetections'].append(candidate)
                                                count_candidates_to_fold_redet = count_candidates_to_fold_redet + 1
                                        elif flag_is_know == False:
                                                config.dict_search_structure[obs][seg][ck]['candidates_new'].append(candidate)
                                                count_candidates_to_fold_new = count_candidates_to_fold_new + 1

                                        dict_cand = {'cand': candidate, 'obs': obs, 'seg': seg, 'ck': ck, 'is_known': flag_is_know, 'known_psrname': known_psrname, 'str_harmonic': str_harmonic}
                                        list_all_cands.append(dict_cand)
                N_cands_all = len(list_all_cands)

                for i_cand, dict_cand in zip(list(range(0, N_cands_all)), sorted(list_all_cands, key=lambda k: k['cand'].p, reverse=False)):
                        if dict_cand['cand'].DM < 2:
                                candidates_summary_file.write("Cand %4d/%d: %12.6f ms    |  DM: %7.2f pc cm-3    (%4s / %4s | sigma: %5.2f)  ---> Likely RFI\n" % (i_cand+1, N_cands_all, dict_cand['cand'].p * 1000., dict_cand['cand'].DM, dict_cand['seg'], dict_cand['ck'], dict_cand['cand'].sigma))
                        else:
                                if dict_cand['is_known'] == True:
                                        candidates_summary_file.write("Cand %4d/%d:  %12.6f ms  |  DM: %7.2f pc cm-3    (%4s / %4s | sigma: %5.2f)  ---> Likely %s - %s\n" % (i_cand+1, N_cands_all, dict_cand['cand'].p * 1000., dict_cand['cand'].DM, dict_cand['seg'], dict_cand['ck'], dict_cand['cand'].sigma, dict_cand['known_psrname'], dict_cand['str_harmonic']))
                                elif dict_cand['is_known'] == False:
                                        candidates_summary_file.write("Cand %4d/%d:  %12.6f ms  |  DM: %7.2f pc cm-3    (%4s / %4s | sigma: %5.2f)\n" % (i_cand+1, N_cands_all, dict_cand['cand'].p * 1000., dict_cand['cand'].DM, dict_cand['seg'], dict_cand['ck'], dict_cand['cand'].sigma))

                candidates_summary_file.close()

                if verbosity_level >= 1:
                        candidates_summary_file = open(candidates_summary_filename, 'r')
                        for line in candidates_summary_file:
                                print(line, end=' ')
                        candidates_summary_file.close()


if config.flag_step_folding == 1:
        print()
        print()
        print("##################################################################################################")
        print("#                                        STEP 5 - FOLDING ")
        print("##################################################################################################")
        print()

        dir_folding = os.path.join(config.root_workdir, "05_FOLDING")
        if verbosity_level >= 1:
                print("5) FOLDING: Creating working directories...", end=' '); sys.stdout.flush()
        if not os.path.exists(dir_folding):
                os.mkdir(dir_folding)
        if verbosity_level >= 1:
                print("done!")

        for i in range(len(config.list_Observations)):
                obs = config.list_Observations[i].file_basename
                print("Folding observation '%s'" % (obs))
                print()

                work_dir_candidate_folding = os.path.join(dir_folding, config.list_Observations[i].file_basename)
                if verbosity_level >= 1:
                        print("5) CANDIDATE FOLDING: creating working directory '%s'..." % (work_dir_candidate_folding), end=' '); sys.stdout.flush()
                if not os.path.exists(work_dir_candidate_folding):
                        os.mkdir(work_dir_candidate_folding)
                if verbosity_level >= 1:
                        print("done!")

                file_script_fold_name = "script_fold.txt"
                file_script_fold_abspath = "%s/%s" % (work_dir_candidate_folding, file_script_fold_name)
                file_script_fold = open(file_script_fold_abspath, "w")
                file_script_fold.close()

                if config.flag_fold_known_pulsars == 1:
                        key_cands_to_fold = 'candidates'
                        if verbosity_level >= 1:
                                print()
                                print("5) CANDIDATE FOLDING: I will fold all the %d candidates (%s likely redetections included)" % (N_cands_all, count_candidates_to_fold_redet))
                        N_cands_to_fold = N_cands_all

                elif config.flag_fold_known_pulsars == 0:
                        key_cands_to_fold = 'candidates_new'
                        if verbosity_level >= 1:
                                print()
                                print("5) CANDIDATE FOLDING: I will fold only the %d putative new pulsars (%s likely redetections will not be folded)" % (count_candidates_to_fold_new, count_candidates_to_fold_redet))
                        N_cands_to_fold = count_candidates_to_fold_new
                count_folded_ts = 1
                if config.flag_fold_timeseries == 1:

                        LOG_basename = "05_folding_%s_timeseries" % (obs)
                        if verbosity_level >= 1:
                                print()
                                print("Folding time series...")
                                print()
                                print("\033[1m >> TIP:\033[0m Check folding progress with '\033[1mtail -f %s/LOG_%s.txt\033[0m'" % (LOG_dir, LOG_basename))
                                print()
                        for seg in sorted(config.dict_search_structure[obs].keys()):
                                for ck in sorted(config.dict_search_structure[obs][seg].keys()):
                                        for j in range(len(config.dict_search_structure[obs][seg][ck][key_cands_to_fold])):
                                                candidate = config.dict_search_structure[obs][seg][ck][key_cands_to_fold][j]

                                                print("FOLDING CANDIDATE TIMESERIES %d/%d of %s: seg %s / %s..." % (count_folded_ts, N_cands_to_fold, obs, seg, ck), end=' ')
                                                sys.stdout.flush()
                                                tstart_folding_cand_ts = time.time()
                                                file_to_fold = os.path.join(dir_dedispersion, obs, seg, ck, candidate.filename.split("_ACCEL")[0] + ".dat")
                                                flag_remove_dat_after_folding = 0
                                                if os.path.exists(file_to_fold):

                                                        fold_candidate(work_dir_candidate_folding,
                                                                LOG_dir,
                                                                LOG_basename,
                                                                config.list_Observations[i],
                                                                dir_dedispersion,
                                                                obs,
                                                                seg,
                                                                ck,
                                                                candidate,
                                                                config.ignorechan_list,
                                                                config.prepfold_flags,
                                                                config.presto_env,
                                                                verbosity_level,
                                                                1,
                                                                "timeseries",
                                                               config.num_simultaneous_prepfolds
                                                        )

                                                        tend_folding_cand_ts = time.time()
                                                        time_taken_folding_cand_ts_s = tend_folding_cand_ts - tstart_folding_cand_ts
                                                        print("done in %.2f s!" % (time_taken_folding_cand_ts_s))
                                                        sys.stdout.flush()
                                                        count_folded_ts = count_folded_ts + 1
                                                else:
                                                        print("dat file does not exists! Likely if you set FLAG_REMOVE_DATFILES_OF_SEGMENTS = 1 in the config file. Skipping...")

                count_folded_raw = 1
                if config.flag_fold_rawdata == 1:
                        LOG_basename = "05_folding_%s_rawdata" % (obs)
                        print()
                        print("Folding raw data \033[1m >> TIP:\033[0m Check folding progress with '\033[1mtail -f %s/LOG_%s.txt\033[0m'" % (LOG_dir, LOG_basename))
                        for seg in sorted(list(config.dict_search_structure[obs].keys()), reverse=True):
                                for ck in sorted(config.dict_search_structure[obs][seg].keys()):
                                        for j in range(len(config.dict_search_structure[obs][seg][ck][key_cands_to_fold])):
                                                candidate = config.dict_search_structure[obs][seg][ck][key_cands_to_fold][j]
                                                LOG_basename = "05_folding_%s_%s_%s_rawdata" % (obs, seg, ck)

                                                fold_candidate(work_dir_candidate_folding,
                                                                LOG_dir,
                                                                LOG_basename,
                                                                config.list_Observations[i],
                                                                dir_dedispersion,
                                                                obs,
                                                                seg,
                                                                ck,
                                                                candidate,
                                                                config.ignorechan_list,
                                                                config.prepfold_flags,
                                                                config.presto_env,
                                                                verbosity_level,
                                                                1,
                                                                "rawdata",
                                                               config.num_simultaneous_prepfolds
                                                )

                                                count_folded_raw = count_folded_raw + 1

                os.chdir(work_dir_candidate_folding)
                cmd_pm_run_multithread = "%s/pm_run_multithread -cmdfile %s -ncpus %d" % (os.path.dirname(sys.argv[0]), file_script_fold_abspath, config.num_simultaneous_prepfolds)
                print()
                print()
                print("5) CANDIDATE FOLDING - Now running:")
                print("%s" % cmd_pm_run_multithread)
                os.system(cmd_pm_run_multithread)





if config.flag_singlepulse_search == 1 and config.flag_step_singlepulse_search == 1:
        print()
        print()
        print("##################################################################################################")
        print("#                                        STEP 6 - SINGLE-PULSE SEARCH (PRESTO) ")
        print("##################################################################################################")
        print()

        dir_singlepulse_search = os.path.join(config.root_workdir, "06_SINGLEPULSE")
        if verbosity_level >= 1:
                print("6) SINGLE-PULSE SEARCH: Creating working directories...", end=' '); sys.stdout.flush()
        if not os.path.exists(dir_singlepulse_search):
                os.mkdir(dir_singlepulse_search)

        for i in range(len(config.list_Observations)):
                obs = config.list_Observations[i].file_basename
                time.sleep(1.0)
                work_dir_singlepulse_search_obs = os.path.join(dir_singlepulse_search, config.list_Observations[i].file_basename)
                if verbosity_level >= 2:
                        print("6) SINGLE-PULSE SEARCH: Creating working directory '%s'..." % (work_dir_singlepulse_search_obs), end=' '); sys.stdout.flush()
                if not os.path.exists(work_dir_singlepulse_search_obs):
                        os.mkdir(work_dir_singlepulse_search_obs)
                if verbosity_level >= 2:
                        print("done!"); sys.stdout.flush()

        if verbosity_level >= 1:
                print("done!")



        # Go into the 06_SINGLEPULSE directory
        os.chdir(work_dir_singlepulse_search_obs)

        # Create symbolic links to all the full-length *.dat and corresponding *.inf files
        search_string_datfiles_full_length = "%s/03_DEDISPERSION/%s/full/ck00/*.dat" % (config.root_workdir, config.list_Observations[0].file_basename) #List of datfiles
        search_string_inffiles_full_length = "%s/03_DEDISPERSION/%s/full/ck00/*.inf" % (config.root_workdir, config.list_Observations[0].file_basename) #List of inffiles
        list_datfiles_full_length = glob.glob(search_string_datfiles_full_length)
        list_inffiles_full_length = glob.glob(search_string_inffiles_full_length)

        for f in list_datfiles_full_length + list_inffiles_full_length:
                symlink_filename = os.path.basename(f)
                if os.path.exists(symlink_filename) and os.path.islink(symlink_filename):
                        print("Symlink %s already exists. Skipping..." % (symlink_filename))
                else:
                        print("Making symbolic link of '%s'..." % (symlink_filename), end=''); sys.stdout.flush()
                        os.symlink(f, symlink_filename)
                        print("done!"); sys.stdout.flush()

        LOG_singlepulse_search_basename = "06_singlepulse_search_%s" % (config.list_Observations[0].file_basename)
        LOG_singlepulse_search_abspath  = "%s/LOG_%s.txt" % (LOG_dir, LOG_singlepulse_search_basename)
        
        list_datfiles_to_singlepulse_search = glob.glob("%s/*.dat" % work_dir_singlepulse_search_obs)


        singlepulse_search(work_dir_singlepulse_search_obs,
                           LOG_dir,
                           LOG_singlepulse_search_basename,
                           list_datfiles_to_singlepulse_search,
                           config.singlepulse_search_flags,
                           config.num_simultaneous_singlepulse_searches,
                           config.presto_env,
                           verbosity_level,
                           config.flag_step_singlepulse_search)

                
if config.list_Observations[i].file_buffer_copy != "":
        if config.flag_keep_data_in_buffer_dir == 1:
                print()
                print("Keeping a copy of '%s' from the buffer directory (%s)." % (config.list_Observations[i].file_nameonly, config.fast_buffer_dir))
                print("Remember to delete if you are not using it further.")
        else:
                print("Removing copy of '%s' from the buffer directory (%s)..." % (config.list_Observations[i].file_nameonly, config.fast_buffer_dir), end=""), ; sys.stdout.flush()
                os.remove(config.list_Observations[i].file_buffer_copy)
                print("done!")

print()
